diff --git a/server/src/main/java/org/apache/druid/catalog/model/CatalogUtils.java b/server/src/main/java/org/apache/druid/catalog/model/CatalogUtils.java
index 02779c7532..2099282060 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/CatalogUtils.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/CatalogUtils.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.catalog.model;
 
+import static java.util.stream.Collectors.joining;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.google.common.base.Strings;
 import java.net.URI;
@@ -30,7 +33,6 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 import javax.annotation.Nullable;
 import org.apache.druid.catalog.model.ModelProperties.PropertyDefn;
@@ -47,7 +49,7 @@ import org.joda.time.Period;
 
 public class CatalogUtils {
   public static List<String> columnNames(List<ColumnSpec> columns) {
-    return columns.stream().map(col -> col.name()).collect(Collectors.toList());
+    return columns.stream().map(ColumnSpec::name).collect(toList());
   }
 
   /**
@@ -156,7 +158,7 @@ public class CatalogUtils {
     return Stream.of(base, additions)
         .filter(Objects::nonNull)
         .flatMap(Collection::stream)
-        .collect(Collectors.toList());
+        .collect(toList());
   }
 
   /** Get a string parameter that can either be null or non-blank. */
@@ -248,10 +250,10 @@ public class CatalogUtils {
           "Unsupported segment graularity. "
               + "Please use an equivalent of these granularities: %s.",
           Arrays.stream(GranularityType.values())
-              .filter(granularityType -> !granularityType.equals(GranularityType.NONE))
+              .filter(granularityType -> granularityType != GranularityType.NONE)
               .map(Enum::name)
               .map(StringUtils::toLowerCase)
-              .collect(Collectors.joining(", ")));
+              .collect(joining(", ")));
     }
   }
 
diff --git a/server/src/main/java/org/apache/druid/catalog/model/ColumnSpec.java b/server/src/main/java/org/apache/druid/catalog/model/ColumnSpec.java
index e81a5980d3..78674c783c 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/ColumnSpec.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/ColumnSpec.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.catalog.model;
 
+import static java.util.Collections.emptyMap;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonInclude.Include;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.base.Strings;
-import java.util.Collections;
 import java.util.Map;
 import java.util.Objects;
 import javax.annotation.Nullable;
@@ -64,7 +65,7 @@ public class ColumnSpec {
       @JsonProperty("properties") @Nullable final Map<String, Object> properties) {
     this.name = name;
     this.sqlType = sqlType;
-    this.properties = properties == null ? Collections.emptyMap() : properties;
+    this.properties = properties == null ? emptyMap() : properties;
   }
 
   public ColumnSpec(ColumnSpec from) {
@@ -76,14 +77,14 @@ public class ColumnSpec {
     return name;
   }
 
-  @JsonProperty("sqlType")
   @JsonInclude(Include.NON_NULL)
+  @JsonProperty("sqlType")
   public String sqlType() {
     return sqlType;
   }
 
-  @JsonProperty("properties")
   @JsonInclude(Include.NON_EMPTY)
+  @JsonProperty("properties")
   public Map<String, Object> properties() {
     return properties;
   }
diff --git a/server/src/main/java/org/apache/druid/catalog/model/ModelProperties.java b/server/src/main/java/org/apache/druid/catalog/model/ModelProperties.java
index e123fba4eb..3baeaae6dc 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/ModelProperties.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/ModelProperties.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.catalog.model;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.List;
@@ -177,7 +178,7 @@ public interface ModelProperties {
     public TypeRefPropertyDefn(
         final String name, final String typeName, final TypeReference<T> valueType) {
       super(name);
-      this.typeName = Preconditions.checkNotNull(typeName);
+      this.typeName = requireNonNull(typeName);
       this.valueType = valueType;
     }
 
@@ -246,8 +247,8 @@ public interface ModelProperties {
       super(name, typeName, valueType);
     }
 
-    @SuppressWarnings("unchecked")
     @Override
+    @SuppressWarnings("unchecked")
     public Object merge(Object existing, Object updates) {
       if (updates == null) {
         return existing;
diff --git a/server/src/main/java/org/apache/druid/catalog/model/SchemaRegistryImpl.java b/server/src/main/java/org/apache/druid/catalog/model/SchemaRegistryImpl.java
index d6cca5872b..f850526bc8 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/SchemaRegistryImpl.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/SchemaRegistryImpl.java
@@ -20,7 +20,7 @@
 package org.apache.druid.catalog.model;
 
 import com.google.common.collect.ImmutableSet;
-import com.google.common.collect.Lists;
+import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
@@ -119,7 +119,7 @@ public class SchemaRegistryImpl implements SchemaRegistry {
   public List<SchemaSpec> schemas() {
     // No real need to sort every time. However, this is used infrequently,
     // so OK for now.
-    List<SchemaSpec> schemas = Lists.newArrayList(builtIns.values());
+    List<SchemaSpec> schemas = new ArrayList<>(builtIns.values());
     Collections.sort(schemas, (s1, s2) -> s1.name().compareTo(s2.name()));
     return schemas;
   }
diff --git a/server/src/main/java/org/apache/druid/catalog/model/TableDefn.java b/server/src/main/java/org/apache/druid/catalog/model/TableDefn.java
index becd7ac2e6..72227c6d42 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/TableDefn.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/TableDefn.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.catalog.model;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Strings;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -52,7 +53,7 @@ public class TableDefn extends ObjectDefn {
         name,
         typeValue,
         CatalogUtils.concatLists(
-            Collections.singletonList(new ModelProperties.StringPropertyDefn(DESCRIPTION_PROPERTY)),
+            singletonList(new ModelProperties.StringPropertyDefn(DESCRIPTION_PROPERTY)),
             properties));
     this.columnProperties = toPropertyMap(columnProperties);
   }
diff --git a/server/src/main/java/org/apache/druid/catalog/model/TableSpec.java b/server/src/main/java/org/apache/druid/catalog/model/TableSpec.java
index b1794f9c81..a17af8ce7f 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/TableSpec.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/TableSpec.java
@@ -19,12 +19,14 @@
 
 package org.apache.druid.catalog.model;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonInclude.Include;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.base.Strings;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
@@ -48,8 +50,8 @@ public class TableSpec {
       @JsonProperty("properties") @Nullable final Map<String, Object> properties,
       @JsonProperty("columns") @Nullable final List<ColumnSpec> columns) {
     this.type = type;
-    this.properties = properties == null ? Collections.emptyMap() : properties;
-    this.columns = columns == null ? Collections.emptyList() : columns;
+    this.properties = properties == null ? emptyMap() : properties;
+    this.columns = columns == null ? emptyList() : columns;
 
     // Note: no validation here. If a bad definition got into the
     // DB, don't prevent deserialization.
@@ -68,14 +70,14 @@ public class TableSpec {
     return type;
   }
 
-  @JsonProperty("properties")
   @JsonInclude(Include.NON_NULL)
+  @JsonProperty("properties")
   public Map<String, Object> properties() {
     return properties;
   }
 
-  @JsonProperty("columns")
   @JsonInclude(Include.NON_NULL)
+  @JsonProperty("columns")
   public List<ColumnSpec> columns() {
     return columns;
   }
diff --git a/server/src/main/java/org/apache/druid/catalog/model/facade/DatasourceFacade.java b/server/src/main/java/org/apache/druid/catalog/model/facade/DatasourceFacade.java
index f00eecfcf6..aaab4bdfdc 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/facade/DatasourceFacade.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/facade/DatasourceFacade.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.catalog.model.facade;
 
-import java.util.Collections;
+import static java.util.Collections.emptyList;
+
 import java.util.List;
 import org.apache.druid.catalog.model.CatalogUtils;
 import org.apache.druid.catalog.model.ResolvedTable;
@@ -56,7 +57,7 @@ public class DatasourceFacade extends TableFacade {
   public List<ClusterKeySpec> clusterKeys() {
     Object value = property(DatasourceDefn.CLUSTER_KEYS_PROPERTY);
     if (value == null) {
-      return Collections.emptyList();
+      return emptyList();
     }
     try {
       return jsonMapper().convertValue(value, ClusterKeySpec.CLUSTER_KEY_LIST_TYPE_REF);
@@ -64,14 +65,14 @@ public class DatasourceFacade extends TableFacade {
       LOG.error(
           "Failed to convert a catalog %s property of value [%s]",
           DatasourceDefn.CLUSTER_KEYS_PROPERTY, value);
-      return Collections.emptyList();
+      return emptyList();
     }
   }
 
   @SuppressWarnings("unchecked")
   public List<String> hiddenColumns() {
     Object value = property(DatasourceDefn.HIDDEN_COLUMNS_PROPERTY);
-    return value == null ? Collections.emptyList() : (List<String>) value;
+    return value == null ? emptyList() : (List<String>) value;
   }
 
   public boolean isSealed() {
diff --git a/server/src/main/java/org/apache/druid/catalog/model/table/BaseInputSourceDefn.java b/server/src/main/java/org/apache/druid/catalog/model/table/BaseInputSourceDefn.java
index ca40a70839..4dcdb403ac 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/table/BaseInputSourceDefn.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/table/BaseInputSourceDefn.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.singleton;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -131,7 +132,7 @@ public abstract class BaseInputSourceDefn implements InputSourceDefn {
         convertArgsToSource(args, jsonMapper),
         convertArgsToFormat(args, columns, jsonMapper),
         Columns.convertSignature(columns),
-        () -> Collections.singleton(typeValue()));
+        () -> singleton(typeValue()));
   }
 
   /** Convert the input source using arguments to a "from scratch" table function. */
@@ -177,7 +178,7 @@ public abstract class BaseInputSourceDefn implements InputSourceDefn {
         convertTableToSource(table),
         convertTableToFormat(table),
         Columns.convertSignature(table.resolvedTable().spec().columns()),
-        () -> Collections.singleton(typeValue()));
+        () -> singleton(typeValue()));
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/catalog/model/table/ClusterKeySpec.java b/server/src/main/java/org/apache/druid/catalog/model/table/ClusterKeySpec.java
index 2b0265598d..23bf604454 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/table/ClusterKeySpec.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/table/ClusterKeySpec.java
@@ -54,8 +54,8 @@ public class ClusterKeySpec {
     return expr;
   }
 
-  @JsonProperty("desc")
   @JsonInclude(Include.NON_DEFAULT)
+  @JsonProperty("desc")
   public boolean desc() {
     return desc;
   }
diff --git a/server/src/main/java/org/apache/druid/catalog/model/table/FormattedInputSourceDefn.java b/server/src/main/java/org/apache/druid/catalog/model/table/FormattedInputSourceDefn.java
index a39a69d236..2981f5876d 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/table/FormattedInputSourceDefn.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/table/FormattedInputSourceDefn.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.singleton;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -172,6 +173,6 @@ public abstract class FormattedInputSourceDefn extends BaseInputSourceDefn {
         convertSource(sourceMap, jsonMapper),
         inputFormat,
         Columns.convertSignature(completedCols),
-        () -> Collections.singleton(typeValue()));
+        () -> singleton(typeValue()));
   }
 }
diff --git a/server/src/main/java/org/apache/druid/catalog/model/table/HttpInputSourceDefn.java b/server/src/main/java/org/apache/druid/catalog/model/table/HttpInputSourceDefn.java
index df467091d4..b824bf1f07 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/table/HttpInputSourceDefn.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/table/HttpInputSourceDefn.java
@@ -19,16 +19,18 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.collect.ImmutableMap;
 import java.net.URI;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
-import java.util.stream.Collectors;
 import org.apache.druid.catalog.model.CatalogUtils;
 import org.apache.druid.catalog.model.ColumnSpec;
 import org.apache.druid.catalog.model.table.BaseTableFunction.Parameter;
@@ -90,7 +92,7 @@ public class HttpInputSourceDefn extends FormattedInputSourceDefn {
   public static final String PASSWORD_ENV_VAR_PARAMETER = "passwordEnvVar";
 
   private static final List<ParameterDefn> URI_PARAMS =
-      Collections.singletonList(new Parameter(URIS_PARAMETER, ParameterType.VARCHAR_ARRAY, true));
+      singletonList(new Parameter(URIS_PARAMETER, ParameterType.VARCHAR_ARRAY, true));
 
   private static final List<ParameterDefn> USER_PWD_PARAMS =
       Arrays.asList(
@@ -147,7 +149,7 @@ public class HttpInputSourceDefn extends FormattedInputSourceDefn {
       // Patch in a dummy URI so that validation of the rest of the fields
       // will pass.
       try {
-        sourceMap.put(URIS_FIELD, Collections.singletonList(new URI("https://bogus.com/file")));
+        sourceMap.put(URIS_FIELD, singletonList(new URI("https://bogus.com/file")));
       } catch (Exception e) {
         throw new ISE(e, "URI parse failed");
       }
@@ -191,7 +193,7 @@ public class HttpInputSourceDefn extends FormattedInputSourceDefn {
 
   @Override
   public TableFunction partialTableFn(ResolvedExternalTable table) {
-    List<ParameterDefn> params = Collections.emptyList();
+    List<ParameterDefn> params = emptyList();
 
     // Does the table define URIs?
     Map<String, Object> sourceMap = table.inputSourceMap;
@@ -239,8 +241,7 @@ public class HttpInputSourceDefn extends FormattedInputSourceDefn {
       throw new IAE("One or more URIs is required in parameter %s", URIS_PARAMETER);
     }
     final Matcher m = templateMatcher(uriTemplate);
-    final List<String> uris =
-        uriStrings.stream().map(uri -> m.replaceFirst(uri)).collect(Collectors.toList());
+    final List<String> uris = uriStrings.stream().map(m::replaceFirst).collect(toList());
     jsonMap.put(URIS_FIELD, CatalogUtils.stringListToUriList(uris));
   }
 
diff --git a/server/src/main/java/org/apache/druid/catalog/model/table/InlineInputSourceDefn.java b/server/src/main/java/org/apache/druid/catalog/model/table/InlineInputSourceDefn.java
index fa3c9ce468..959c388cac 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/table/InlineInputSourceDefn.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/table/InlineInputSourceDefn.java
@@ -19,7 +19,9 @@
 
 package org.apache.druid.catalog.model.table;
 
-import java.util.Collections;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singletonList;
+
 import java.util.List;
 import java.util.Map;
 import org.apache.druid.catalog.model.CatalogUtils;
@@ -55,13 +57,12 @@ public class InlineInputSourceDefn extends FormattedInputSourceDefn {
 
   @Override
   protected List<ParameterDefn> adHocTableFnParameters() {
-    return Collections.singletonList(
-        new Parameter(DATA_PROPERTY, ParameterType.VARCHAR_ARRAY, false));
+    return singletonList(new Parameter(DATA_PROPERTY, ParameterType.VARCHAR_ARRAY, false));
   }
 
   @Override
   public TableFunction partialTableFn(ResolvedExternalTable table) {
-    return new PartialTableFunction(table, Collections.emptyList());
+    return new PartialTableFunction(table, emptyList());
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/catalog/model/table/InputFormats.java b/server/src/main/java/org/apache/druid/catalog/model/table/InputFormats.java
index b5b4005bd6..a72e10a459 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/table/InputFormats.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/table/InputFormats.java
@@ -19,14 +19,16 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.stream.Collectors;
 import org.apache.druid.catalog.model.CatalogUtils;
 import org.apache.druid.catalog.model.ColumnSpec;
 import org.apache.druid.catalog.model.ResolvedTable;
@@ -56,7 +58,7 @@ public class InputFormats {
     private final List<ParameterDefn> parameters;
 
     public BaseFormatDefn(List<ParameterDefn> parameters) {
-      this.parameters = parameters == null ? Collections.emptyList() : parameters;
+      this.parameters = parameters == null ? emptyList() : parameters;
     }
 
     @Override
@@ -82,7 +84,7 @@ public class InputFormats {
      * form requires by input formats.
      */
     protected void convertColumns(Map<String, Object> jsonMap, List<ColumnSpec> columns) {
-      List<String> cols = columns.stream().map(col -> col.name()).collect(Collectors.toList());
+      List<String> cols = columns.stream().map(ColumnSpec::name).collect(toList());
       jsonMap.put("columns", cols);
     }
 
@@ -139,7 +141,7 @@ public class InputFormats {
       ResolvedTable resolvedTable = table.resolvedTable();
       Map<String, Object> jsonMap = toMap(table);
       // Make up a column just so that validation will pass.
-      jsonMap.putIfAbsent(COLUMNS_FIELD, Collections.singletonList("a"));
+      jsonMap.putIfAbsent(COLUMNS_FIELD, singletonList("a"));
       convert(jsonMap, resolvedTable.jsonMapper());
     }
 
@@ -212,9 +214,7 @@ public class InputFormats {
     @VisibleForTesting public static final String DELIMITER_FIELD = "delimiter";
 
     public DelimitedFormatDefn() {
-      super(
-          Collections.singletonList(
-              new Parameter(DELIMITER_PARAMETER, ParameterType.VARCHAR, true)));
+      super(singletonList(new Parameter(DELIMITER_PARAMETER, ParameterType.VARCHAR, true)));
     }
 
     @Override
diff --git a/server/src/main/java/org/apache/druid/catalog/model/table/LocalInputSourceDefn.java b/server/src/main/java/org/apache/druid/catalog/model/table/LocalInputSourceDefn.java
index 62ea6eb695..d13e9a12f4 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/table/LocalInputSourceDefn.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/table/LocalInputSourceDefn.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.base.Strings;
 import java.io.File;
 import java.util.ArrayList;
@@ -26,7 +28,6 @@ import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.stream.Collectors;
 import org.apache.druid.catalog.model.CatalogUtils;
 import org.apache.druid.catalog.model.ColumnSpec;
 import org.apache.druid.catalog.model.table.BaseTableFunction.Parameter;
@@ -148,7 +149,7 @@ public class LocalInputSourceDefn extends FormattedInputSourceDefn {
 
   private List<String> absolutePath(String baseDirPath, List<String> files) {
     final File baseDir = new File(baseDirPath);
-    return files.stream().map(f -> new File(baseDir, f).toString()).collect(Collectors.toList());
+    return files.stream().map(f -> new File(baseDir, f).toString()).collect(toList());
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/catalog/model/table/TableBuilder.java b/server/src/main/java/org/apache/druid/catalog/model/table/TableBuilder.java
index 31560662db..38c8fe8840 100644
--- a/server/src/main/java/org/apache/druid/catalog/model/table/TableBuilder.java
+++ b/server/src/main/java/org/apache/druid/catalog/model/table/TableBuilder.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Strings;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -163,7 +164,7 @@ public class TableBuilder {
   }
 
   public TableBuilder column(String name, String sqlType, Map<String, Object> properties) {
-    Preconditions.checkNotNull(tableType);
+    requireNonNull(tableType);
     return column(new ColumnSpec(name, sqlType, properties));
   }
 
@@ -176,7 +177,7 @@ public class TableBuilder {
   }
 
   public ResolvedTable buildResolved(ObjectMapper mapper) {
-    Preconditions.checkNotNull(defn);
+    requireNonNull(defn);
     return new ResolvedTable(defn, buildSpec(), mapper);
   }
 }
diff --git a/server/src/main/java/org/apache/druid/client/BatchServerInventoryView.java b/server/src/main/java/org/apache/druid/client/BatchServerInventoryView.java
index e8b8509b4f..900c2c80c8 100644
--- a/server/src/main/java/org/apache/druid/client/BatchServerInventoryView.java
+++ b/server/src/main/java/org/apache/druid/client/BatchServerInventoryView.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Predicates.or;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Predicate;
-import com.google.common.base.Predicates;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Sets;
 import java.io.IOException;
@@ -155,7 +156,7 @@ public class BatchServerInventoryView implements ServerInventoryView, FilteredSe
               }
             });
 
-    this.defaultFilter = Preconditions.checkNotNull(defaultFilter);
+    this.defaultFilter = requireNonNull(defaultFilter);
   }
 
   @LifecycleStart
@@ -288,30 +289,28 @@ public class BatchServerInventoryView implements ServerInventoryView, FilteredSe
   private Set<DataSegment> filterInventory(
       final DruidServer container, Set<DataSegment> inventory) {
     Predicate<Pair<DruidServerMetadata, DataSegment>> predicate =
-        Predicates.or(defaultFilter, Predicates.or(segmentPredicates.values()));
+        or(defaultFilter, or(segmentPredicates.values()));
 
     // make a copy of the set and not just a filtered view, in order to not keep all the segment
     // data in memory
-    Set<DataSegment> filteredInventory =
-        Sets.newHashSet(
-            Iterables.transform(
-                Iterables.filter(
-                    Iterables.transform(
-                        inventory,
-                        new Function<DataSegment, Pair<DruidServerMetadata, DataSegment>>() {
-                          @Override
-                          public Pair<DruidServerMetadata, DataSegment> apply(DataSegment input) {
-                            return Pair.of(container.getMetadata(), input);
-                          }
-                        }),
-                    predicate),
-                new Function<Pair<DruidServerMetadata, DataSegment>, DataSegment>() {
-                  @Override
-                  public DataSegment apply(Pair<DruidServerMetadata, DataSegment> input) {
-                    return DataSegmentInterner.intern(input.rhs);
-                  }
-                }));
-    return filteredInventory;
+    return Sets.newHashSet(
+        Iterables.transform(
+            Iterables.filter(
+                Iterables.transform(
+                    inventory,
+                    new Function<DataSegment, Pair<DruidServerMetadata, DataSegment>>() {
+                      @Override
+                      public Pair<DruidServerMetadata, DataSegment> apply(DataSegment input) {
+                        return Pair.of(container.getMetadata(), input);
+                      }
+                    }),
+                predicate),
+            new Function<Pair<DruidServerMetadata, DataSegment>, DataSegment>() {
+              @Override
+              public DataSegment apply(Pair<DruidServerMetadata, DataSegment> input) {
+                return DataSegmentInterner.intern(input.rhs);
+              }
+            }));
   }
 
   protected DruidServer updateInnerInventory(
diff --git a/server/src/main/java/org/apache/druid/client/BatchServerInventoryViewProvider.java b/server/src/main/java/org/apache/druid/client/BatchServerInventoryViewProvider.java
index ce202aefa8..061113faff 100644
--- a/server/src/main/java/org/apache/druid/client/BatchServerInventoryViewProvider.java
+++ b/server/src/main/java/org/apache/druid/client/BatchServerInventoryViewProvider.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Predicates.alwaysTrue;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Predicates;
 import javax.validation.constraints.NotNull;
 import org.apache.curator.framework.CuratorFramework;
 import org.apache.druid.server.initialization.ZkPathsConfig;
@@ -37,6 +38,6 @@ public class BatchServerInventoryViewProvider implements ServerInventoryViewProv
   @Override
   public BatchServerInventoryView get() {
     return new BatchServerInventoryView(
-        zkPaths, curator, jsonMapper, Predicates.alwaysTrue(), "BatchServerInventoryView");
+        zkPaths, curator, jsonMapper, alwaysTrue(), "BatchServerInventoryView");
   }
 }
diff --git a/server/src/main/java/org/apache/druid/client/BrokerServerView.java b/server/src/main/java/org/apache/druid/client/BrokerServerView.java
index a2950e20b6..22e5c0b946 100644
--- a/server/src/main/java/org/apache/druid/client/BrokerServerView.java
+++ b/server/src/main/java/org/apache/druid/client/BrokerServerView.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.client;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Predicate;
 import com.google.common.collect.Ordering;
@@ -33,7 +35,6 @@ import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Executor;
 import java.util.concurrent.ExecutorService;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 import org.apache.druid.client.selector.QueryableDruidServer;
 import org.apache.druid.client.selector.ServerSelector;
 import org.apache.druid.client.selector.TierSelectorStrategy;
@@ -256,7 +257,7 @@ public class BrokerServerView implements TimelineServerView {
       // query topologies, but for now just skip all brokers, so we don't create some sort of wild
       // infinite query
       // loop...
-      if (!server.getType().equals(ServerType.BROKER)) {
+      if (server.getType() != ServerType.BROKER) {
         log.debug("Adding segment[%s] for server[%s]", segment, server);
         ServerSelector selector = selectors.get(segmentId);
         if (selector == null) {
@@ -307,7 +308,7 @@ public class BrokerServerView implements TimelineServerView {
       // we don't store broker segments here, but still run the callbacks for the segment being
       // removed from the server
       // since the broker segments are not stored on the timeline, do not fire segmentRemoved event
-      if (server.getType().equals(ServerType.BROKER)) {
+      if (server.getType() == ServerType.BROKER) {
         runTimelineCallbacks(callback -> callback.serverSegmentRemoved(server, segment));
         return;
       }
@@ -411,6 +412,6 @@ public class BrokerServerView implements TimelineServerView {
   public List<ImmutableDruidServer> getDruidServers() {
     return clients.values().stream()
         .map(queryableDruidServer -> queryableDruidServer.getServer().toImmutableDruidServer())
-        .collect(Collectors.toList());
+        .collect(toList());
   }
 }
diff --git a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java
index cfd7ec08f6..5720b210cc 100644
--- a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java
+++ b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java
@@ -19,11 +19,16 @@
 
 package org.apache.druid.client;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.emptyIterator;
+import static java.util.Collections.emptyList;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Iterators;
@@ -36,9 +41,7 @@ import com.google.common.hash.Hashing;
 import com.google.common.primitives.Bytes;
 import com.google.inject.Inject;
 import java.io.IOException;
-import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.LinkedHashSet;
@@ -52,7 +55,6 @@ import java.util.TreeMap;
 import java.util.concurrent.ForkJoinPool;
 import java.util.function.BinaryOperator;
 import java.util.function.UnaryOperator;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.client.cache.Cache;
 import org.apache.druid.client.cache.CacheConfig;
@@ -277,7 +279,7 @@ public class CachingClusteredClient implements QuerySegmentWalker {
           dataSourceAnalysis
               .getBaseQuerySegmentSpec()
               .map(QuerySegmentSpec::getIntervals)
-              .orElseGet(() -> query.getIntervals());
+              .orElseGet(query::getIntervals);
       this.cacheKeyManager = new CacheKeyManager<>(query, strategy, useCache, populateCache);
     }
 
@@ -335,7 +337,7 @@ public class CachingClusteredClient implements QuerySegmentWalker {
         @Nullable
         final String currentEtag =
             cacheKeyManager.computeResultLevelCachingEtag(segmentServers, queryCacheKey);
-        if (null != currentEtag) {
+        if (currentEtag != null) {
           responseContext.putEntityTag(currentEtag);
         }
         if (currentEtag != null && currentEtag.equals(prevEtag)) {
@@ -436,7 +438,7 @@ public class CachingClusteredClient implements QuerySegmentWalker {
           lookupFn = specificSegments ? timeline::lookupWithIncompletePartitions : timeline::lookup;
 
       List<TimelineObjectHolder<String, ServerSelector>> timelineObjectHolders =
-          intervals.stream().flatMap(i -> lookupFn.apply(i).stream()).collect(Collectors.toList());
+          intervals.stream().flatMap(i -> lookupFn.apply(i).stream()).collect(toList());
       final List<TimelineObjectHolder<String, ServerSelector>> serversLookup =
           toolChest.filterSegments(query, timelineObjectHolders);
 
@@ -508,7 +510,7 @@ public class CachingClusteredClient implements QuerySegmentWalker {
     private List<Pair<Interval, byte[]>> pruneSegmentsWithCachedResults(
         final byte[] queryCacheKey, final Set<SegmentServerSelector> segments) {
       if (queryCacheKey == null) {
-        return Collections.emptyList();
+        return emptyList();
       }
       final List<Pair<Interval, byte[]>> alreadyCachedResults = new ArrayList<>();
       Map<SegmentServerSelector, Cache.NamedKey> perSegmentCacheKeys =
@@ -609,7 +611,7 @@ public class CachingClusteredClient implements QuerySegmentWalker {
                   public Iterator<Object> make() {
                     try {
                       if (cachedResult.length == 0) {
-                        return Collections.emptyIterator();
+                        return emptyIterator();
                       }
 
                       return objectMapper.readValues(
@@ -781,11 +783,11 @@ public class CachingClusteredClient implements QuerySegmentWalker {
           hasOnlyHistoricalSegments = false;
           break;
         }
-        hasher.putString(p.getServer().getSegment().getId().toString(), StandardCharsets.UTF_8);
+        hasher.putString(p.getServer().getSegment().getId().toString(), UTF_8);
         // it is important to add the "query interval" as part ETag calculation
         // to have result level cache work correctly for queries with different
         // intervals covering the same set of segments
-        hasher.putString(p.rhs.getInterval().toString(), StandardCharsets.UTF_8);
+        hasher.putString(p.rhs.getInterval().toString(), UTF_8);
       }
 
       if (!hasOnlyHistoricalSegments) {
@@ -799,8 +801,7 @@ public class CachingClusteredClient implements QuerySegmentWalker {
         return null;
       }
       hasher.putBytes(queryCacheKeyFinal);
-      String currEtag = StringUtils.encodeBase64String(hasher.hash().asBytes());
-      return currEtag;
+      return StringUtils.encodeBase64String(hasher.hash().asBytes());
     }
 
     /**
@@ -809,9 +810,9 @@ public class CachingClusteredClient implements QuerySegmentWalker {
      */
     @Nullable
     private byte[] computeQueryCacheKeyWithJoin() {
-      Preconditions.checkNotNull(strategy, "strategy cannot be null");
+      requireNonNull(strategy, "strategy cannot be null");
       byte[] dataSourceCacheKey = query.getDataSource().getCacheKey();
-      if (null == dataSourceCacheKey) {
+      if (dataSourceCacheKey == null) {
         return null;
       } else if (dataSourceCacheKey.length > 0) {
         return Bytes.concat(dataSourceCacheKey, strategy.computeCacheKey(query));
@@ -850,7 +851,7 @@ public class CachingClusteredClient implements QuerySegmentWalker {
         TimelineLookup<String, ServerSelector> timeline, SegmentDescriptor spec) {
       PartitionChunk<ServerSelector> chunk =
           timeline.findChunk(spec.getInterval(), spec.getVersion(), spec.getPartitionNumber());
-      if (null == chunk) {
+      if (chunk == null) {
         return null;
       }
       return new PartitionChunkEntry<>(spec.getInterval(), spec.getVersion(), chunk);
diff --git a/server/src/main/java/org/apache/druid/client/CachingQueryRunner.java b/server/src/main/java/org/apache/druid/client/CachingQueryRunner.java
index 582f5840c2..5fc2e13452 100644
--- a/server/src/main/java/org/apache/druid/client/CachingQueryRunner.java
+++ b/server/src/main/java/org/apache/druid/client/CachingQueryRunner.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.client;
 
+import static java.util.Collections.emptyIterator;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Function;
 import com.google.common.primitives.Bytes;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.Optional;
 import org.apache.druid.client.cache.Cache;
@@ -109,7 +110,7 @@ public class CachingQueryRunner<T> implements QueryRunner<T> {
                   public Iterator<T> make() {
                     try {
                       if (cachedResult.length == 0) {
-                        return Collections.emptyIterator();
+                        return emptyIterator();
                       }
 
                       return mapper.readValues(
@@ -128,8 +129,7 @@ public class CachingQueryRunner<T> implements QueryRunner<T> {
 
     if (populateCache) {
       final Function cacheFn = strategy.prepareForSegmentLevelCache();
-      return cachePopulator.wrap(
-          base.run(queryPlus, responseContext), value -> cacheFn.apply(value), cache, key);
+      return cachePopulator.wrap(base.run(queryPlus, responseContext), cacheFn::apply, cache, key);
     } else {
       return base.run(queryPlus, responseContext);
     }
diff --git a/server/src/main/java/org/apache/druid/client/DirectDruidClient.java b/server/src/main/java/org/apache/druid/client/DirectDruidClient.java
index cd8fba00bd..a77bd807fe 100644
--- a/server/src/main/java/org/apache/druid/client/DirectDruidClient.java
+++ b/server/src/main/java/org/apache/druid/client/DirectDruidClient.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.client;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.JavaType;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.dataformat.smile.SmileFactory;
 import com.fasterxml.jackson.jaxrs.smile.SmileMediaTypes;
-import com.google.common.base.Preconditions;
 import com.google.common.util.concurrent.FutureCallback;
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
@@ -209,8 +210,7 @@ public class DirectDruidClient<T> implements QueryRunner<T> {
               final long currentQueuedByteCount = queuedByteCount.addAndGet(-holder.getLength());
               if (usingBackpressure && currentQueuedByteCount < maxQueuedBytes) {
                 long backPressureTime =
-                    Preconditions.checkNotNull(
-                            trafficCopRef.get(), "No TrafficCop, how can this be?")
+                    requireNonNull(trafficCopRef.get(), "No TrafficCop, how can this be?")
                         .resume(holder.getChunkNum());
                 channelSuspendedTime.addAndGet(backPressureTime);
               }
diff --git a/server/src/main/java/org/apache/druid/client/DruidDataSource.java b/server/src/main/java/org/apache/druid/client/DruidDataSource.java
index d9d07eb760..a191b0303e 100644
--- a/server/src/main/java/org/apache/druid/client/DruidDataSource.java
+++ b/server/src/main/java/org/apache/druid/client/DruidDataSource.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.client;
 
+import static java.util.Collections.unmodifiableCollection;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
@@ -48,7 +49,7 @@ public class DruidDataSource {
   private final ConcurrentMap<SegmentId, DataSegment> idToSegmentMap = new ConcurrentHashMap<>();
 
   public DruidDataSource(String name, Map<String, String> properties) {
-    this.name = Preconditions.checkNotNull(name);
+    this.name = requireNonNull(name);
     this.properties = properties;
   }
 
@@ -68,7 +69,7 @@ public class DruidDataSource {
   }
 
   public Collection<DataSegment> getSegments() {
-    return Collections.unmodifiableCollection(idToSegmentMap.values());
+    return unmodifiableCollection(idToSegmentMap.values());
   }
 
   /** Removes segments for which the given filter returns true. */
diff --git a/server/src/main/java/org/apache/druid/client/FilteredBatchServerInventoryViewProvider.java b/server/src/main/java/org/apache/druid/client/FilteredBatchServerInventoryViewProvider.java
index 5d081ae111..db3adc5c5c 100644
--- a/server/src/main/java/org/apache/druid/client/FilteredBatchServerInventoryViewProvider.java
+++ b/server/src/main/java/org/apache/druid/client/FilteredBatchServerInventoryViewProvider.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Predicates.alwaysFalse;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Predicates;
 import javax.validation.constraints.NotNull;
 import org.apache.curator.framework.CuratorFramework;
 import org.apache.druid.server.initialization.ZkPathsConfig;
@@ -37,6 +38,6 @@ public class FilteredBatchServerInventoryViewProvider
   @Override
   public BatchServerInventoryView get() {
     return new BatchServerInventoryView(
-        zkPaths, curator, jsonMapper, Predicates.alwaysFalse(), "FilteredBatchServerInventoryView");
+        zkPaths, curator, jsonMapper, alwaysFalse(), "FilteredBatchServerInventoryView");
   }
 }
diff --git a/server/src/main/java/org/apache/druid/client/FilteredHttpServerInventoryViewProvider.java b/server/src/main/java/org/apache/druid/client/FilteredHttpServerInventoryViewProvider.java
index 3f75f6257d..427927666b 100644
--- a/server/src/main/java/org/apache/druid/client/FilteredHttpServerInventoryViewProvider.java
+++ b/server/src/main/java/org/apache/druid/client/FilteredHttpServerInventoryViewProvider.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Predicates.alwaysFalse;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Predicates;
 import javax.validation.constraints.NotNull;
 import org.apache.druid.discovery.DruidNodeDiscoveryProvider;
 import org.apache.druid.guice.annotations.EscalatedClient;
@@ -45,7 +46,7 @@ public class FilteredHttpServerInventoryViewProvider
         smileMapper,
         httpClient,
         druidNodeDiscoveryProvider,
-        Predicates.alwaysFalse(),
+        alwaysFalse(),
         config,
         "FilteredHttpServerInventoryView");
   }
diff --git a/server/src/main/java/org/apache/druid/client/FilteredServerInventoryViewProvider.java b/server/src/main/java/org/apache/druid/client/FilteredServerInventoryViewProvider.java
index d52e97f988..2206faa852 100644
--- a/server/src/main/java/org/apache/druid/client/FilteredServerInventoryViewProvider.java
+++ b/server/src/main/java/org/apache/druid/client/FilteredServerInventoryViewProvider.java
@@ -27,10 +27,9 @@ import com.google.inject.Provider;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = FilteredHttpServerInventoryViewProvider.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "batch", value = FilteredBatchServerInventoryViewProvider.class),
-      @JsonSubTypes.Type(name = "http", value = FilteredHttpServerInventoryViewProvider.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "batch", value = FilteredBatchServerInventoryViewProvider.class),
+  @JsonSubTypes.Type(name = "http", value = FilteredHttpServerInventoryViewProvider.class)
+})
 public interface FilteredServerInventoryViewProvider
     extends Provider<FilteredServerInventoryView> {}
diff --git a/server/src/main/java/org/apache/druid/client/HttpServerInventoryView.java b/server/src/main/java/org/apache/druid/client/HttpServerInventoryView.java
index 0f37c6914a..695fd40b44 100644
--- a/server/src/main/java/org/apache/druid/client/HttpServerInventoryView.java
+++ b/server/src/main/java/org/apache/druid/client/HttpServerInventoryView.java
@@ -19,15 +19,15 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Predicates.or;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Predicate;
-import com.google.common.base.Predicates;
 import com.google.common.collect.Collections2;
-import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Maps;
 import com.google.common.net.HostAndPort;
 import java.net.MalformedURLException;
@@ -236,7 +236,7 @@ public class HttpServerInventoryView implements ServerInventoryView, FilteredSer
     segmentCallbacks.put(filteringSegmentCallback, exec);
     segmentPredicates.put(filteringSegmentCallback, filter);
 
-    finalPredicate = Predicates.or(defaultFilter, Predicates.or(segmentPredicates.values()));
+    finalPredicate = or(defaultFilter, or(segmentPredicates.values()));
   }
 
   @Override
@@ -287,8 +287,7 @@ public class HttpServerInventoryView implements ServerInventoryView, FilteredSer
                   if (CallbackAction.UNREGISTER == fn.apply(entry.getKey())) {
                     segmentCallbacks.remove(entry.getKey());
                     if (segmentPredicates.remove(entry.getKey()) != null) {
-                      finalPredicate =
-                          Predicates.or(defaultFilter, Predicates.or(segmentPredicates.values()));
+                      finalPredicate = or(defaultFilter, or(segmentPredicates.values()));
                     }
                   }
                 }
@@ -398,7 +397,7 @@ public class HttpServerInventoryView implements ServerInventoryView, FilteredSer
    * purpose.
    */
   public Map<String, Object> getDebugInfo() {
-    Preconditions.checkArgument(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
+    checkArgument(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
 
     Map<String, Object> result = Maps.newHashMapWithExpectedSize(servers.size());
     for (Map.Entry<String, DruidServerHolder> e : servers.entrySet()) {
@@ -431,8 +430,7 @@ public class HttpServerInventoryView implements ServerInventoryView, FilteredSer
   @VisibleForTesting
   void syncMonitoring() {
     // Ensure that the collection is not being modified during iteration. Iterate over a copy
-    final Set<Map.Entry<String, DruidServerHolder>> serverEntrySet =
-        ImmutableSet.copyOf(servers.entrySet());
+    final Set<Map.Entry<String, DruidServerHolder>> serverEntrySet = servers.entrySet();
     for (Map.Entry<String, DruidServerHolder> e : serverEntrySet) {
       DruidServerHolder serverHolder = e.getValue();
       if (!serverHolder.syncer.isOK()) {
diff --git a/server/src/main/java/org/apache/druid/client/HttpServerInventoryViewConfig.java b/server/src/main/java/org/apache/druid/client/HttpServerInventoryViewConfig.java
index 96448cdc42..4a778d9b76 100644
--- a/server/src/main/java/org/apache/druid/client/HttpServerInventoryViewConfig.java
+++ b/server/src/main/java/org/apache/druid/client/HttpServerInventoryViewConfig.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.concurrent.TimeUnit;
 import org.joda.time.Period;
 
@@ -54,8 +55,8 @@ public class HttpServerInventoryViewConfig {
 
     this.numThreads = numThreads != null ? numThreads.intValue() : 5;
 
-    Preconditions.checkArgument(this.serverTimeout > 0, "server timeout must be > 0 ms");
-    Preconditions.checkArgument(this.numThreads > 1, "numThreads must be > 1");
+    checkArgument(this.serverTimeout > 0, "server timeout must be > 0 ms");
+    checkArgument(this.numThreads > 1, "numThreads must be > 1");
   }
 
   public long getServerTimeout() {
diff --git a/server/src/main/java/org/apache/druid/client/HttpServerInventoryViewProvider.java b/server/src/main/java/org/apache/druid/client/HttpServerInventoryViewProvider.java
index 692c3db232..e6cdac1a79 100644
--- a/server/src/main/java/org/apache/druid/client/HttpServerInventoryViewProvider.java
+++ b/server/src/main/java/org/apache/druid/client/HttpServerInventoryViewProvider.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Predicates.alwaysTrue;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Predicates;
 import javax.validation.constraints.NotNull;
 import org.apache.druid.discovery.DruidNodeDiscoveryProvider;
 import org.apache.druid.guice.annotations.EscalatedClient;
@@ -44,7 +45,7 @@ public class HttpServerInventoryViewProvider implements ServerInventoryViewProvi
         smileMapper,
         httpClient,
         druidNodeDiscoveryProvider,
-        Predicates.alwaysTrue(),
+        alwaysTrue(),
         config,
         "HttpServerInventoryView");
   }
diff --git a/server/src/main/java/org/apache/druid/client/ImmutableDruidDataSource.java b/server/src/main/java/org/apache/druid/client/ImmutableDruidDataSource.java
index ef221c1c3d..3566c780b1 100644
--- a/server/src/main/java/org/apache/druid/client/ImmutableDruidDataSource.java
+++ b/server/src/main/java/org/apache/druid/client/ImmutableDruidDataSource.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.client;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonIgnore;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSortedMap;
 import java.util.Collection;
@@ -49,7 +50,7 @@ public class ImmutableDruidDataSource {
    */
   public ImmutableDruidDataSource(
       String name, Map<String, String> properties, Map<SegmentId, DataSegment> idToSegments) {
-    this.name = Preconditions.checkNotNull(name);
+    this.name = requireNonNull(name);
     this.properties = ImmutableMap.copyOf(properties);
     this.idToSegments = ImmutableSortedMap.copyOf(idToSegments);
     this.totalSizeOfSegments = idToSegments.values().stream().mapToLong(DataSegment::getSize).sum();
@@ -60,7 +61,7 @@ public class ImmutableDruidDataSource {
       @JsonProperty("name") String name,
       @JsonProperty("properties") Map<String, String> properties,
       @JsonProperty("segments") Collection<DataSegment> segments) {
-    this.name = Preconditions.checkNotNull(name);
+    this.name = requireNonNull(name);
     this.properties = ImmutableMap.copyOf(properties);
 
     final ImmutableSortedMap.Builder<SegmentId, DataSegment> idToSegmentsBuilder =
diff --git a/server/src/main/java/org/apache/druid/client/ImmutableDruidServer.java b/server/src/main/java/org/apache/druid/client/ImmutableDruidServer.java
index 437078efd0..81c788cd15 100644
--- a/server/src/main/java/org/apache/druid/client/ImmutableDruidServer.java
+++ b/server/src/main/java/org/apache/druid/client/ImmutableDruidServer.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.client;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.collect.ImmutableMap;
 import java.util.Collection;
 import javax.annotation.Nullable;
@@ -47,7 +48,7 @@ public class ImmutableDruidServer {
       long currSize,
       ImmutableMap<String, ImmutableDruidDataSource> dataSources,
       int numSegments) {
-    this.metadata = Preconditions.checkNotNull(metadata);
+    this.metadata = requireNonNull(metadata);
     this.currSize = currSize;
     this.dataSources = dataSources;
     this.numSegments = numSegments;
diff --git a/server/src/main/java/org/apache/druid/client/ImmutableSegmentLoadInfo.java b/server/src/main/java/org/apache/druid/client/ImmutableSegmentLoadInfo.java
index 2543e2fd02..cbee756534 100644
--- a/server/src/main/java/org/apache/druid/client/ImmutableSegmentLoadInfo.java
+++ b/server/src/main/java/org/apache/druid/client/ImmutableSegmentLoadInfo.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableSet;
 import java.util.Set;
 import org.apache.druid.server.coordination.DruidServerMetadata;
@@ -35,8 +36,8 @@ public class ImmutableSegmentLoadInfo {
   public ImmutableSegmentLoadInfo(
       @JsonProperty("segment") DataSegment segment,
       @JsonProperty("servers") Set<DruidServerMetadata> servers) {
-    Preconditions.checkNotNull(segment, "segment");
-    Preconditions.checkNotNull(servers, "servers");
+    requireNonNull(segment, "segment");
+    requireNonNull(servers, "servers");
     this.segment = segment;
     this.servers = ImmutableSet.copyOf(servers);
   }
@@ -71,8 +72,7 @@ public class ImmutableSegmentLoadInfo {
   @Override
   public int hashCode() {
     int result = segment.hashCode();
-    result = 31 * result + servers.hashCode();
-    return result;
+    return 31 * result + servers.hashCode();
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/client/SegmentLoadInfo.java b/server/src/main/java/org/apache/druid/client/SegmentLoadInfo.java
index 48431759aa..43ed061cca 100644
--- a/server/src/main/java/org/apache/druid/client/SegmentLoadInfo.java
+++ b/server/src/main/java/org/apache/druid/client/SegmentLoadInfo.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.client;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.collect.Sets;
 import java.util.Set;
 import org.apache.druid.server.coordination.DruidServerMetadata;
@@ -31,7 +32,7 @@ public class SegmentLoadInfo implements Overshadowable<SegmentLoadInfo> {
   private final Set<DruidServerMetadata> servers;
 
   public SegmentLoadInfo(DataSegment segment) {
-    Preconditions.checkNotNull(segment, "segment");
+    requireNonNull(segment, "segment");
     this.segment = segment;
     this.servers = Sets.newConcurrentHashSet();
   }
@@ -72,8 +73,7 @@ public class SegmentLoadInfo implements Overshadowable<SegmentLoadInfo> {
   @Override
   public int hashCode() {
     int result = segment.hashCode();
-    result = 31 * result + servers.hashCode();
-    return result;
+    return 31 * result + servers.hashCode();
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/client/SegmentServerSelector.java b/server/src/main/java/org/apache/druid/client/SegmentServerSelector.java
index ae6083a1d5..7686f1e878 100644
--- a/server/src/main/java/org/apache/druid/client/SegmentServerSelector.java
+++ b/server/src/main/java/org/apache/druid/client/SegmentServerSelector.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.client;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import javax.annotation.Nullable;
 import org.apache.druid.client.selector.ServerSelector;
 import org.apache.druid.java.util.common.Pair;
@@ -40,14 +41,14 @@ public class SegmentServerSelector extends Pair<ServerSelector, SegmentDescripto
    */
   public SegmentServerSelector(ServerSelector server, SegmentDescriptor segment) {
     super(server, segment);
-    Preconditions.checkNotNull(server, "ServerSelector must not be null");
-    Preconditions.checkNotNull(segment, "SegmentDescriptor must not be null");
+    requireNonNull(server, "ServerSelector must not be null");
+    requireNonNull(segment, "SegmentDescriptor must not be null");
   }
 
   /** This is for a segment hosted locally */
   public SegmentServerSelector(SegmentDescriptor segment) {
     super(null, segment);
-    Preconditions.checkNotNull(segment, "SegmentDescriptor must not be null");
+    requireNonNull(segment, "SegmentDescriptor must not be null");
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/client/ServerInventoryViewProvider.java b/server/src/main/java/org/apache/druid/client/ServerInventoryViewProvider.java
index 279f51c372..0789464462 100644
--- a/server/src/main/java/org/apache/druid/client/ServerInventoryViewProvider.java
+++ b/server/src/main/java/org/apache/druid/client/ServerInventoryViewProvider.java
@@ -28,9 +28,8 @@ import com.google.inject.Provider;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = HttpServerInventoryViewProvider.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "batch", value = BatchServerInventoryViewProvider.class),
-      @JsonSubTypes.Type(name = "http", value = HttpServerInventoryViewProvider.class),
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "batch", value = BatchServerInventoryViewProvider.class),
+  @JsonSubTypes.Type(name = "http", value = HttpServerInventoryViewProvider.class)
+})
 public interface ServerInventoryViewProvider extends Provider<ServerInventoryView> {}
diff --git a/server/src/main/java/org/apache/druid/client/ServerViewUtil.java b/server/src/main/java/org/apache/druid/client/ServerViewUtil.java
index 5424b2cd12..8923f14f28 100644
--- a/server/src/main/java/org/apache/druid/client/ServerViewUtil.java
+++ b/server/src/main/java/org/apache/druid/client/ServerViewUtil.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.client;
 
+import static java.util.Collections.emptyList;
+
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Optional;
 import org.apache.druid.client.selector.ServerSelector;
@@ -55,7 +56,7 @@ public class ServerViewUtil {
     final Optional<? extends TimelineLookup<String, ServerSelector>> maybeTimeline =
         serverView.getTimeline(analysis);
     if (!maybeTimeline.isPresent()) {
-      return Collections.emptyList();
+      return emptyList();
     }
     List<LocatedSegmentDescriptor> located = new ArrayList<>();
     for (Interval interval : intervals) {
diff --git a/server/src/main/java/org/apache/druid/client/cache/BackgroundCachePopulator.java b/server/src/main/java/org/apache/druid/client/cache/BackgroundCachePopulator.java
index 66c7e9fadc..62b59d3a31 100644
--- a/server/src/main/java/org/apache/druid/client/cache/BackgroundCachePopulator.java
+++ b/server/src/main/java/org/apache/druid/client/cache/BackgroundCachePopulator.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.SerializerProvider;
-import com.google.common.base.Preconditions;
 import com.google.common.util.concurrent.FutureCallback;
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
@@ -58,9 +59,8 @@ public class BackgroundCachePopulator implements CachePopulator {
       final CachePopulatorStats cachePopulatorStats,
       final long maxEntrySize) {
     this.exec = MoreExecutors.listeningDecorator(exec);
-    this.objectMapper = Preconditions.checkNotNull(objectMapper, "objectMapper");
-    this.cachePopulatorStats =
-        Preconditions.checkNotNull(cachePopulatorStats, "cachePopulatorStats");
+    this.objectMapper = requireNonNull(objectMapper, "objectMapper");
+    this.cachePopulatorStats = requireNonNull(cachePopulatorStats, "cachePopulatorStats");
     this.maxEntrySize = maxEntrySize;
   }
 
diff --git a/server/src/main/java/org/apache/druid/client/cache/ByteCountingLRUMap.java b/server/src/main/java/org/apache/druid/client/cache/ByteCountingLRUMap.java
index 2c1d665817..05d4fe6a1c 100644
--- a/server/src/main/java/org/apache/druid/client/cache/ByteCountingLRUMap.java
+++ b/server/src/main/java/org/apache/druid/client/cache/ByteCountingLRUMap.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.Collections.unmodifiableSet;
+
 import java.nio.ByteBuffer;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.List;
@@ -110,7 +111,7 @@ class ByteCountingLRUMap extends LinkedHashMap<ByteBuffer, byte[]> {
    */
   @Override
   public Set<ByteBuffer> keySet() {
-    return Collections.unmodifiableSet(super.keySet());
+    return unmodifiableSet(super.keySet());
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/client/cache/BytesBoundedLinkedQueue.java b/server/src/main/java/org/apache/druid/client/cache/BytesBoundedLinkedQueue.java
index 97272d1f0b..ae0e9fedf2 100644
--- a/server/src/main/java/org/apache/druid/client/cache/BytesBoundedLinkedQueue.java
+++ b/server/src/main/java/org/apache/druid/client/cache/BytesBoundedLinkedQueue.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.client.cache;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Objects.requireNonNull;
+
 import java.util.AbstractQueue;
 import java.util.Collection;
 import java.util.Iterator;
@@ -55,9 +59,7 @@ public abstract class BytesBoundedLinkedQueue<E> extends AbstractQueue<E>
   }
 
   private static void checkNotNull(Object v) {
-    if (v == null) {
-      throw new NullPointerException();
-    }
+    requireNonNull(v);
   }
 
   private void checkSize(E e) {
@@ -190,12 +192,8 @@ public abstract class BytesBoundedLinkedQueue<E> extends AbstractQueue<E>
 
   @Override
   public int drainTo(Collection<? super E> c, int maxElements) {
-    if (c == null) {
-      throw new NullPointerException();
-    }
-    if (c == this) {
-      throw new IllegalArgumentException();
-    }
+    requireNonNull(c);
+    checkArgument(c != this);
     int n;
     takeLock.lock();
     try {
@@ -332,9 +330,7 @@ public abstract class BytesBoundedLinkedQueue<E> extends AbstractQueue<E>
     public void remove() {
       fullyLock();
       try {
-        if (this.lastReturned == null) {
-          throw new IllegalStateException();
-        }
+        checkState(this.lastReturned != null);
         delegate.remove();
         elementRemoved(lastReturned);
         signalNotFull();
diff --git a/server/src/main/java/org/apache/druid/client/cache/Cache.java b/server/src/main/java/org/apache/druid/client/cache/Cache.java
index 340a5a9458..773baeb044 100644
--- a/server/src/main/java/org/apache/druid/client/cache/Cache.java
+++ b/server/src/main/java/org/apache/druid/client/cache/Cache.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.client.cache;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+
 import java.io.Closeable;
 import java.nio.ByteBuffer;
 import java.util.Arrays;
@@ -61,8 +62,8 @@ public interface Cache extends Closeable {
     public final byte[] key;
 
     public NamedKey(String namespace, byte[] key) {
-      Preconditions.checkArgument(namespace != null, "namespace must not be null");
-      Preconditions.checkArgument(key != null, "key must not be null");
+      checkArgument(namespace != null, "namespace must not be null");
+      checkArgument(key != null, "key must not be null");
       this.namespace = namespace;
       this.key = key;
     }
@@ -100,8 +101,7 @@ public interface Cache extends Closeable {
     @Override
     public int hashCode() {
       int result = namespace.hashCode();
-      result = 31 * result + Arrays.hashCode(key);
-      return result;
+      return 31 * result + Arrays.hashCode(key);
     }
   }
 }
diff --git a/server/src/main/java/org/apache/druid/client/cache/CacheExecutorFactory.java b/server/src/main/java/org/apache/druid/client/cache/CacheExecutorFactory.java
index c54ea54c70..32337a7485 100644
--- a/server/src/main/java/org/apache/druid/client/cache/CacheExecutorFactory.java
+++ b/server/src/main/java/org/apache/druid/client/cache/CacheExecutorFactory.java
@@ -51,7 +51,7 @@ public enum CacheExecutorFactory {
 
   public abstract Executor createExecutor();
 
-  @JsonCreator
+  @JsonCreator(mode = JsonCreator.Mode.DELEGATING)
   public static CacheExecutorFactory from(String str) {
     return Enum.valueOf(CacheExecutorFactory.class, StringUtils.toUpperCase(str));
   }
diff --git a/server/src/main/java/org/apache/druid/client/cache/CacheProvider.java b/server/src/main/java/org/apache/druid/client/cache/CacheProvider.java
index 8599e24fa7..b6dc64966d 100644
--- a/server/src/main/java/org/apache/druid/client/cache/CacheProvider.java
+++ b/server/src/main/java/org/apache/druid/client/cache/CacheProvider.java
@@ -27,11 +27,10 @@ import com.google.inject.Provider;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = CaffeineCacheProvider.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "local", value = LocalCacheProvider.class),
-      @JsonSubTypes.Type(name = "memcached", value = MemcachedCacheProvider.class),
-      @JsonSubTypes.Type(name = "hybrid", value = HybridCacheProvider.class),
-      @JsonSubTypes.Type(name = "caffeine", value = CaffeineCacheProvider.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "caffeine", value = CaffeineCacheProvider.class),
+  @JsonSubTypes.Type(name = "hybrid", value = HybridCacheProvider.class),
+  @JsonSubTypes.Type(name = "local", value = LocalCacheProvider.class),
+  @JsonSubTypes.Type(name = "memcached", value = MemcachedCacheProvider.class)
+})
 public interface CacheProvider extends Provider<Cache> {}
diff --git a/server/src/main/java/org/apache/druid/client/cache/CaffeineCache.java b/server/src/main/java/org/apache/druid/client/cache/CaffeineCache.java
index ac976a5ea1..e2628f0c98 100644
--- a/server/src/main/java/org/apache/druid/client/cache/CaffeineCache.java
+++ b/server/src/main/java/org/apache/druid/client/cache/CaffeineCache.java
@@ -23,7 +23,6 @@ import com.github.benmanes.caffeine.cache.Cache;
 import com.github.benmanes.caffeine.cache.Caffeine;
 import com.github.benmanes.caffeine.cache.stats.CacheStats;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Maps;
 import java.nio.ByteBuffer;
 import java.util.Map;
@@ -99,7 +98,7 @@ public class CaffeineCache implements org.apache.druid.client.cache.Cache {
     // The assumption here is that every value is accessed at least once. Materializing here ensures
     // deserialize is only
     // called *once* per value.
-    return ImmutableMap.copyOf(Maps.transformValues(cache.getAllPresent(keys), this::deserialize));
+    return Maps.transformValues(cache.getAllPresent(keys), this::deserialize);
   }
 
   // This is completely racy with put. Any values missed should be evicted later anyways. So no
@@ -111,8 +110,8 @@ public class CaffeineCache implements org.apache.druid.client.cache.Cache {
     }
   }
 
-  @Override
   @LifecycleStop
+  @Override
   public void close() {
     cache.cleanUp();
   }
diff --git a/server/src/main/java/org/apache/druid/client/cache/ForegroundCachePopulator.java b/server/src/main/java/org/apache/druid/client/cache/ForegroundCachePopulator.java
index 87adc780ea..648b8a064f 100644
--- a/server/src/main/java/org/apache/druid/client/cache/ForegroundCachePopulator.java
+++ b/server/src/main/java/org/apache/druid/client/cache/ForegroundCachePopulator.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.SerializerProvider;
-import com.google.common.base.Preconditions;
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.util.function.Function;
@@ -49,9 +50,8 @@ public class ForegroundCachePopulator implements CachePopulator {
       final ObjectMapper objectMapper,
       final CachePopulatorStats cachePopulatorStats,
       final long maxEntrySize) {
-    this.objectMapper = Preconditions.checkNotNull(objectMapper, "objectMapper");
-    this.cachePopulatorStats =
-        Preconditions.checkNotNull(cachePopulatorStats, "cachePopulatorStats");
+    this.objectMapper = requireNonNull(objectMapper, "objectMapper");
+    this.cachePopulatorStats = requireNonNull(cachePopulatorStats, "cachePopulatorStats");
     this.maxEntrySize = maxEntrySize;
   }
 
diff --git a/server/src/main/java/org/apache/druid/client/cache/HybridCache.java b/server/src/main/java/org/apache/druid/client/cache/HybridCache.java
index ff64a13f05..e1142b94c5 100644
--- a/server/src/main/java/org/apache/druid/client/cache/HybridCache.java
+++ b/server/src/main/java/org/apache/druid/client/cache/HybridCache.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.Collections.emptyMap;
+
 import com.google.common.collect.Sets;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
@@ -115,7 +116,7 @@ public class HybridCache implements Cache {
     if (config.getUseL2()) {
       return level2.getBulk(keys);
     } else {
-      return Collections.emptyMap();
+      return emptyMap();
     }
   }
 
@@ -140,8 +141,8 @@ public class HybridCache implements Cache {
     }
   }
 
-  @Override
   @LifecycleStop
+  @Override
   public void close() throws IOException {
     CloseableUtils.closeAll(level1, level2);
   }
diff --git a/server/src/main/java/org/apache/druid/client/cache/HybridCacheProvider.java b/server/src/main/java/org/apache/druid/client/cache/HybridCacheProvider.java
index 49dc6d5152..ce652a5452 100644
--- a/server/src/main/java/org/apache/druid/client/cache/HybridCacheProvider.java
+++ b/server/src/main/java/org/apache/druid/client/cache/HybridCacheProvider.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.client.cache;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
-import com.google.common.base.Preconditions;
 import com.google.inject.name.Named;
 
 public class HybridCacheProvider extends HybridCacheConfig implements CacheProvider {
@@ -32,13 +34,12 @@ public class HybridCacheProvider extends HybridCacheConfig implements CacheProvi
   public HybridCacheProvider(
       @JacksonInject @Named("l1") CacheProvider level1,
       @JacksonInject @Named("l2") CacheProvider level2) {
-    this.level1 = Preconditions.checkNotNull(level1, "l1 cache not specified for hybrid cache");
-    this.level2 = Preconditions.checkNotNull(level2, "l2 cache not specified for hybrid cache");
-    if (!getUseL2() && !getPopulateL2()) {
-      throw new IllegalStateException(
-          "Doesn't make sense to use Hybrid cache with both use and populate disabled for L2, "
-              + "use just L1 cache in this case");
-    }
+    this.level1 = requireNonNull(level1, "l1 cache not specified for hybrid cache");
+    this.level2 = requireNonNull(level2, "l2 cache not specified for hybrid cache");
+    checkState(
+        getUseL2() || getPopulateL2(),
+        "Doesn't make sense to use Hybrid cache with both use and populate disabled for L2, "
+            + "use just L1 cache in this case");
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/client/cache/LZ4Transcoder.java b/server/src/main/java/org/apache/druid/client/cache/LZ4Transcoder.java
index 3c85c8090f..b45775ab26 100644
--- a/server/src/main/java/org/apache/druid/client/cache/LZ4Transcoder.java
+++ b/server/src/main/java/org/apache/druid/client/cache/LZ4Transcoder.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.Objects.requireNonNull;
+
 import java.nio.ByteBuffer;
 import net.jpountz.lz4.LZ4Compressor;
 import net.jpountz.lz4.LZ4Factory;
@@ -39,9 +41,7 @@ public class LZ4Transcoder extends SerializingTranscoder {
 
   @Override
   protected byte[] compress(byte[] in) {
-    if (in == null) {
-      throw new NullPointerException("Can't compress null");
-    }
+    requireNonNull(in, "Can't compress null");
 
     LZ4Compressor compressor = lz4Factory.fastCompressor();
 
diff --git a/server/src/main/java/org/apache/druid/client/cache/MapCache.java b/server/src/main/java/org/apache/druid/client/cache/MapCache.java
index ec345f2590..6a213ca49f 100644
--- a/server/src/main/java/org/apache/druid/client/cache/MapCache.java
+++ b/server/src/main/java/org/apache/druid/client/cache/MapCache.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.Collections.synchronizedMap;
+
 import com.google.common.primitives.Ints;
 import java.nio.ByteBuffer;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -51,7 +52,7 @@ public class MapCache implements Cache {
 
   MapCache(ByteCountingLRUMap byteCountingLRUMap) {
     this.byteCountingLRUMap = byteCountingLRUMap;
-    this.baseMap = Collections.synchronizedMap(byteCountingLRUMap);
+    this.baseMap = synchronizedMap(byteCountingLRUMap);
 
     namespaceId = new HashMap<>();
     ids = new AtomicInteger();
@@ -132,8 +133,8 @@ public class MapCache implements Cache {
     }
   }
 
-  @Override
   @LifecycleStop
+  @Override
   public void close() {
     baseMap.clear();
     byteCountingLRUMap.clear();
diff --git a/server/src/main/java/org/apache/druid/client/cache/MemcacheClientPool.java b/server/src/main/java/org/apache/druid/client/cache/MemcacheClientPool.java
index 1f4919969e..b806e61421 100644
--- a/server/src/main/java/org/apache/druid/client/cache/MemcacheClientPool.java
+++ b/server/src/main/java/org/apache/druid/client/cache/MemcacheClientPool.java
@@ -19,8 +19,10 @@
 
 package org.apache.druid.client.cache;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Supplier;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
@@ -55,8 +57,8 @@ final class MemcacheClientPool implements Supplier<ResourceHolder<MemcachedClien
   private final CountingHolder[] connections;
 
   MemcacheClientPool(int capacity, Supplier<MemcachedClientIF> generator) {
-    Preconditions.checkArgument(capacity > 0, "capacity must be greater than 0");
-    Preconditions.checkNotNull(generator);
+    checkArgument(capacity > 0, "capacity must be greater than 0");
+    requireNonNull(generator);
 
     CountingHolder[] connections = new CountingHolder[capacity];
     // eagerly instantiate all items in the pool
diff --git a/server/src/main/java/org/apache/druid/client/cache/MemcachedCache.java b/server/src/main/java/org/apache/druid/client/cache/MemcachedCache.java
index 6e6630afde..36ebd117fa 100644
--- a/server/src/main/java/org/apache/druid/client/cache/MemcachedCache.java
+++ b/server/src/main/java/org/apache/druid/client/cache/MemcachedCache.java
@@ -19,8 +19,11 @@
 
 package org.apache.druid.client.cache;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkState;
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Predicate;
 import com.google.common.base.Supplier;
 import com.google.common.base.Suppliers;
@@ -32,7 +35,6 @@ import com.google.common.hash.Hashing;
 import java.io.IOException;
 import java.net.InetSocketAddress;
 import java.nio.ByteBuffer;
-import java.nio.charset.StandardCharsets;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
@@ -85,7 +87,7 @@ public class MemcachedCache implements Cache {
 
         @Override
         public long hash(String k) {
-          return fn.hashString(k, StandardCharsets.UTF_8).asLong();
+          return fn.hashString(k, UTF_8).asLong();
         }
 
         @Override
@@ -389,7 +391,7 @@ public class MemcachedCache implements Cache {
       Supplier<ResourceHolder<MemcachedClientIF>> client,
       MemcachedCacheConfig config,
       AbstractMonitor monitor) {
-    Preconditions.checkArgument(
+    checkArgument(
         config.getMemcachedPrefix().length() <= MAX_PREFIX_LENGTH,
         "memcachedPrefix length [%s] exceeds maximum length [%s]",
         config.getMemcachedPrefix().length(),
@@ -473,7 +475,7 @@ public class MemcachedCache implements Cache {
     byte[] value = new byte[buf.remaining()];
     buf.get(value);
 
-    Preconditions.checkState(
+    checkState(
         Arrays.equals(keyBytes, key.toByteArray()), "Keys do not match, possible hash collision?");
     return value;
   }
@@ -538,8 +540,8 @@ public class MemcachedCache implements Cache {
     // no resources to cleanup
   }
 
-  @Override
   @LifecycleStop
+  @Override
   public void close() {
     monitor.stop();
   }
diff --git a/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpec.java b/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpec.java
index 550b1e316f..6289e03bd8 100644
--- a/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpec.java
+++ b/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionIntervalSpec.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.client.indexing;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import java.util.List;
 import java.util.Objects;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.java.util.common.IAE;
 import org.apache.druid.java.util.common.JodaUtils;
@@ -49,7 +50,7 @@ public class ClientCompactionIntervalSpec {
       List<DataSegment> segments, @Nullable Granularity segmentGranularity) {
     Interval interval =
         JodaUtils.umbrellaInterval(
-            segments.stream().map(DataSegment::getInterval).collect(Collectors.toList()));
+            segments.stream().map(DataSegment::getInterval).collect(toList()));
     LOGGER.info(
         "Original umbrella interval %s in compaction task for datasource %s",
         interval, segments.get(0).getDataSource());
@@ -111,8 +112,8 @@ public class ClientCompactionIntervalSpec {
     return interval;
   }
 
-  @Nullable
   @JsonProperty
+  @Nullable
   public String getSha256OfSortedSegmentIds() {
     return sha256OfSortedSegmentIds;
   }
diff --git a/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionTaskDimensionsSpec.java b/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionTaskDimensionsSpec.java
index 54c8008029..e8d403db69 100644
--- a/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionTaskDimensionsSpec.java
+++ b/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionTaskDimensionsSpec.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.client.indexing;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import java.util.List;
 import java.util.Objects;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.data.input.impl.DimensionSchema;
 import org.apache.druid.java.util.common.parsers.ParserUtils;
@@ -43,14 +44,14 @@ public class ClientCompactionTaskDimensionsSpec {
       @Nullable @JsonProperty("dimensions") List<DimensionSchema> dimensions) {
     if (dimensions != null) {
       List<String> dimensionNames =
-          dimensions.stream().map(DimensionSchema::getName).collect(Collectors.toList());
+          dimensions.stream().map(DimensionSchema::getName).collect(toList());
       ParserUtils.validateFields(dimensionNames);
     }
     this.dimensions = dimensions;
   }
 
-  @Nullable
   @JsonProperty
+  @Nullable
   public List<DimensionSchema> getDimensions() {
     return dimensions;
   }
diff --git a/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionTaskQuery.java b/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionTaskQuery.java
index ea9b900262..1aa76a9f84 100644
--- a/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionTaskQuery.java
+++ b/server/src/main/java/org/apache/druid/client/indexing/ClientCompactionTaskQuery.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client.indexing;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.Arrays;
 import java.util.Map;
 import java.util.Objects;
@@ -57,7 +58,7 @@ public class ClientCompactionTaskQuery implements ClientTaskQuery {
       @JsonProperty("metricsSpec") AggregatorFactory[] metrics,
       @JsonProperty("transformSpec") ClientCompactionTaskTransformSpec transformSpec,
       @JsonProperty("context") Map<String, Object> context) {
-    this.id = Preconditions.checkNotNull(id, "id");
+    this.id = requireNonNull(id, "id");
     this.dataSource = dataSource;
     this.ioConfig = ioConfig;
     this.tuningConfig = tuningConfig;
@@ -154,8 +155,7 @@ public class ClientCompactionTaskQuery implements ClientTaskQuery {
             dimensionsSpec,
             transformSpec,
             context);
-    result = 31 * result + Arrays.hashCode(metricsSpec);
-    return result;
+    return 31 * result + Arrays.hashCode(metricsSpec);
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/client/indexing/ClientKillUnusedSegmentsTaskQuery.java b/server/src/main/java/org/apache/druid/client/indexing/ClientKillUnusedSegmentsTaskQuery.java
index 11dbbf2a2d..3cf89a9518 100644
--- a/server/src/main/java/org/apache/druid/client/indexing/ClientKillUnusedSegmentsTaskQuery.java
+++ b/server/src/main/java/org/apache/druid/client/indexing/ClientKillUnusedSegmentsTaskQuery.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client.indexing;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.Objects;
 import org.joda.time.Interval;
 
@@ -45,7 +46,7 @@ public class ClientKillUnusedSegmentsTaskQuery implements ClientTaskQuery {
       @JsonProperty("dataSource") String dataSource,
       @JsonProperty("interval") Interval interval,
       @JsonProperty("markAsUnused") Boolean markAsUnused) {
-    this.id = Preconditions.checkNotNull(id, "id");
+    this.id = requireNonNull(id, "id");
     this.dataSource = dataSource;
     this.interval = interval;
     this.markAsUnused = markAsUnused;
diff --git a/server/src/main/java/org/apache/druid/client/indexing/ClientTaskQuery.java b/server/src/main/java/org/apache/druid/client/indexing/ClientTaskQuery.java
index f4b766a89d..b6b1e03680 100644
--- a/server/src/main/java/org/apache/druid/client/indexing/ClientTaskQuery.java
+++ b/server/src/main/java/org/apache/druid/client/indexing/ClientTaskQuery.java
@@ -33,13 +33,12 @@ import com.fasterxml.jackson.annotation.JsonTypeInfo;
  * org.apache.druid.indexing.common.task.Task objects.
  */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(
-    value = {
-      @Type(
-          name = ClientKillUnusedSegmentsTaskQuery.TYPE,
-          value = ClientKillUnusedSegmentsTaskQuery.class),
-      @Type(name = ClientCompactionTaskQuery.TYPE, value = ClientCompactionTaskQuery.class)
-    })
+@JsonSubTypes({
+  @Type(name = ClientCompactionTaskQuery.TYPE, value = ClientCompactionTaskQuery.class),
+  @Type(
+      name = ClientKillUnusedSegmentsTaskQuery.TYPE,
+      value = ClientKillUnusedSegmentsTaskQuery.class)
+})
 public interface ClientTaskQuery {
   String getId();
 
diff --git a/server/src/main/java/org/apache/druid/client/indexing/HttpIndexingServiceClient.java b/server/src/main/java/org/apache/druid/client/indexing/HttpIndexingServiceClient.java
index a98f46de32..a4688d9158 100644
--- a/server/src/main/java/org/apache/druid/client/indexing/HttpIndexingServiceClient.java
+++ b/server/src/main/java/org/apache/druid/client/indexing/HttpIndexingServiceClient.java
@@ -19,16 +19,20 @@
 
 package org.apache.druid.client.indexing;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Collections.emptyMap;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Strings;
 import com.google.common.collect.Iterables;
 import com.google.inject.Inject;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -86,10 +90,10 @@ public class HttpIndexingServiceClient implements IndexingServiceClient {
       @Nullable ClientCompactionTaskTransformSpec transformSpec,
       @Nullable Boolean dropExisting,
       @Nullable Map<String, Object> context) {
-    Preconditions.checkArgument(!segments.isEmpty(), "Expect non-empty segments to compact");
+    checkArgument(!segments.isEmpty(), "Expect non-empty segments to compact");
 
     final String dataSource = segments.get(0).getDataSource();
-    Preconditions.checkArgument(
+    checkArgument(
         segments.stream().allMatch(segment -> segment.getDataSource().equals(dataSource)),
         "Segments must have the same dataSource");
 
@@ -141,7 +145,7 @@ public class HttpIndexingServiceClient implements IndexingServiceClient {
           jsonMapper.readValue(
               response.getContent(), JacksonUtils.TYPE_REFERENCE_MAP_STRING_OBJECT);
       final String returnedTaskId = (String) resultMap.get("task");
-      Preconditions.checkState(
+      checkState(
           taskId.equals(returnedTaskId),
           "Got a different taskId[%s]. Expected taskId[%s]",
           returnedTaskId,
@@ -170,8 +174,8 @@ public class HttpIndexingServiceClient implements IndexingServiceClient {
           jsonMapper.readValue(
               response.getContent(), JacksonUtils.TYPE_REFERENCE_MAP_STRING_OBJECT);
       final String cancelledTaskId = (String) resultMap.get("task");
-      Preconditions.checkNotNull(cancelledTaskId, "Null task id returned for task[%s]", taskId);
-      Preconditions.checkState(
+      checkNotNull(cancelledTaskId, "Null task id returned for task[%s]", taskId);
+      checkState(
           taskId.equals(cancelledTaskId),
           "Requested to cancel task[%s], but another task[%s] was cancelled!",
           taskId,
@@ -289,8 +293,8 @@ public class HttpIndexingServiceClient implements IndexingServiceClient {
     }
   }
 
-  @Override
   @Nullable
+  @Override
   public TaskStatusPlus getLastCompleteTask() {
     final List<TaskStatusPlus> completeTaskStatuses = getTasks("completeTasks?n=1");
     return completeTaskStatuses.isEmpty() ? null : completeTaskStatuses.get(0);
@@ -323,7 +327,7 @@ public class HttpIndexingServiceClient implements IndexingServiceClient {
                   StringUtils.format(
                       "/druid/indexer/v1/task/%s/reports", StringUtils.urlEncode(taskId))));
 
-      if (responseHolder.getContent().length() == 0
+      if (responseHolder.getContent().isEmpty()
           || !HttpResponseStatus.OK.equals(responseHolder.getStatus())) {
         if (responseHolder.getStatus() == HttpResponseStatus.NOT_FOUND) {
           log.info(
@@ -357,7 +361,7 @@ public class HttpIndexingServiceClient implements IndexingServiceClient {
       final Map<String, List<Interval>> response =
           jsonMapper.readValue(
               responseHolder.getContent(), new TypeReference<Map<String, List<Interval>>>() {});
-      return response == null ? Collections.emptyMap() : response;
+      return response == null ? emptyMap() : response;
     } catch (IOException | InterruptedException e) {
       throw new RuntimeException(e);
     }
@@ -411,7 +415,7 @@ public class HttpIndexingServiceClient implements IndexingServiceClient {
               responseHolder.getContent(), JacksonUtils.TYPE_REFERENCE_MAP_STRING_OBJECT);
 
       final Object numDeletedObject = resultMap.get("numDeleted");
-      return (Integer) Preconditions.checkNotNull(numDeletedObject, "numDeletedObject");
+      return (Integer) requireNonNull(numDeletedObject, "numDeletedObject");
     } catch (Exception e) {
       throw new RuntimeException(e);
     }
diff --git a/server/src/main/java/org/apache/druid/client/indexing/QueryStatus.java b/server/src/main/java/org/apache/druid/client/indexing/QueryStatus.java
index 09d7b17515..5cb962ee73 100644
--- a/server/src/main/java/org/apache/druid/client/indexing/QueryStatus.java
+++ b/server/src/main/java/org/apache/druid/client/indexing/QueryStatus.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.client.indexing;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonIgnore;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.base.MoreObjects;
-import com.google.common.base.Preconditions;
 
 /** Should be synchronized with org.apache.druid.indexing.common.TaskStatus. */
 public class QueryStatus {
@@ -42,8 +43,8 @@ public class QueryStatus {
       @JsonProperty("id") String id,
       @JsonProperty("status") Status status,
       @JsonProperty("duration") long duration) {
-    this.id = Preconditions.checkNotNull(id, "id");
-    this.status = Preconditions.checkNotNull(status, "status");
+    this.id = requireNonNull(id, "id");
+    this.status = requireNonNull(status, "status");
     this.duration = duration;
   }
 
diff --git a/server/src/main/java/org/apache/druid/client/selector/ConnectionCountServerSelectorStrategy.java b/server/src/main/java/org/apache/druid/client/selector/ConnectionCountServerSelectorStrategy.java
index d29e7a8470..1fd25f3998 100644
--- a/server/src/main/java/org/apache/druid/client/selector/ConnectionCountServerSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/client/selector/ConnectionCountServerSelectorStrategy.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.client.selector;
 
+import static java.util.Comparator.comparingInt;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Ordering;
 import java.util.Collections;
@@ -31,8 +33,7 @@ import org.apache.druid.timeline.DataSegment;
 
 public class ConnectionCountServerSelectorStrategy implements ServerSelectorStrategy {
   private static final Comparator<QueryableDruidServer> COMPARATOR =
-      Comparator.comparingInt(
-          s -> ((DirectDruidClient) s.getQueryRunner()).getNumOpenConnections());
+      comparingInt(s -> ((DirectDruidClient) s.getQueryRunner()).getNumOpenConnections());
 
   @Nullable
   @Override
diff --git a/server/src/main/java/org/apache/druid/client/selector/HighestPriorityTierSelectorStrategy.java b/server/src/main/java/org/apache/druid/client/selector/HighestPriorityTierSelectorStrategy.java
index 0867ee41f2..02d0fdcd78 100644
--- a/server/src/main/java/org/apache/druid/client/selector/HighestPriorityTierSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/client/selector/HighestPriorityTierSelectorStrategy.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.client.selector;
 
+import static java.util.Comparator.reverseOrder;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import java.util.Comparator;
@@ -33,6 +35,6 @@ public class HighestPriorityTierSelectorStrategy extends AbstractTierSelectorStr
 
   @Override
   public Comparator<Integer> getComparator() {
-    return Comparator.reverseOrder();
+    return reverseOrder();
   }
 }
diff --git a/server/src/main/java/org/apache/druid/client/selector/LowestPriorityTierSelectorStrategy.java b/server/src/main/java/org/apache/druid/client/selector/LowestPriorityTierSelectorStrategy.java
index a1eb8358ab..c1a68df3ae 100644
--- a/server/src/main/java/org/apache/druid/client/selector/LowestPriorityTierSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/client/selector/LowestPriorityTierSelectorStrategy.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.client.selector;
 
+import static java.util.Comparator.naturalOrder;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import java.util.Comparator;
@@ -33,6 +35,6 @@ public class LowestPriorityTierSelectorStrategy extends AbstractTierSelectorStra
 
   @Override
   public Comparator<Integer> getComparator() {
-    return Comparator.naturalOrder();
+    return naturalOrder();
   }
 }
diff --git a/server/src/main/java/org/apache/druid/client/selector/RandomServerSelectorStrategy.java b/server/src/main/java/org/apache/druid/client/selector/RandomServerSelectorStrategy.java
index c55b887e19..d9cbe87006 100644
--- a/server/src/main/java/org/apache/druid/client/selector/RandomServerSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/client/selector/RandomServerSelectorStrategy.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.client.selector;
 
+import static java.util.Collections.shuffle;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterators;
-import com.google.common.collect.Lists;
-import java.util.Collections;
+import java.util.ArrayList;
 import java.util.List;
 import java.util.Set;
 import java.util.concurrent.ThreadLocalRandom;
@@ -42,8 +43,8 @@ public class RandomServerSelectorStrategy implements ServerSelectorStrategy {
     if (servers.size() <= numServersToPick) {
       return ImmutableList.copyOf(servers);
     }
-    List<QueryableDruidServer> list = Lists.newArrayList(servers);
-    Collections.shuffle(list, ThreadLocalRandom.current());
-    return ImmutableList.copyOf(list.subList(0, numServersToPick));
+    List<QueryableDruidServer> list = new ArrayList<>(servers);
+    shuffle(list, ThreadLocalRandom.current());
+    return list.subList(0, numServersToPick);
   }
 }
diff --git a/server/src/main/java/org/apache/druid/client/selector/ServerSelectorStrategy.java b/server/src/main/java/org/apache/druid/client/selector/ServerSelectorStrategy.java
index d9cc5830c2..5bdd23a882 100644
--- a/server/src/main/java/org/apache/druid/client/selector/ServerSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/client/selector/ServerSelectorStrategy.java
@@ -32,13 +32,10 @@ import org.apache.druid.timeline.DataSegment;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = RandomServerSelectorStrategy.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "random", value = RandomServerSelectorStrategy.class),
-      @JsonSubTypes.Type(
-          name = "connectionCount",
-          value = ConnectionCountServerSelectorStrategy.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "connectionCount", value = ConnectionCountServerSelectorStrategy.class),
+  @JsonSubTypes.Type(name = "random", value = RandomServerSelectorStrategy.class)
+})
 public interface ServerSelectorStrategy {
   @Nullable
   default <T> QueryableDruidServer pick(
diff --git a/server/src/main/java/org/apache/druid/client/selector/TierSelectorStrategy.java b/server/src/main/java/org/apache/druid/client/selector/TierSelectorStrategy.java
index 7ab1bd8e6c..6b24f4d67b 100644
--- a/server/src/main/java/org/apache/druid/client/selector/TierSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/client/selector/TierSelectorStrategy.java
@@ -34,14 +34,11 @@ import org.apache.druid.timeline.DataSegment;
     use = JsonTypeInfo.Id.NAME,
     property = "tier",
     defaultImpl = HighestPriorityTierSelectorStrategy.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(
-          name = "highestPriority",
-          value = HighestPriorityTierSelectorStrategy.class),
-      @JsonSubTypes.Type(name = "lowestPriority", value = LowestPriorityTierSelectorStrategy.class),
-      @JsonSubTypes.Type(name = "custom", value = CustomTierSelectorStrategy.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "custom", value = CustomTierSelectorStrategy.class),
+  @JsonSubTypes.Type(name = "highestPriority", value = HighestPriorityTierSelectorStrategy.class),
+  @JsonSubTypes.Type(name = "lowestPriority", value = LowestPriorityTierSelectorStrategy.class)
+})
 public interface TierSelectorStrategy {
   Comparator<Integer> getComparator();
 
diff --git a/server/src/main/java/org/apache/druid/curator/CuratorConfig.java b/server/src/main/java/org/apache/druid/curator/CuratorConfig.java
index 817e17439d..7420ee736d 100644
--- a/server/src/main/java/org/apache/druid/curator/CuratorConfig.java
+++ b/server/src/main/java/org/apache/druid/curator/CuratorConfig.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.curator;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import javax.validation.constraints.Min;
 import org.apache.druid.metadata.DefaultPasswordProvider;
 import org.apache.druid.metadata.PasswordProvider;
@@ -98,7 +99,7 @@ public class CuratorConfig {
   }
 
   public void setEnableCompression(Boolean enableCompression) {
-    Preconditions.checkNotNull(enableCompression, "enableCompression");
+    requireNonNull(enableCompression, "enableCompression");
     this.enableCompression = enableCompression;
   }
 
@@ -107,7 +108,7 @@ public class CuratorConfig {
   }
 
   public void setEnableAcl(Boolean enableAcl) {
-    Preconditions.checkNotNull(enableAcl, "enableAcl");
+    requireNonNull(enableAcl, "enableAcl");
     this.enableAcl = enableAcl;
   }
 
diff --git a/server/src/main/java/org/apache/druid/curator/CuratorModule.java b/server/src/main/java/org/apache/druid/curator/CuratorModule.java
index f53e67c7f1..b6c08cae07 100644
--- a/server/src/main/java/org/apache/druid/curator/CuratorModule.java
+++ b/server/src/main/java/org/apache/druid/curator/CuratorModule.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.curator;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.inject.Binder;
 import com.google.inject.Module;
 import com.google.inject.Provides;
-import java.nio.charset.StandardCharsets;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
 import org.apache.curator.RetryPolicy;
@@ -74,8 +75,7 @@ public class CuratorModule implements Module {
     if (!Strings.isNullOrEmpty(config.getZkUser()) && !Strings.isNullOrEmpty(config.getZkPwd())) {
       builder.authorization(
           config.getAuthScheme(),
-          StringUtils.format("%s:%s", config.getZkUser(), config.getZkPwd())
-              .getBytes(StandardCharsets.UTF_8));
+          StringUtils.format("%s:%s", config.getZkUser(), config.getZkPwd()).getBytes(UTF_8));
     }
 
     RetryPolicy retryPolicy =
@@ -94,8 +94,8 @@ public class CuratorModule implements Module {
   }
 
   /** Provide the Curator framework via Guice, integrated with the Druid lifecycle. */
-  @Provides
   @LazySingleton
+  @Provides
   public CuratorFramework makeCurator(
       ZkEnablementConfig zkEnablementConfig, CuratorConfig config, Lifecycle lifecycle) {
     if (!zkEnablementConfig.isEnabled()) {
diff --git a/server/src/main/java/org/apache/druid/curator/CuratorUtils.java b/server/src/main/java/org/apache/druid/curator/CuratorUtils.java
index 85cbf84d5d..54b7a6cb6b 100644
--- a/server/src/main/java/org/apache/druid/curator/CuratorUtils.java
+++ b/server/src/main/java/org/apache/druid/curator/CuratorUtils.java
@@ -113,6 +113,6 @@ public class CuratorUtils {
   }
 
   public static boolean isChildAdded(PathChildrenCacheEvent event) {
-    return event.getType().equals(PathChildrenCacheEvent.Type.CHILD_ADDED);
+    return event.getType() == PathChildrenCacheEvent.Type.CHILD_ADDED;
   }
 }
diff --git a/server/src/main/java/org/apache/druid/curator/discovery/CuratorDruidLeaderSelector.java b/server/src/main/java/org/apache/druid/curator/discovery/CuratorDruidLeaderSelector.java
index fab4439469..e7bf910cb7 100644
--- a/server/src/main/java/org/apache/druid/curator/discovery/CuratorDruidLeaderSelector.java
+++ b/server/src/main/java/org/apache/druid/curator/discovery/CuratorDruidLeaderSelector.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.curator.discovery;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.ThreadLocalRandom;
 import java.util.concurrent.atomic.AtomicReference;
@@ -170,7 +171,7 @@ public class CuratorDruidLeaderSelector implements DruidLeaderSelector {
 
   @Override
   public void registerListener(DruidLeaderSelector.Listener listener) {
-    Preconditions.checkArgument(listener != null, "listener is null.");
+    checkArgument(listener != null, "listener is null.");
 
     if (!lifecycleLock.canStart()) {
       throw new ISE("can't start.");
diff --git a/server/src/main/java/org/apache/druid/curator/discovery/CuratorDruidNodeDiscoveryProvider.java b/server/src/main/java/org/apache/druid/curator/discovery/CuratorDruidNodeDiscoveryProvider.java
index e1075296ee..90248d448e 100644
--- a/server/src/main/java/org/apache/druid/curator/discovery/CuratorDruidNodeDiscoveryProvider.java
+++ b/server/src/main/java/org/apache/druid/curator/discovery/CuratorDruidNodeDiscoveryProvider.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.curator.discovery;
 
+import static com.google.common.base.Preconditions.checkState;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.errorprone.annotations.concurrent.GuardedBy;
 import com.google.inject.Inject;
 import java.io.Closeable;
@@ -85,7 +86,7 @@ public class CuratorDruidNodeDiscoveryProvider extends DruidNodeDiscoveryProvide
 
   @Override
   public BooleanSupplier getForNode(DruidNode node, NodeRole nodeRole) {
-    Preconditions.checkState(lifecycleLock.isStarted());
+    checkState(lifecycleLock.isStarted());
     log.debug("Creating a NodeDiscoverer for node [%s] and role [%s]", node, nodeRole);
     NodeDiscoverer nodeDiscoverer =
         new NodeDiscoverer(config, jsonMapper, curatorFramework, node, nodeRole);
@@ -95,7 +96,7 @@ public class CuratorDruidNodeDiscoveryProvider extends DruidNodeDiscoveryProvide
 
   @Override
   public DruidNodeDiscovery getForNodeRole(NodeRole nodeRole) {
-    Preconditions.checkState(lifecycleLock.isStarted());
+    checkState(lifecycleLock.isStarted());
 
     return nodeRoleWatchers.computeIfAbsent(
         nodeRole,
diff --git a/server/src/main/java/org/apache/druid/curator/discovery/DiscoveryModule.java b/server/src/main/java/org/apache/druid/curator/discovery/DiscoveryModule.java
index f9a2c3ec80..9402d93782 100644
--- a/server/src/main/java/org/apache/druid/curator/discovery/DiscoveryModule.java
+++ b/server/src/main/java/org/apache/druid/curator/discovery/DiscoveryModule.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.curator.discovery;
 
+import static java.util.Collections.emptyList;
+
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Binder;
 import com.google.inject.Inject;
@@ -33,7 +35,6 @@ import com.google.inject.name.Names;
 import java.lang.annotation.Annotation;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Properties;
 import java.util.Set;
@@ -221,9 +222,9 @@ public class DiscoveryModule implements Module {
         .in(LazySingleton.class);
   }
 
-  @Provides
   @LazySingleton
   @Named(NAME)
+  @Provides
   public CuratorServiceAnnouncer getServiceAnnouncer(
       final CuratorServiceAnnouncer announcer,
       final Injector injector,
@@ -262,8 +263,8 @@ public class DiscoveryModule implements Module {
     return announcer;
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public ServiceDiscovery<Void> getServiceDiscovery(
       CuratorFramework curator, CuratorDiscoveryConfig config, Lifecycle lifecycle)
       throws Exception {
@@ -297,8 +298,8 @@ public class DiscoveryModule implements Module {
     return serviceDiscovery;
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public ServerDiscoveryFactory getServerDiscoveryFactory(ServiceDiscovery<Void> serviceDiscovery) {
     return new ServerDiscoveryFactory(serviceDiscovery);
   }
@@ -454,7 +455,7 @@ public class DiscoveryModule implements Module {
 
     @Override
     public Collection<ServiceInstance<T>> getAllInstances() {
-      return Collections.emptyList();
+      return emptyList();
     }
 
     @Override
diff --git a/server/src/main/java/org/apache/druid/curator/discovery/ServerDiscoverySelector.java b/server/src/main/java/org/apache/druid/curator/discovery/ServerDiscoverySelector.java
index fa390f473e..d122f8d5b2 100644
--- a/server/src/main/java/org/apache/druid/curator/discovery/ServerDiscoverySelector.java
+++ b/server/src/main/java/org/apache/druid/curator/discovery/ServerDiscoverySelector.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.curator.discovery;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Collections.emptyList;
+
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.Collections2;
 import com.google.common.net.HostAndPort;
 import java.io.IOException;
 import java.util.Collection;
-import java.util.Collections;
 import javax.annotation.Nullable;
 import org.apache.curator.x.discovery.ServiceInstance;
 import org.apache.curator.x.discovery.ServiceProvider;
@@ -52,7 +53,7 @@ public class ServerDiscoverySelector implements DiscoverySelector<Server> {
       new Function<ServiceInstance, Server>() {
         @Override
         public Server apply(final ServiceInstance instance) {
-          Preconditions.checkState(
+          checkState(
               instance.getPort() >= 0
                   || (instance.getSslPort() != null && instance.getSslPort() >= 0),
               "Both port and sslPort not set");
@@ -113,7 +114,7 @@ public class ServerDiscoverySelector implements DiscoverySelector<Server> {
       return Collections2.transform(serviceProvider.getAllInstances(), TO_SERVER);
     } catch (Exception e) {
       log.info(e, "Unable to get all instances");
-      return Collections.emptyList();
+      return emptyList();
     }
   }
 
diff --git a/server/src/main/java/org/apache/druid/curator/inventory/CuratorInventoryManager.java b/server/src/main/java/org/apache/druid/curator/inventory/CuratorInventoryManager.java
index d58c2ca7bc..3e6a9f8344 100644
--- a/server/src/main/java/org/apache/druid/curator/inventory/CuratorInventoryManager.java
+++ b/server/src/main/java/org/apache/druid/curator/inventory/CuratorInventoryManager.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.curator.inventory;
 
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.collect.Sets;
 import java.io.IOException;
 import java.util.Collection;
@@ -27,7 +29,6 @@ import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.atomic.AtomicReference;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.curator.framework.CuratorFramework;
 import org.apache.curator.framework.recipes.cache.ChildData;
@@ -162,9 +163,7 @@ public class CuratorInventoryManager<ContainerClass, InventoryClass> {
   }
 
   public Collection<ContainerClass> getInventory() {
-    return containers.values().stream()
-        .map(ContainerHolder::getContainer)
-        .collect(Collectors.toList());
+    return containers.values().stream().map(ContainerHolder::getContainer).collect(toList());
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/discovery/BaseNodeRoleWatcher.java b/server/src/main/java/org/apache/druid/discovery/BaseNodeRoleWatcher.java
index 866e9f643e..86a3909e41 100644
--- a/server/src/main/java/org/apache/druid/discovery/BaseNodeRoleWatcher.java
+++ b/server/src/main/java/org/apache/druid/discovery/BaseNodeRoleWatcher.java
@@ -19,12 +19,12 @@
 
 package org.apache.druid.discovery;
 
+import static java.util.Collections.unmodifiableCollection;
+
 import com.google.common.collect.ImmutableList;
-import com.google.common.collect.Lists;
 import com.google.errorprone.annotations.concurrent.GuardedBy;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
@@ -53,7 +53,7 @@ public class BaseNodeRoleWatcher {
   private final ConcurrentMap<String, DiscoveryDruidNode> nodes = new ConcurrentHashMap<>();
 
   private final Collection<DiscoveryDruidNode> unmodifiableNodes =
-      Collections.unmodifiableCollection(nodes.values());
+      unmodifiableCollection(nodes.values());
 
   private final ExecutorService listenerExecutor;
 
@@ -92,7 +92,7 @@ public class BaseNodeRoleWatcher {
         // It is important to take a snapshot here as list of nodes might change by the time
         // listeners process
         // the changes.
-        List<DiscoveryDruidNode> currNodes = Lists.newArrayList(nodes.values());
+        List<DiscoveryDruidNode> currNodes = new ArrayList<>(nodes.values());
         safeSchedule(
             () -> {
               listener.nodesAdded(currNodes);
@@ -207,7 +207,7 @@ public class BaseNodeRoleWatcher {
       // It is important to take a snapshot here as list of nodes might change by the time listeners
       // process
       // the changes.
-      List<DiscoveryDruidNode> currNodes = Lists.newArrayList(nodes.values());
+      List<DiscoveryDruidNode> currNodes = new ArrayList<>(nodes.values());
       LOGGER.info(
           "Node watcher of role [%s] is now initialized with %d nodes.",
           nodeRole.getJsonName(), currNodes.size());
diff --git a/server/src/main/java/org/apache/druid/discovery/DataNodeService.java b/server/src/main/java/org/apache/druid/discovery/DataNodeService.java
index 9d644ad828..6b56a8e8c1 100644
--- a/server/src/main/java/org/apache/druid/discovery/DataNodeService.java
+++ b/server/src/main/java/org/apache/druid/discovery/DataNodeService.java
@@ -121,8 +121,8 @@ public class DataNodeService extends DruidService {
   // leaving the "JsonIgnore" annotation to remember that "discoverable" is ignored in
   // serialization,
   // even though the annotation is not actually used.
-  @Override
   @JsonIgnore
+  @Override
   public boolean isDiscoverable() {
     return isDiscoverable;
   }
diff --git a/server/src/main/java/org/apache/druid/discovery/DiscoveryDruidNode.java b/server/src/main/java/org/apache/druid/discovery/DiscoveryDruidNode.java
index 7043039567..740d5fbe34 100644
--- a/server/src/main/java/org/apache/druid/discovery/DiscoveryDruidNode.java
+++ b/server/src/main/java/org/apache/druid/discovery/DiscoveryDruidNode.java
@@ -158,8 +158,8 @@ public class DiscoveryDruidNode {
     return druidNode;
   }
 
-  @Nullable
   @JsonIgnore
+  @Nullable
   public <T extends DruidService> T getService(String key, Class<T> clazz) {
     final DruidService o = services.get(key);
     if (o != null && clazz.isAssignableFrom(o.getClass())) {
diff --git a/server/src/main/java/org/apache/druid/discovery/DruidLeaderClient.java b/server/src/main/java/org/apache/druid/discovery/DruidLeaderClient.java
index b1492b570e..432eaa17b4 100644
--- a/server/src/main/java/org/apache/druid/discovery/DruidLeaderClient.java
+++ b/server/src/main/java/org/apache/druid/discovery/DruidLeaderClient.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.discovery;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkState;
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.common.base.Throwables;
 import java.io.IOException;
 import java.net.MalformedURLException;
 import java.net.URL;
-import java.nio.charset.StandardCharsets;
 import java.util.Iterator;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeUnit;
@@ -108,13 +109,13 @@ public class DruidLeaderClient {
    * Make a Request object aimed at the leader. Throws IOException if the leader cannot be located.
    */
   public Request makeRequest(HttpMethod httpMethod, String urlPath) throws IOException {
-    Preconditions.checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
+    checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
     return new Request(
         httpMethod, new URL(StringUtils.format("%s%s", getCurrentKnownLeader(true), urlPath)));
   }
 
   public StringFullResponseHolder go(Request request) throws IOException, InterruptedException {
-    return go(request, new StringFullResponseHandler(StandardCharsets.UTF_8));
+    return go(request, new StringFullResponseHandler(UTF_8));
   }
 
   /**
@@ -124,7 +125,7 @@ public class DruidLeaderClient {
   public <T, H extends FullResponseHolder<T>> H go(
       Request request, HttpResponseHandler<H, H> responseHandler)
       throws IOException, InterruptedException {
-    Preconditions.checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
+    checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
     for (int counter = 0; counter < MAX_RETRIES; counter++) {
 
       final H fullResponseHolder;
@@ -211,7 +212,7 @@ public class DruidLeaderClient {
   }
 
   public String findCurrentLeader() {
-    Preconditions.checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
+    checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
     final StringFullResponseHolder responseHolder;
     try {
       responseHolder = go(makeRequest(HttpMethod.GET, leaderRequestPath));
diff --git a/server/src/main/java/org/apache/druid/discovery/DruidNodeDiscoveryProvider.java b/server/src/main/java/org/apache/druid/discovery/DruidNodeDiscoveryProvider.java
index 33dbded397..375a6375fb 100644
--- a/server/src/main/java/org/apache/druid/discovery/DruidNodeDiscoveryProvider.java
+++ b/server/src/main/java/org/apache/druid/discovery/DruidNodeDiscoveryProvider.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.discovery;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Collections.unmodifiableCollection;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -83,7 +84,7 @@ public abstract class DruidNodeDiscoveryProvider {
     private final String service;
     private final ConcurrentMap<String, DiscoveryDruidNode> nodes = new ConcurrentHashMap<>();
     private final Collection<DiscoveryDruidNode> unmodifiableNodes =
-        Collections.unmodifiableCollection(nodes.values());
+        unmodifiableCollection(nodes.values());
 
     private final List<Listener> listeners = new ArrayList<>();
 
@@ -92,7 +93,7 @@ public abstract class DruidNodeDiscoveryProvider {
     private int uninitializedNodeRoles;
 
     ServiceDruidNodeDiscovery(String service, int watchedNodeRoles) {
-      Preconditions.checkArgument(watchedNodeRoles > 0);
+      checkArgument(watchedNodeRoles > 0);
       this.service = service;
       this.uninitializedNodeRoles = watchedNodeRoles;
     }
@@ -163,7 +164,7 @@ public abstract class DruidNodeDiscoveryProvider {
           }
 
           Collection<DiscoveryDruidNode> unmodifiableNodesAdded =
-              Collections.unmodifiableCollection(nodesAdded);
+              unmodifiableCollection(nodesAdded);
           for (Listener listener : listeners) {
             try {
               listener.nodesAdded(unmodifiableNodesAdded);
@@ -198,7 +199,7 @@ public abstract class DruidNodeDiscoveryProvider {
           }
 
           Collection<DiscoveryDruidNode> unmodifiableNodesRemoved =
-              Collections.unmodifiableCollection(nodesRemoved);
+              unmodifiableCollection(nodesRemoved);
           for (Listener listener : listeners) {
             try {
               listener.nodesRemoved(unmodifiableNodesRemoved);
diff --git a/server/src/main/java/org/apache/druid/discovery/DruidService.java b/server/src/main/java/org/apache/druid/discovery/DruidService.java
index 6e9b8e86e0..8fb6f9ff48 100644
--- a/server/src/main/java/org/apache/druid/discovery/DruidService.java
+++ b/server/src/main/java/org/apache/druid/discovery/DruidService.java
@@ -27,18 +27,15 @@ import com.fasterxml.jackson.annotation.JsonTypeInfo;
  * Metadata of a service announced by node. See DataNodeService and LookupNodeService for examples.
  */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(
-          name = DataNodeService.DISCOVERY_SERVICE_KEY,
-          value = DataNodeService.class),
-      @JsonSubTypes.Type(
-          name = LookupNodeService.DISCOVERY_SERVICE_KEY,
-          value = LookupNodeService.class),
-      @JsonSubTypes.Type(
-          name = WorkerNodeService.DISCOVERY_SERVICE_KEY,
-          value = WorkerNodeService.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = DataNodeService.DISCOVERY_SERVICE_KEY, value = DataNodeService.class),
+  @JsonSubTypes.Type(
+      name = LookupNodeService.DISCOVERY_SERVICE_KEY,
+      value = LookupNodeService.class),
+  @JsonSubTypes.Type(
+      name = WorkerNodeService.DISCOVERY_SERVICE_KEY,
+      value = WorkerNodeService.class)
+})
 public abstract class DruidService {
   public abstract String getName();
 
diff --git a/server/src/main/java/org/apache/druid/discovery/NodeRole.java b/server/src/main/java/org/apache/druid/discovery/NodeRole.java
index 3d1ac60368..0ab3bc0119 100644
--- a/server/src/main/java/org/apache/druid/discovery/NodeRole.java
+++ b/server/src/main/java/org/apache/druid/discovery/NodeRole.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.discovery;
 
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toMap;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonValue;
 import com.google.inject.name.Named;
@@ -26,8 +29,6 @@ import com.google.inject.name.Names;
 import java.util.Arrays;
 import java.util.Map;
 import java.util.Objects;
-import java.util.function.Function;
-import java.util.stream.Collectors;
 
 /**
  * Defines the 'role' of a Druid service, utilized to strongly type announcement and service
@@ -78,7 +79,7 @@ public class NodeRole {
       };
 
   private static final Map<String, NodeRole> BUILT_IN_LOOKUP =
-      Arrays.stream(BUILT_IN).collect(Collectors.toMap(NodeRole::getJsonName, Function.identity()));
+      Arrays.stream(BUILT_IN).collect(toMap(NodeRole::getJsonName, identity()));
 
   /**
    * For built-in roles, to preserve backwards compatibility when this was an enum, this provides
diff --git a/server/src/main/java/org/apache/druid/guice/AnnouncerModule.java b/server/src/main/java/org/apache/druid/guice/AnnouncerModule.java
index 985229702b..b88c969b2f 100644
--- a/server/src/main/java/org/apache/druid/guice/AnnouncerModule.java
+++ b/server/src/main/java/org/apache/druid/guice/AnnouncerModule.java
@@ -64,8 +64,8 @@ public class AnnouncerModule implements Module {
     }
   }
 
-  @Provides
   @ManageLifecycleAnnouncements
+  @Provides
   public Announcer getAnnouncer(CuratorFramework curator) {
     return new Announcer(curator, Execs.singleThreaded("Announcer-%s"));
   }
diff --git a/server/src/main/java/org/apache/druid/guice/BrokerProcessingModule.java b/server/src/main/java/org/apache/druid/guice/BrokerProcessingModule.java
index c7e6fd2953..f68cd1d55d 100644
--- a/server/src/main/java/org/apache/druid/guice/BrokerProcessingModule.java
+++ b/server/src/main/java/org/apache/druid/guice/BrokerProcessingModule.java
@@ -68,8 +68,8 @@ public class BrokerProcessingModule implements Module {
     MetricsModule.register(binder, ExecutorServiceMonitor.class);
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public CachePopulator getCachePopulator(
       @Smile ObjectMapper smileMapper,
       CachePopulatorStats cachePopulatorStats,
@@ -92,15 +92,15 @@ public class BrokerProcessingModule implements Module {
     }
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public QueryProcessingPool getProcessingExecutorPool(DruidProcessingConfig config) {
     return new ForwardingQueryProcessingPool(Execs.dummy());
   }
 
-  @Provides
-  @LazySingleton
   @Global
+  @LazySingleton
+  @Provides
   public NonBlockingPool<ByteBuffer> getIntermediateResultsPool(DruidProcessingConfig config) {
     verifyDirectMemory(config);
 
@@ -112,9 +112,9 @@ public class BrokerProcessingModule implements Module {
         config.poolCacheMaxCount());
   }
 
-  @Provides
   @LazySingleton
   @Merging
+  @Provides
   public BlockingPool<ByteBuffer> getMergeBufferPool(DruidProcessingConfig config) {
     verifyDirectMemory(config);
     return new DefaultBlockingPool<>(
@@ -122,8 +122,8 @@ public class BrokerProcessingModule implements Module {
         config.getNumMergeBuffers());
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public LifecycleForkJoinPoolProvider getMergeProcessingPoolProvider(
       DruidProcessingConfig config) {
     return new LifecycleForkJoinPoolProvider(
@@ -134,8 +134,8 @@ public class BrokerProcessingModule implements Module {
         config.getMergePoolAwaitShutdownMillis());
   }
 
-  @Provides
   @Merging
+  @Provides
   public ForkJoinPool getMergeProcessingPool(LifecycleForkJoinPoolProvider poolProvider) {
     return poolProvider.getPool();
   }
diff --git a/server/src/main/java/org/apache/druid/guice/CoordinatorDiscoveryModule.java b/server/src/main/java/org/apache/druid/guice/CoordinatorDiscoveryModule.java
index fec75511c6..d46c8f248c 100644
--- a/server/src/main/java/org/apache/druid/guice/CoordinatorDiscoveryModule.java
+++ b/server/src/main/java/org/apache/druid/guice/CoordinatorDiscoveryModule.java
@@ -37,9 +37,9 @@ public class CoordinatorDiscoveryModule implements Module {
     JsonConfigProvider.bind(binder, "druid.selectors.coordinator", CoordinatorSelectorConfig.class);
   }
 
-  @Provides
   @Coordinator
   @ManageLifecycle
+  @Provides
   public DruidLeaderClient getLeaderHttpClient(
       @EscalatedGlobal HttpClient httpClient,
       DruidNodeDiscoveryProvider druidNodeDiscoveryProvider) {
diff --git a/server/src/main/java/org/apache/druid/guice/DruidInjectorBuilder.java b/server/src/main/java/org/apache/druid/guice/DruidInjectorBuilder.java
index 637a53dcd6..1178566c33 100644
--- a/server/src/main/java/org/apache/druid/guice/DruidInjectorBuilder.java
+++ b/server/src/main/java/org/apache/druid/guice/DruidInjectorBuilder.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.guice;
 
+import static java.util.Collections.emptySet;
+import static java.util.stream.Collectors.toSet;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.inject.Guice;
 import com.google.inject.Injector;
@@ -26,10 +29,8 @@ import com.google.inject.Key;
 import com.google.inject.Module;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.discovery.NodeRole;
 import org.apache.druid.guice.annotations.Json;
 import org.apache.druid.guice.annotations.LoadScope;
@@ -61,7 +62,7 @@ public class DruidInjectorBuilder {
   private boolean ignoreLoadScopes;
 
   public DruidInjectorBuilder(final Injector baseInjector) {
-    this(baseInjector, Collections.emptySet());
+    this(baseInjector, emptySet());
   }
 
   public DruidInjectorBuilder(final Injector baseInjector, final Set<NodeRole> nodeRoles) {
@@ -183,7 +184,7 @@ public class DruidInjectorBuilder {
       return true;
     }
     Set<NodeRole> rolesPredicate =
-        Arrays.stream(loadScope.roles()).map(NodeRole::fromJsonName).collect(Collectors.toSet());
+        Arrays.stream(loadScope.roles()).map(NodeRole::fromJsonName).collect(toSet());
     return rolesPredicate.stream().anyMatch(nodeRoles::contains);
   }
 
diff --git a/server/src/main/java/org/apache/druid/guice/DruidProcessingModule.java b/server/src/main/java/org/apache/druid/guice/DruidProcessingModule.java
index 4ce7cf6cf4..394dc09a4e 100644
--- a/server/src/main/java/org/apache/druid/guice/DruidProcessingModule.java
+++ b/server/src/main/java/org/apache/druid/guice/DruidProcessingModule.java
@@ -64,8 +64,8 @@ public class DruidProcessingModule implements Module {
     MetricsModule.register(binder, ExecutorServiceMonitor.class);
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public CachePopulator getCachePopulator(
       @Smile ObjectMapper smileMapper,
       CachePopulatorStats cachePopulatorStats,
@@ -88,8 +88,8 @@ public class DruidProcessingModule implements Module {
     }
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public QueryProcessingPool getProcessingExecutorPool(
       DruidProcessingConfig config,
       ExecutorServiceMonitor executorServiceMonitor,
@@ -98,9 +98,9 @@ public class DruidProcessingModule implements Module {
         PrioritizedExecutorService.create(lifecycle, config), executorServiceMonitor);
   }
 
-  @Provides
-  @LazySingleton
   @Global
+  @LazySingleton
+  @Provides
   public NonBlockingPool<ByteBuffer> getIntermediateResultsPool(DruidProcessingConfig config) {
     verifyDirectMemory(config);
     return new StupidPool<>(
@@ -111,9 +111,9 @@ public class DruidProcessingModule implements Module {
         config.poolCacheMaxCount());
   }
 
-  @Provides
   @LazySingleton
   @Merging
+  @Provides
   public BlockingPool<ByteBuffer> getMergeBufferPool(DruidProcessingConfig config) {
     verifyDirectMemory(config);
     return new DefaultBlockingPool<>(
@@ -121,8 +121,8 @@ public class DruidProcessingModule implements Module {
         config.getNumMergeBuffers());
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public LifecycleForkJoinPoolProvider getMergeProcessingPoolProvider(
       DruidProcessingConfig config) {
     return new LifecycleForkJoinPoolProvider(
@@ -133,8 +133,8 @@ public class DruidProcessingModule implements Module {
         config.getMergePoolAwaitShutdownMillis());
   }
 
-  @Provides
   @Merging
+  @Provides
   public ForkJoinPool getMergeProcessingPool(LifecycleForkJoinPoolProvider poolProvider) {
     return poolProvider.getPool();
   }
diff --git a/server/src/main/java/org/apache/druid/guice/IndexingServiceDiscoveryModule.java b/server/src/main/java/org/apache/druid/guice/IndexingServiceDiscoveryModule.java
index 3113aa8ca3..ada6147af4 100644
--- a/server/src/main/java/org/apache/druid/guice/IndexingServiceDiscoveryModule.java
+++ b/server/src/main/java/org/apache/druid/guice/IndexingServiceDiscoveryModule.java
@@ -38,9 +38,9 @@ public class IndexingServiceDiscoveryModule implements Module {
         binder, "druid.selectors.indexing", IndexingServiceSelectorConfig.class);
   }
 
-  @Provides
   @IndexingService
   @ManageLifecycle
+  @Provides
   public DruidLeaderClient getLeaderHttpClient(
       @EscalatedGlobal HttpClient httpClient,
       DruidNodeDiscoveryProvider druidNodeDiscoveryProvider) {
diff --git a/server/src/main/java/org/apache/druid/guice/QueryableModule.java b/server/src/main/java/org/apache/druid/guice/QueryableModule.java
index f5fd0c3c4f..afa8e7d5f6 100644
--- a/server/src/main/java/org/apache/druid/guice/QueryableModule.java
+++ b/server/src/main/java/org/apache/druid/guice/QueryableModule.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.guice;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.Module;
 import com.fasterxml.jackson.databind.module.SimpleModule;
 import com.google.inject.Binder;
-import java.util.Collections;
 import java.util.List;
 import org.apache.druid.initialization.DruidModule;
 import org.apache.druid.query.DefaultQueryRunnerFactoryConglomerate;
@@ -59,7 +60,7 @@ public class QueryableModule implements DruidModule {
 
   @Override
   public List<Module> getJacksonModules() {
-    return Collections.singletonList(
+    return singletonList(
         new SimpleModule("QueryableModule")
             .registerSubtypes(
                 NoopRequestLoggerProvider.class,
diff --git a/server/src/main/java/org/apache/druid/guice/RouterProcessingModule.java b/server/src/main/java/org/apache/druid/guice/RouterProcessingModule.java
index ab1740eb35..e9fca040ab 100644
--- a/server/src/main/java/org/apache/druid/guice/RouterProcessingModule.java
+++ b/server/src/main/java/org/apache/druid/guice/RouterProcessingModule.java
@@ -54,8 +54,8 @@ public class RouterProcessingModule implements Module {
     MetricsModule.register(binder, ExecutorServiceMonitor.class);
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public QueryProcessingPool getProcessingExecutorPool(DruidProcessingConfig config) {
     if (config.getNumThreadsConfigured() != ExecutorServiceConfig.DEFAULT_NUM_THREADS) {
       log.error(
@@ -64,16 +64,16 @@ public class RouterProcessingModule implements Module {
     return new ForwardingQueryProcessingPool(Execs.dummy());
   }
 
-  @Provides
-  @LazySingleton
   @Global
+  @LazySingleton
+  @Provides
   public NonBlockingPool<ByteBuffer> getIntermediateResultsPool() {
     return DummyNonBlockingPool.instance();
   }
 
-  @Provides
   @LazySingleton
   @Merging
+  @Provides
   public BlockingPool<ByteBuffer> getMergeBufferPool(DruidProcessingConfig config) {
     if (config.getNumMergeBuffersConfigured() != DruidProcessingConfig.DEFAULT_NUM_MERGE_BUFFERS) {
       log.error(
diff --git a/server/src/main/java/org/apache/druid/guice/ServerModule.java b/server/src/main/java/org/apache/druid/guice/ServerModule.java
index 28caab9259..92aaa67d89 100644
--- a/server/src/main/java/org/apache/druid/guice/ServerModule.java
+++ b/server/src/main/java/org/apache/druid/guice/ServerModule.java
@@ -46,8 +46,8 @@ public class ServerModule implements DruidModule {
     JsonConfigProvider.bind(binder, "druid", DruidNode.class, Self.class);
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public ScheduledExecutorFactory getScheduledExecutorFactory(Lifecycle lifecycle) {
     return ScheduledExecutors.createFactory(lifecycle);
   }
diff --git a/server/src/main/java/org/apache/druid/guice/StorageNodeModule.java b/server/src/main/java/org/apache/druid/guice/StorageNodeModule.java
index 68d5149d5b..76e499713e 100644
--- a/server/src/main/java/org/apache/druid/guice/StorageNodeModule.java
+++ b/server/src/main/java/org/apache/druid/guice/StorageNodeModule.java
@@ -58,8 +58,8 @@ public class StorageNodeModule implements Module {
     binder.bind(ColumnConfig.class).to(DruidProcessingConfig.class);
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public DruidServerMetadata getMetadata(
       @Self DruidNode node, @Nullable ServerTypeConfig serverTypeConfig, DruidServerConfig config) {
     if (serverTypeConfig == null) {
@@ -77,8 +77,8 @@ public class StorageNodeModule implements Module {
         config.getPriority());
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public DataNodeService getDataNodeService(
       @Nullable ServerTypeConfig serverTypeConfig,
       DruidServerConfig config,
@@ -91,7 +91,7 @@ public class StorageNodeModule implements Module {
       log.info(
           "Segment cache not configured on ServerType [%s]. It will not be assignable for segment placement",
           serverTypeConfig.getServerType());
-      if (ServerType.HISTORICAL.equals(serverTypeConfig.getServerType())) {
+      if (ServerType.HISTORICAL == serverTypeConfig.getServerType()) {
         throw new ProvisionException("druid.segmentCache.locations must be set on historicals.");
       }
     }
@@ -104,9 +104,9 @@ public class StorageNodeModule implements Module {
         isSegmentCacheConfigured);
   }
 
-  @Provides
   @LazySingleton
   @Named(IS_SEGMENT_CACHE_CONFIGURED)
+  @Provides
   public Boolean isSegmentCacheConfigured(SegmentLoaderConfig segmentLoaderConfig) {
     return !segmentLoaderConfig.getLocations().isEmpty();
   }
@@ -115,8 +115,8 @@ public class StorageNodeModule implements Module {
    * provide a list of StorageLocation so that it can be injected into objects such as
    * implementations of {@link StorageLocationSelectorStrategy}
    */
-  @Provides
   @LazySingleton
+  @Provides
   public List<StorageLocation> provideStorageLocation(SegmentLoaderConfig config) {
     return config.toStorageLocations();
   }
diff --git a/server/src/main/java/org/apache/druid/guice/annotations/LoadScope.java b/server/src/main/java/org/apache/druid/guice/annotations/LoadScope.java
index 85bbc7dabd..fc22d58c05 100644
--- a/server/src/main/java/org/apache/druid/guice/annotations/LoadScope.java
+++ b/server/src/main/java/org/apache/druid/guice/annotations/LoadScope.java
@@ -32,7 +32,7 @@ import java.lang.annotation.Target;
  *
  * <p>A module is loaded in every node if this annotation is missing.
  */
-@Target({ElementType.TYPE})
+@Target(ElementType.TYPE)
 @Retention(RetentionPolicy.RUNTIME)
 public @interface LoadScope {
   String[] roles();
diff --git a/server/src/main/java/org/apache/druid/guice/http/AbstractHttpClientProvider.java b/server/src/main/java/org/apache/druid/guice/http/AbstractHttpClientProvider.java
index 2dd7797c75..490fdca5c2 100644
--- a/server/src/main/java/org/apache/druid/guice/http/AbstractHttpClientProvider.java
+++ b/server/src/main/java/org/apache/druid/guice/http/AbstractHttpClientProvider.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.guice.http;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.base.Supplier;
 import com.google.inject.Binding;
 import com.google.inject.Inject;
@@ -40,7 +41,7 @@ public abstract class AbstractHttpClientProvider<HttpClientType>
   private Injector injector;
 
   public AbstractHttpClientProvider(Class<? extends Annotation> annotation) {
-    Preconditions.checkNotNull(annotation, "annotation");
+    requireNonNull(annotation, "annotation");
 
     configKey = Key.get(new TypeLiteral<Supplier<DruidHttpClientConfig>>() {}, annotation);
     sslContextKey = Key.get(SSLContext.class, annotation);
diff --git a/server/src/main/java/org/apache/druid/guice/http/HttpClientModule.java b/server/src/main/java/org/apache/druid/guice/http/HttpClientModule.java
index 9fc465f6be..06c4666642 100644
--- a/server/src/main/java/org/apache/druid/guice/http/HttpClientModule.java
+++ b/server/src/main/java/org/apache/druid/guice/http/HttpClientModule.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.guice.http;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.collect.ImmutableSet;
 import com.google.inject.Binder;
 import com.google.inject.Binding;
@@ -57,8 +58,8 @@ public class HttpClientModule implements Module {
   private final boolean isEscalated;
 
   public HttpClientModule(String propertyPrefix, Class<? extends Annotation> annotationClazz) {
-    this.propertyPrefix = Preconditions.checkNotNull(propertyPrefix, "propertyPrefix");
-    this.annotationClazz = Preconditions.checkNotNull(annotationClazz, "annotationClazz");
+    this.propertyPrefix = requireNonNull(propertyPrefix, "propertyPrefix");
+    this.annotationClazz = requireNonNull(annotationClazz, "annotationClazz");
 
     isEscalated = ESCALATING_ANNOTATIONS.contains(this.annotationClazz);
   }
diff --git a/server/src/main/java/org/apache/druid/guice/http/JettyHttpClientModule.java b/server/src/main/java/org/apache/druid/guice/http/JettyHttpClientModule.java
index f79486fa37..a04a71c489 100644
--- a/server/src/main/java/org/apache/druid/guice/http/JettyHttpClientModule.java
+++ b/server/src/main/java/org/apache/druid/guice/http/JettyHttpClientModule.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.guice.http;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import com.google.inject.Binder;
 import com.google.inject.Binding;
 import com.google.inject.Module;
@@ -46,8 +47,8 @@ public class JettyHttpClientModule implements Module {
   private final Class<? extends Annotation> annotationClazz;
 
   public JettyHttpClientModule(String propertyPrefix, Class<? extends Annotation> annotationClazz) {
-    this.propertyPrefix = Preconditions.checkNotNull(propertyPrefix, "propertyPrefix");
-    this.annotationClazz = Preconditions.checkNotNull(annotationClazz, "annotationClazz");
+    this.propertyPrefix = requireNonNull(propertyPrefix, "propertyPrefix");
+    this.annotationClazz = requireNonNull(annotationClazz, "annotationClazz");
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/guice/security/AuthenticatorModule.java b/server/src/main/java/org/apache/druid/guice/security/AuthenticatorModule.java
index 7400205748..1c2dc3358d 100644
--- a/server/src/main/java/org/apache/druid/guice/security/AuthenticatorModule.java
+++ b/server/src/main/java/org/apache/druid/guice/security/AuthenticatorModule.java
@@ -42,8 +42,8 @@ public class AuthenticatorModule implements Module {
         .in(LazySingleton.class);
   }
 
-  @Provides
   @Named(AuthConfig.ALLOW_ALL_NAME)
+  @Provides
   public Authenticator getAuthenticator() {
     return new AllowAllAuthenticator();
   }
diff --git a/server/src/main/java/org/apache/druid/guice/security/AuthorizerModule.java b/server/src/main/java/org/apache/druid/guice/security/AuthorizerModule.java
index 61c71f4d8e..95d34bf6ed 100644
--- a/server/src/main/java/org/apache/druid/guice/security/AuthorizerModule.java
+++ b/server/src/main/java/org/apache/druid/guice/security/AuthorizerModule.java
@@ -42,8 +42,8 @@ public class AuthorizerModule implements Module {
         .in(LazySingleton.class);
   }
 
-  @Provides
   @Named(AuthConfig.ALLOW_ALL_NAME)
+  @Provides
   public Authorizer getAuthorizer() {
     return new AllowAllAuthorizer();
   }
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/DataSourceMetadata.java b/server/src/main/java/org/apache/druid/indexing/overlord/DataSourceMetadata.java
index 8e8fea3aa8..1044074164 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/DataSourceMetadata.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/DataSourceMetadata.java
@@ -32,7 +32,7 @@ import java.util.Set;
  * right-hand side. This means metadata can be partitioned.
  */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(value = {@JsonSubTypes.Type(name = "object", value = ObjectMetadata.class)})
+@JsonSubTypes(@JsonSubTypes.Type(name = "object", value = ObjectMetadata.class))
 public interface DataSourceMetadata {
   /**
    * Returns true if this instance should be considered a valid starting point for a new dataSource
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java b/server/src/main/java/org/apache/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java
index 2b6029fec6..1d5ca62502 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/IndexerMetadataStorageCoordinator.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.indexing.overlord;
 
+import static java.util.Collections.singletonList;
+
 import java.io.IOException;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -59,8 +60,7 @@ public interface IndexerMetadataStorageCoordinator {
    */
   default Collection<DataSegment> retrieveUsedSegmentsForInterval(
       String dataSource, Interval interval, Segments visibility) {
-    return retrieveUsedSegmentsForIntervals(
-        dataSource, Collections.singletonList(interval), visibility);
+    return retrieveUsedSegmentsForIntervals(dataSource, singletonList(interval), visibility);
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/SegmentPublishResult.java b/server/src/main/java/org/apache/druid/indexing/overlord/SegmentPublishResult.java
index 046375c921..59c85d09fd 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/SegmentPublishResult.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/SegmentPublishResult.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.indexing.overlord;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableSet;
 import java.util.Objects;
 import java.util.Set;
@@ -58,13 +60,12 @@ public class SegmentPublishResult {
       @JsonProperty("segments") Set<DataSegment> segments,
       @JsonProperty("success") boolean success,
       @JsonProperty("errorMsg") @Nullable String errorMsg) {
-    this.segments = Preconditions.checkNotNull(segments, "segments");
+    this.segments = requireNonNull(segments, "segments");
     this.success = success;
     this.errorMsg = errorMsg;
 
     if (!success) {
-      Preconditions.checkArgument(
-          segments.isEmpty(), "segments must be empty for unsuccessful publishes");
+      checkArgument(segments.isEmpty(), "segments must be empty for unsuccessful publishes");
     }
   }
 
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java
index 33f0786849..4b379aec77 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/NoopSupervisorSpec.java
@@ -81,40 +81,40 @@ public class NoopSupervisorSpec implements SupervisorSpec {
     this.source = "noop";
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getId() {
     return id;
   }
 
-  @Override
-  @Nullable
   @JsonProperty("dataSources")
+  @Nullable
+  @Override
   public List<String> getDataSources() {
     return datasources;
   }
 
-  @Override
   @JsonProperty("suspended")
+  @Override
   public boolean isSuspended() {
     return suspended;
   }
 
-  @Override
   @JsonProperty("type")
+  @Override
   public String getType() {
     return type;
   }
 
-  @Nonnull
   @JsonIgnore
+  @Nonnull
   @Override
   public Set<ResourceAction> getInputSourceResources() {
     return ImmutableSet.of();
   }
 
-  @Override
   @JsonProperty("source")
+  @Override
   public String getSource() {
     return source;
   }
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/Supervisor.java b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/Supervisor.java
index b31d0d7b44..24b2495165 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/Supervisor.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/Supervisor.java
@@ -54,7 +54,7 @@ public interface Supervisor {
   @Nullable
   default Boolean isHealthy() {
     return null; // default implementation for interface compatability; returning null since true or
-                 // false is misleading
+    // false is misleading
   }
 
   void reset(DataSourceMetadata dataSourceMetadata);
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorSpec.java b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorSpec.java
index e9bc79f679..1a7949d3ab 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorSpec.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorSpec.java
@@ -31,8 +31,7 @@ import org.apache.druid.java.util.common.UOE;
 import org.apache.druid.server.security.ResourceAction;
 
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(
-    value = {@JsonSubTypes.Type(name = "NoopSupervisorSpec", value = NoopSupervisorSpec.class)})
+@JsonSubTypes(@JsonSubTypes.Type(name = "NoopSupervisorSpec", value = NoopSupervisorSpec.class))
 public interface SupervisorSpec {
   /** Return an unique id of {@link Supervisor}. */
   String getId();
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorStateManager.java b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorStateManager.java
index 9aee5e990d..29270558b3 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorStateManager.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorStateManager.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.indexing.overlord.supervisor;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.ArrayList;
 import java.util.Deque;
 import java.util.List;
@@ -103,7 +104,7 @@ public class SupervisorStateManager {
 
   public SupervisorStateManager(
       SupervisorStateManagerConfig supervisorStateManagerConfig, boolean suspended) {
-    Preconditions.checkArgument(
+    checkArgument(
         supervisorStateManagerConfig.getMaxStoredExceptionEvents()
             >= Math.max(
                 supervisorStateManagerConfig.getHealthinessThreshold(),
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorStatus.java b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorStatus.java
index efbe44b829..3106efa675 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorStatus.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/SupervisorStatus.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.indexing.overlord.supervisor;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
 import com.fasterxml.jackson.databind.annotation.JsonPOJOBuilder;
-import com.google.common.base.Preconditions;
 import java.util.Objects;
 
 /**
@@ -51,7 +52,7 @@ public class SupervisorStatus {
   private final boolean suspended;
 
   private SupervisorStatus(Builder builder) {
-    this.id = Preconditions.checkNotNull(builder.id, "id");
+    this.id = requireNonNull(builder.id, "id");
     this.state = builder.state;
     this.detailedState = builder.detailedState;
     this.healthy = builder.healthy;
@@ -164,7 +165,7 @@ public class SupervisorStatus {
 
     @JsonProperty
     public Builder withId(String id) {
-      this.id = Preconditions.checkNotNull(id, "id");
+      this.id = requireNonNull(id, "id");
       return this;
     }
 
diff --git a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/VersionedSupervisorSpec.java b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/VersionedSupervisorSpec.java
index f74c95564b..e7031cb003 100644
--- a/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/VersionedSupervisorSpec.java
+++ b/server/src/main/java/org/apache/druid/indexing/overlord/supervisor/VersionedSupervisorSpec.java
@@ -68,7 +68,6 @@ public class VersionedSupervisorSpec {
   @Override
   public int hashCode() {
     int result = getSpec() != null ? getSpec().hashCode() : 0;
-    result = 31 * result + (getVersion() != null ? getVersion().hashCode() : 0);
-    return result;
+    return 31 * result + (getVersion() != null ? getVersion().hashCode() : 0);
   }
 }
diff --git a/server/src/main/java/org/apache/druid/initialization/CoreInjectorBuilder.java b/server/src/main/java/org/apache/druid/initialization/CoreInjectorBuilder.java
index 2b85d7c0e0..0334659da7 100644
--- a/server/src/main/java/org/apache/druid/initialization/CoreInjectorBuilder.java
+++ b/server/src/main/java/org/apache/druid/initialization/CoreInjectorBuilder.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.initialization;
 
+import static java.util.Collections.emptySet;
+
 import com.google.inject.Injector;
-import java.util.Collections;
 import java.util.Set;
 import org.apache.druid.curator.CuratorModule;
 import org.apache.druid.curator.discovery.DiscoveryModule;
@@ -70,7 +71,7 @@ import org.apache.druid.storage.StorageConnectorModule;
  */
 public class CoreInjectorBuilder extends DruidInjectorBuilder {
   public CoreInjectorBuilder(final Injector baseInjector) {
-    this(baseInjector, Collections.emptySet());
+    this(baseInjector, emptySet());
   }
 
   public CoreInjectorBuilder(final Injector baseInjector, final Set<NodeRole> nodeRoles) {
diff --git a/server/src/main/java/org/apache/druid/initialization/ServerInjectorBuilder.java b/server/src/main/java/org/apache/druid/initialization/ServerInjectorBuilder.java
index 8fd2e3b4fb..49fd34164c 100644
--- a/server/src/main/java/org/apache/druid/initialization/ServerInjectorBuilder.java
+++ b/server/src/main/java/org/apache/druid/initialization/ServerInjectorBuilder.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.initialization;
 
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
@@ -82,8 +83,8 @@ public class ServerInjectorBuilder {
   }
 
   public Injector build() {
-    Preconditions.checkNotNull(baseInjector);
-    Preconditions.checkNotNull(nodeRoles);
+    requireNonNull(baseInjector);
+    requireNonNull(nodeRoles);
 
     Module registerNodeRoleModule = registerNodeRoleModule(nodeRoles);
 
diff --git a/server/src/main/java/org/apache/druid/metadata/BasicDataSourceExt.java b/server/src/main/java/org/apache/druid/metadata/BasicDataSourceExt.java
index cd2eee0fff..f4251db9ac 100644
--- a/server/src/main/java/org/apache/druid/metadata/BasicDataSourceExt.java
+++ b/server/src/main/java/org/apache/druid/metadata/BasicDataSourceExt.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.annotations.VisibleForTesting;
 import java.sql.Driver;
 import java.sql.DriverManager;
@@ -72,14 +74,12 @@ public class BasicDataSourceExt extends BasicDataSource {
 
   @Override
   public void setConnectionProperties(String connectionProperties) {
-    if (connectionProperties == null) {
-      throw new NullPointerException("connectionProperties is null");
-    }
+    requireNonNull(connectionProperties, "connectionProperties is null");
 
     String[] entries = connectionProperties.split(";");
     Properties properties = new Properties();
     for (String entry : entries) {
-      if (entry.length() > 0) {
+      if (!entry.isEmpty()) {
         int index = entry.indexOf('=');
         if (index > 0) {
           String name = entry.substring(0, index);
diff --git a/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java b/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java
index b5e11278f1..4321f24205 100644
--- a/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java
+++ b/server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java
@@ -19,10 +19,18 @@
 
 package org.apache.druid.metadata;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonList;
+import static java.util.Comparator.comparing;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.joining;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.FluentIterable;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
@@ -38,8 +46,6 @@ import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.Collections;
-import java.util.Comparator;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -47,7 +53,6 @@ import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import javax.annotation.Nullable;
 import javax.validation.constraints.NotNull;
@@ -129,7 +134,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
 
   @Override
   public Collection<DataSegment> retrieveAllUsedSegments(String dataSource, Segments visibility) {
-    return doRetrieveUsedSegments(dataSource, Collections.emptyList(), visibility);
+    return doRetrieveUsedSegments(dataSource, emptyList(), visibility);
   }
 
   /**
@@ -178,7 +183,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
             (handle, status) -> {
               try (final CloseableIterator<DataSegment> iterator =
                   SqlSegmentsMetadataQuery.forHandle(handle, connector, dbTables, jsonMapper)
-                      .retrieveUnusedSegments(dataSource, Collections.singletonList(interval))) {
+                      .retrieveUnusedSegments(dataSource, singletonList(interval))) {
                 return ImmutableList.copyOf(iterator);
               }
             });
@@ -286,21 +291,19 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       @Nullable final DataSourceMetadata startMetadata,
       @Nullable final DataSourceMetadata endMetadata)
       throws IOException {
-    if (segments.isEmpty()) {
-      throw new IllegalArgumentException("segment set must not be empty");
-    }
+    checkArgument(!segments.isEmpty(), "segment set must not be empty");
 
     final String dataSource = segments.iterator().next().getDataSource();
     for (DataSegment segment : segments) {
-      if (!dataSource.equals(segment.getDataSource())) {
-        throw new IllegalArgumentException("segments must all be from the same dataSource");
-      }
+      checkArgument(
+          dataSource.equals(segment.getDataSource()),
+          "segments must all be from the same dataSource");
     }
 
-    if ((startMetadata == null && endMetadata != null)
-        || (startMetadata != null && endMetadata == null)) {
-      throw new IllegalArgumentException("start/end metadata pair must be either null or non-null");
-    }
+    checkArgument(
+        !(startMetadata == null && endMetadata != null)
+            && !(startMetadata != null && endMetadata == null),
+        "start/end metadata pair must be either null or non-null");
 
     // Find which segments are used (i.e. not overshadowed).
     final Set<DataSegment> usedSegments = new HashSet<>();
@@ -360,7 +363,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
               final Set<DataSegment> inserted =
                   announceHistoricalSegmentBatch(handle, segments, usedSegments);
 
-              return SegmentPublishResult.ok(ImmutableSet.copyOf(inserted));
+              return SegmentPublishResult.ok(inserted);
             }
           },
           3,
@@ -378,15 +381,9 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
   @Override
   public SegmentPublishResult commitMetadataOnly(
       String dataSource, DataSourceMetadata startMetadata, DataSourceMetadata endMetadata) {
-    if (dataSource == null) {
-      throw new IllegalArgumentException("datasource name cannot be null");
-    }
-    if (startMetadata == null) {
-      throw new IllegalArgumentException("start metadata cannot be null");
-    }
-    if (endMetadata == null) {
-      throw new IllegalArgumentException("end metadata cannot be null");
-    }
+    checkArgument(dataSource != null, "datasource name cannot be null");
+    checkArgument(startMetadata != null, "start metadata cannot be null");
+    checkArgument(endMetadata != null, "end metadata cannot be null");
 
     final AtomicBoolean definitelyNotUpdated = new AtomicBoolean(false);
 
@@ -441,8 +438,8 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       Interval allocateInterval,
       boolean skipSegmentLineageCheck,
       List<SegmentCreateRequest> requests) {
-    Preconditions.checkNotNull(dataSource, "dataSource");
-    Preconditions.checkNotNull(allocateInterval, "interval");
+    requireNonNull(dataSource, "dataSource");
+    requireNonNull(allocateInterval, "interval");
 
     final Interval interval = allocateInterval.withChronology(ISOChronology.getInstanceUTC());
     return connector.retryWithHandle(
@@ -460,10 +457,10 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       final PartialShardSpec partialShardSpec,
       final String maxVersion,
       final boolean skipSegmentLineageCheck) {
-    Preconditions.checkNotNull(dataSource, "dataSource");
-    Preconditions.checkNotNull(sequenceName, "sequenceName");
-    Preconditions.checkNotNull(interval, "interval");
-    Preconditions.checkNotNull(maxVersion, "version");
+    requireNonNull(dataSource, "dataSource");
+    requireNonNull(sequenceName, "sequenceName");
+    requireNonNull(interval, "interval");
+    requireNonNull(maxVersion, "version");
     Interval allocateInterval = interval.withChronology(ISOChronology.getInstanceUTC());
 
     return connector.retryWithHandle(
@@ -894,12 +891,12 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       List<SegmentCreateRequest> requests)
       throws IOException {
     if (requests.isEmpty()) {
-      return Collections.emptyMap();
+      return emptyMap();
     }
 
     // Get the time chunk and associated data segments for the given interval, if any
     final List<TimelineObjectHolder<String, DataSegment>> existingChunks =
-        getTimelineForIntervalsWithHandle(handle, dataSource, Collections.singletonList(interval))
+        getTimelineForIntervalsWithHandle(handle, dataSource, singletonList(interval))
             .lookup(interval);
 
     if (existingChunks.size() > 1) {
@@ -907,7 +904,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       log.warn(
           "Cannot allocate new segments for dataSource[%s], interval[%s]: already have [%,d] chunks.",
           dataSource, interval, existingChunks.size());
-      return Collections.emptyMap();
+      return emptyMap();
     }
 
     // Shard spec of any of the requests (as they are all compatible) can be used to
@@ -1017,8 +1014,8 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
                     versionOfExistingChunk == null
                         || id.getVersion().equals(versionOfExistingChunk))
             .max(
-                Comparator.comparing(SegmentIdWithShardSpec::getVersion)
-                    .thenComparing(id -> id.getShardSpec().getPartitionNum()))
+                comparing(SegmentIdWithShardSpec::getVersion)
+                    .thenComparingInt(id -> id.getShardSpec().getPartitionNum()))
             .orElse(null);
 
     // Determine the version of the new segment
@@ -1073,7 +1070,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       return new SegmentIdWithShardSpec(
           dataSource,
           interval,
-          Preconditions.checkNotNull(newSegmentVersion, "newSegmentVersion"),
+          requireNonNull(newSegmentVersion, "newSegmentVersion"),
           partialShardSpec.complete(
               jsonMapper,
               overallMaxId.getShardSpec().getPartitionNum() + 1,
@@ -1169,8 +1166,8 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
                       versionOfExistingChunk == null
                           || id.getVersion().equals(versionOfExistingChunk))
               .max(
-                  Comparator.comparing(SegmentIdWithShardSpec::getVersion)
-                      .thenComparing(id -> id.getShardSpec().getPartitionNum()))
+                  comparing(SegmentIdWithShardSpec::getVersion)
+                      .thenComparingInt(id -> id.getShardSpec().getPartitionNum()))
               .orElse(null);
 
       // Determine the version of the new segment
@@ -1227,7 +1224,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
         return new SegmentIdWithShardSpec(
             dataSource,
             interval,
-            Preconditions.checkNotNull(newSegmentVersion, "newSegmentVersion"),
+            requireNonNull(newSegmentVersion, "newSegmentVersion"),
             partialShardSpec.complete(
                 jsonMapper,
                 overallMaxId.getShardSpec().getPartitionNum() + 1,
@@ -1325,7 +1322,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
               IntStream.range(0, partition.size())
                   .filter(i -> affectedRows[i] != 1)
                   .mapToObj(partition::get)
-                  .collect(Collectors.toList());
+                  .collect(toList());
           throw new ISE(
               "Failed to publish segments to DB: %s",
               SegmentUtils.commaSeparatedIdentifiers(failedToPublish));
@@ -1348,7 +1345,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       String segmentIds =
           segmentList.stream()
               .map(segment -> "'" + StringEscapeUtils.escapeSql(segment.getId().toString()) + "'")
-              .collect(Collectors.joining(","));
+              .collect(joining(","));
       List<String> existIds =
           handle
               .createQuery(
@@ -1363,8 +1360,8 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
   }
 
   /** Read dataSource metadata. Returns null if there is no metadata. */
-  @Override
-  public @Nullable DataSourceMetadata retrieveDataSourceMetadata(final String dataSource) {
+  @Nullable
+  public @Override DataSourceMetadata retrieveDataSourceMetadata(final String dataSource) {
     final byte[] bytes =
         connector.lookup(
             dbTables.getDataSourceTable(), "dataSource", "commit_metadata_payload", dataSource);
@@ -1410,9 +1407,9 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
       final DataSourceMetadata startMetadata,
       final DataSourceMetadata endMetadata)
       throws IOException {
-    Preconditions.checkNotNull(dataSource, "dataSource");
-    Preconditions.checkNotNull(startMetadata, "startMetadata");
-    Preconditions.checkNotNull(endMetadata, "endMetadata");
+    requireNonNull(dataSource, "dataSource");
+    requireNonNull(startMetadata, "startMetadata");
+    requireNonNull(endMetadata, "endMetadata");
 
     final byte[] oldCommitMetadataBytesFromDb =
         retrieveDataSourceMetadataWithHandleAsBytes(handle, dataSource);
@@ -1533,8 +1530,8 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
    */
   protected DataStoreMetadataUpdateResult dropSegmentsWithHandle(
       final Handle handle, final Collection<DataSegment> segmentsToDrop, final String dataSource) {
-    Preconditions.checkNotNull(dataSource, "dataSource");
-    Preconditions.checkNotNull(segmentsToDrop, "segmentsToDrop");
+    requireNonNull(dataSource, "dataSource");
+    requireNonNull(segmentsToDrop, "segmentsToDrop");
 
     if (segmentsToDrop.isEmpty()) {
       return DataStoreMetadataUpdateResult.SUCCESS;
@@ -1551,9 +1548,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
 
     final int numChangedSegments =
         SqlSegmentsMetadataQuery.forHandle(handle, connector, dbTables, jsonMapper)
-            .markSegments(
-                segmentsToDrop.stream().map(DataSegment::getId).collect(Collectors.toList()),
-                false);
+            .markSegments(segmentsToDrop.stream().map(DataSegment::getId).collect(toList()), false);
 
     if (numChangedSegments != segmentsToDrop.size()) {
       return new DataStoreMetadataUpdateResult(
@@ -1786,7 +1781,7 @@ public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStor
         boolean failed, boolean canRetry, @Nullable String errorMsg, Object... errorFormatArgs) {
       this.failed = failed;
       this.canRetry = canRetry;
-      this.errorMsg = null == errorMsg ? null : StringUtils.format(errorMsg, errorFormatArgs);
+      this.errorMsg = errorMsg == null ? null : StringUtils.format(errorMsg, errorFormatArgs);
     }
 
     public boolean isFailed() {
diff --git a/server/src/main/java/org/apache/druid/metadata/SQLFirehoseDatabaseConnector.java b/server/src/main/java/org/apache/druid/metadata/SQLFirehoseDatabaseConnector.java
index ef86d5d69d..2692b342a6 100644
--- a/server/src/main/java/org/apache/druid/metadata/SQLFirehoseDatabaseConnector.java
+++ b/server/src/main/java/org/apache/druid/metadata/SQLFirehoseDatabaseConnector.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.metadata;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JsonTypeInfo;
 import com.google.common.base.Predicate;
 import com.google.common.base.Strings;
@@ -79,9 +81,7 @@ public abstract class SQLFirehoseDatabaseConnector {
   }
 
   private void validateConfigs(String urlString, JdbcAccessSecurityConfig securityConfig) {
-    if (Strings.isNullOrEmpty(urlString)) {
-      throw new IllegalArgumentException("connectURI cannot be null or empty");
-    }
+    checkArgument(!Strings.isNullOrEmpty(urlString), "connectURI cannot be null or empty");
     if (!securityConfig.isEnforceAllowedProperties()) {
       // You don't want to do anything with properties.
       return;
diff --git a/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java b/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java
index 85d446ea94..08365dfa61 100644
--- a/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java
+++ b/server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java
@@ -566,8 +566,8 @@ public abstract class SQLMetadataConnector implements MetadataStorageConnector {
     }
   }
 
-  @Override
-  public @Nullable byte[] lookup(
+  @Nullable
+  public @Override byte[] lookup(
       final String tableName, final String keyColumn, final String valueColumn, final String key) {
     return getDBI()
         .withHandle(
diff --git a/server/src/main/java/org/apache/druid/metadata/SQLMetadataRuleManager.java b/server/src/main/java/org/apache/druid/metadata/SQLMetadataRuleManager.java
index 61e8e379fa..bdaeb95a9b 100644
--- a/server/src/main/java/org/apache/druid/metadata/SQLMetadataRuleManager.java
+++ b/server/src/main/java/org/apache/druid/metadata/SQLMetadataRuleManager.java
@@ -19,17 +19,18 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.Collections.singletonList;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import com.google.inject.Inject;
 import java.io.IOException;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -90,7 +91,7 @@ public class SQLMetadataRuleManager implements MetadataRuleManager {
               }
 
               final List<Rule> defaultRules =
-                  Collections.singletonList(
+                  singletonList(
                       new ForeverLoadRule(
                           ImmutableMap.of(
                               DruidServer.DEFAULT_TIER, DruidServer.DEFAULT_NUM_REPLICANTS)));
@@ -156,14 +157,14 @@ public class SQLMetadataRuleManager implements MetadataRuleManager {
     this.auditManager = auditManager;
 
     // Verify configured Periods can be treated as Durations (fail-fast before they're needed).
-    Preconditions.checkNotNull(config.getAlertThreshold().toStandardDuration());
-    Preconditions.checkNotNull(config.getPollDuration().toStandardDuration());
+    requireNonNull(config.getAlertThreshold().toStandardDuration());
+    requireNonNull(config.getPollDuration().toStandardDuration());
 
     this.rules = new AtomicReference<>(ImmutableMap.of());
   }
 
-  @Override
   @LifecycleStart
+  @Override
   public void start() {
     synchronized (lock) {
       if (currentStartOrder >= 0) {
@@ -206,8 +207,8 @@ public class SQLMetadataRuleManager implements MetadataRuleManager {
     }
   }
 
-  @Override
   @LifecycleStop
+  @Override
   public void stop() {
     synchronized (lock) {
       if (currentStartOrder == -1) {
diff --git a/server/src/main/java/org/apache/druid/metadata/SQLMetadataStorageActionHandler.java b/server/src/main/java/org/apache/druid/metadata/SQLMetadataStorageActionHandler.java
index d41485da7e..931c27b807 100644
--- a/server/src/main/java/org/apache/druid/metadata/SQLMetadataStorageActionHandler.java
+++ b/server/src/main/java/org/apache/druid/metadata/SQLMetadataStorageActionHandler.java
@@ -258,8 +258,8 @@ public abstract class SQLMetadataStorageActionHandler<EntryType, StatusType, Log
         });
   }
 
-  @Override
   @Nullable
+  @Override
   public TaskInfo<EntryType, StatusType> getTaskInfo(String entryId) {
     return connector.retryWithHandle(
         handle -> {
@@ -674,14 +674,12 @@ public abstract class SQLMetadataStorageActionHandler<EntryType, StatusType, Log
         log.error(e, "Encountered exception while deserializing task status_payload");
         throw new SQLException(e);
       }
-      taskInfo =
-          new TaskInfo<>(
-              resultSet.getString("id"),
-              DateTimes.of(resultSet.getString("created_date")),
-              status,
-              resultSet.getString("datasource"),
-              task);
-      return taskInfo;
+      return new TaskInfo<>(
+          resultSet.getString("id"),
+          DateTimes.of(resultSet.getString("created_date")),
+          status,
+          resultSet.getString("datasource"),
+          task);
     }
   }
 
@@ -882,8 +880,8 @@ public abstract class SQLMetadataStorageActionHandler<EntryType, StatusType, Log
         });
   }
 
-  @Override
   @Nullable
+  @Override
   public Long getLockId(String entryId, LockType lock) {
     return getLocks(entryId).entrySet().stream()
         .filter(entry -> entry.getValue().equals(lock))
@@ -935,7 +933,7 @@ public abstract class SQLMetadataStorageActionHandler<EntryType, StatusType, Log
   @Override
   public void populateTaskTypeAndGroupIdAsync() {
     ExecutorService executorService = Executors.newSingleThreadExecutor();
-    taskMigrationCompleteFuture = executorService.submit(() -> populateTaskTypeAndGroupId());
+    taskMigrationCompleteFuture = executorService.submit(this::populateTaskTypeAndGroupId);
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/metadata/SQLMetadataSupervisorManager.java b/server/src/main/java/org/apache/druid/metadata/SQLMetadataSupervisorManager.java
index c926307ee2..1df94343a2 100644
--- a/server/src/main/java/org/apache/druid/metadata/SQLMetadataSupervisorManager.java
+++ b/server/src/main/java/org/apache/druid/metadata/SQLMetadataSupervisorManager.java
@@ -24,8 +24,6 @@ import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.JsonMappingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Supplier;
-import com.google.common.collect.ImmutableList;
-import com.google.common.collect.ImmutableMap;
 import com.google.inject.Inject;
 import java.io.IOException;
 import java.sql.ResultSet;
@@ -75,8 +73,8 @@ public class SQLMetadataSupervisorManager implements MetadataSupervisorManager {
     this.dbi = connector.getDBI();
   }
 
-  @Override
   @LifecycleStart
+  @Override
   public void start() {
     connector.createSupervisorsTable();
   }
@@ -104,75 +102,71 @@ public class SQLMetadataSupervisorManager implements MetadataSupervisorManager {
 
   @Override
   public Map<String, List<VersionedSupervisorSpec>> getAll() {
-    return ImmutableMap.copyOf(
-        dbi.withHandle(
-            new HandleCallback<Map<String, List<VersionedSupervisorSpec>>>() {
-              @Override
-              public Map<String, List<VersionedSupervisorSpec>> withHandle(Handle handle) {
-                return handle
-                    .createQuery(
-                        StringUtils.format(
-                            "SELECT id, spec_id, created_date, payload FROM %1$s ORDER BY id DESC",
-                            getSupervisorsTable()))
-                    .map(
-                        new ResultSetMapper<Pair<String, VersionedSupervisorSpec>>() {
-                          @Override
-                          public Pair<String, VersionedSupervisorSpec> map(
-                              int index, ResultSet r, StatementContext ctx) throws SQLException {
-                            return Pair.of(
-                                r.getString("spec_id"), createVersionSupervisorSpecFromResponse(r));
-                          }
-                        })
-                    .fold(
-                        new HashMap<>(),
-                        new Folder3<
-                            Map<String, List<VersionedSupervisorSpec>>,
-                            Pair<String, VersionedSupervisorSpec>>() {
-                          @Override
-                          public Map<String, List<VersionedSupervisorSpec>> fold(
-                              Map<String, List<VersionedSupervisorSpec>> retVal,
-                              Pair<String, VersionedSupervisorSpec> pair,
-                              FoldController foldController,
-                              StatementContext statementContext) {
-                            try {
-                              String specId = pair.lhs;
-                              retVal
-                                  .computeIfAbsent(specId, sId -> new ArrayList<>())
-                                  .add(pair.rhs);
-                              return retVal;
-                            } catch (Exception e) {
-                              throw new RuntimeException(e);
-                            }
-                          }
-                        });
-              }
-            }));
+    return dbi.withHandle(
+        new HandleCallback<Map<String, List<VersionedSupervisorSpec>>>() {
+          @Override
+          public Map<String, List<VersionedSupervisorSpec>> withHandle(Handle handle) {
+            return handle
+                .createQuery(
+                    StringUtils.format(
+                        "SELECT id, spec_id, created_date, payload FROM %1$s ORDER BY id DESC",
+                        getSupervisorsTable()))
+                .map(
+                    new ResultSetMapper<Pair<String, VersionedSupervisorSpec>>() {
+                      @Override
+                      public Pair<String, VersionedSupervisorSpec> map(
+                          int index, ResultSet r, StatementContext ctx) throws SQLException {
+                        return Pair.of(
+                            r.getString("spec_id"), createVersionSupervisorSpecFromResponse(r));
+                      }
+                    })
+                .fold(
+                    new HashMap<>(),
+                    new Folder3<
+                        Map<String, List<VersionedSupervisorSpec>>,
+                        Pair<String, VersionedSupervisorSpec>>() {
+                      @Override
+                      public Map<String, List<VersionedSupervisorSpec>> fold(
+                          Map<String, List<VersionedSupervisorSpec>> retVal,
+                          Pair<String, VersionedSupervisorSpec> pair,
+                          FoldController foldController,
+                          StatementContext statementContext) {
+                        try {
+                          String specId = pair.lhs;
+                          retVal.computeIfAbsent(specId, sId -> new ArrayList<>()).add(pair.rhs);
+                          return retVal;
+                        } catch (Exception e) {
+                          throw new RuntimeException(e);
+                        }
+                      }
+                    });
+          }
+        });
   }
 
   @Override
   public List<VersionedSupervisorSpec> getAllForId(String id) {
-    return ImmutableList.copyOf(
-        dbi.withHandle(
-            new HandleCallback<List<VersionedSupervisorSpec>>() {
-              @Override
-              public List<VersionedSupervisorSpec> withHandle(Handle handle) {
-                return handle
-                    .createQuery(
-                        StringUtils.format(
-                            "SELECT id, spec_id, created_date, payload FROM %1$s WHERE spec_id = :spec_id ORDER BY id DESC",
-                            getSupervisorsTable()))
-                    .bind("spec_id", id)
-                    .map(
-                        new ResultSetMapper<VersionedSupervisorSpec>() {
-                          @Override
-                          public VersionedSupervisorSpec map(
-                              int index, ResultSet r, StatementContext ctx) throws SQLException {
-                            return createVersionSupervisorSpecFromResponse(r);
-                          }
-                        })
-                    .list();
-              }
-            }));
+    return dbi.withHandle(
+        new HandleCallback<List<VersionedSupervisorSpec>>() {
+          @Override
+          public List<VersionedSupervisorSpec> withHandle(Handle handle) {
+            return handle
+                .createQuery(
+                    StringUtils.format(
+                        "SELECT id, spec_id, created_date, payload FROM %1$s WHERE spec_id = :spec_id ORDER BY id DESC",
+                        getSupervisorsTable()))
+                .bind("spec_id", id)
+                .map(
+                    new ResultSetMapper<VersionedSupervisorSpec>() {
+                      @Override
+                      public VersionedSupervisorSpec map(
+                          int index, ResultSet r, StatementContext ctx) throws SQLException {
+                        return createVersionSupervisorSpecFromResponse(r);
+                      }
+                    })
+                .list();
+          }
+        });
   }
 
   private VersionedSupervisorSpec createVersionSupervisorSpecFromResponse(ResultSet r)
@@ -191,54 +185,52 @@ public class SQLMetadataSupervisorManager implements MetadataSupervisorManager {
 
   @Override
   public Map<String, SupervisorSpec> getLatest() {
-    return ImmutableMap.copyOf(
-        dbi.withHandle(
-            new HandleCallback<Map<String, SupervisorSpec>>() {
-              @Override
-              public Map<String, SupervisorSpec> withHandle(Handle handle) {
-                return handle
-                    .createQuery(
-                        StringUtils.format(
-                            "SELECT r.spec_id, r.payload "
-                                + "FROM %1$s r "
-                                + "INNER JOIN(SELECT spec_id, max(id) as id FROM %1$s GROUP BY spec_id) latest "
-                                + "ON r.id = latest.id",
-                            getSupervisorsTable()))
-                    .map(
-                        new ResultSetMapper<Pair<String, SupervisorSpec>>() {
-                          @Override
-                          public Pair<String, SupervisorSpec> map(
-                              int index, ResultSet r, StatementContext ctx) throws SQLException {
-                            try {
-                              return Pair.of(
-                                  r.getString("spec_id"),
-                                  jsonMapper.readValue(
-                                      r.getBytes("payload"),
-                                      new TypeReference<SupervisorSpec>() {}));
-                            } catch (IOException e) {
-                              throw new RuntimeException(e);
-                            }
-                          }
-                        })
-                    .fold(
-                        new HashMap<>(),
-                        new Folder3<Map<String, SupervisorSpec>, Pair<String, SupervisorSpec>>() {
-                          @Override
-                          public Map<String, SupervisorSpec> fold(
-                              Map<String, SupervisorSpec> retVal,
-                              Pair<String, SupervisorSpec> stringObjectMap,
-                              FoldController foldController,
-                              StatementContext statementContext) {
-                            try {
-                              retVal.put(stringObjectMap.lhs, stringObjectMap.rhs);
-                              return retVal;
-                            } catch (Exception e) {
-                              throw new RuntimeException(e);
-                            }
-                          }
-                        });
-              }
-            }));
+    return dbi.withHandle(
+        new HandleCallback<Map<String, SupervisorSpec>>() {
+          @Override
+          public Map<String, SupervisorSpec> withHandle(Handle handle) {
+            return handle
+                .createQuery(
+                    StringUtils.format(
+                        "SELECT r.spec_id, r.payload "
+                            + "FROM %1$s r "
+                            + "INNER JOIN(SELECT spec_id, max(id) as id FROM %1$s GROUP BY spec_id) latest "
+                            + "ON r.id = latest.id",
+                        getSupervisorsTable()))
+                .map(
+                    new ResultSetMapper<Pair<String, SupervisorSpec>>() {
+                      @Override
+                      public Pair<String, SupervisorSpec> map(
+                          int index, ResultSet r, StatementContext ctx) throws SQLException {
+                        try {
+                          return Pair.of(
+                              r.getString("spec_id"),
+                              jsonMapper.readValue(
+                                  r.getBytes("payload"), new TypeReference<SupervisorSpec>() {}));
+                        } catch (IOException e) {
+                          throw new RuntimeException(e);
+                        }
+                      }
+                    })
+                .fold(
+                    new HashMap<>(),
+                    new Folder3<Map<String, SupervisorSpec>, Pair<String, SupervisorSpec>>() {
+                      @Override
+                      public Map<String, SupervisorSpec> fold(
+                          Map<String, SupervisorSpec> retVal,
+                          Pair<String, SupervisorSpec> stringObjectMap,
+                          FoldController foldController,
+                          StatementContext statementContext) {
+                        try {
+                          retVal.put(stringObjectMap.lhs, stringObjectMap.rhs);
+                          return retVal;
+                        } catch (Exception e) {
+                          throw new RuntimeException(e);
+                        }
+                      }
+                    });
+          }
+        });
   }
 
   @Override
@@ -252,7 +244,7 @@ public class SQLMetadataSupervisorManager implements MetadataSupervisorManager {
         activeSupervisors.put(entry.getKey(), entry.getValue());
       }
     }
-    return ImmutableMap.copyOf(activeSupervisors);
+    return activeSupervisors;
   }
 
   @Override
@@ -266,7 +258,7 @@ public class SQLMetadataSupervisorManager implements MetadataSupervisorManager {
         activeSupervisors.put(entry.getKey(), entry.getValue());
       }
     }
-    return ImmutableMap.copyOf(activeSupervisors);
+    return activeSupervisors;
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/metadata/SegmentPublisherProvider.java b/server/src/main/java/org/apache/druid/metadata/SegmentPublisherProvider.java
index 61e9c091eb..434e23cb56 100644
--- a/server/src/main/java/org/apache/druid/metadata/SegmentPublisherProvider.java
+++ b/server/src/main/java/org/apache/druid/metadata/SegmentPublisherProvider.java
@@ -30,6 +30,5 @@ import org.apache.druid.segment.realtime.SegmentPublisher;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = NoopSegmentPublisherProvider.class)
-@JsonSubTypes(
-    value = {@JsonSubTypes.Type(name = "metadata", value = MetadataSegmentPublisherProvider.class)})
+@JsonSubTypes(@JsonSubTypes.Type(name = "metadata", value = MetadataSegmentPublisherProvider.class))
 public interface SegmentPublisherProvider extends Provider<SegmentPublisher> {}
diff --git a/server/src/main/java/org/apache/druid/metadata/SqlSegmentsMetadataManager.java b/server/src/main/java/org/apache/druid/metadata/SqlSegmentsMetadataManager.java
index bd536c11d4..3206714a77 100644
--- a/server/src/main/java/org/apache/druid/metadata/SqlSegmentsMetadataManager.java
+++ b/server/src/main/java/org/apache/druid/metadata/SqlSegmentsMetadataManager.java
@@ -19,10 +19,15 @@
 
 package org.apache.druid.metadata;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Collections.emptyIterator;
+import static java.util.Collections.singletonList;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Optional;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Supplier;
 import com.google.common.base.Throwables;
 import com.google.common.collect.ImmutableMap;
@@ -37,7 +42,6 @@ import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
@@ -49,7 +53,6 @@ import java.util.concurrent.Future;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.client.DataSourcesSnapshot;
 import org.apache.druid.client.ImmutableDruidDataSource;
@@ -289,9 +292,7 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
     ReentrantReadWriteLock.WriteLock lock = startStopPollLock.writeLock();
     lock.lock();
     try {
-      if (exec == null) {
-        throw new IllegalStateException(getClass().getName() + " is not started");
-      }
+      checkState(exec != null, "%s is not started", getClass().getName());
       if (isPollingDatabasePeriodically()) {
         return;
       }
@@ -557,7 +558,7 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
   @Override
   public int markAsUsedNonOvershadowedSegmentsInInterval(
       final String dataSource, final Interval interval) {
-    Preconditions.checkNotNull(interval);
+    requireNonNull(interval);
     return doMarkAsUsedNonOvershadowedSegments(dataSource, interval);
   }
 
@@ -568,7 +569,7 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
   private int doMarkAsUsedNonOvershadowedSegments(
       String dataSourceName, @Nullable Interval interval) {
     final List<DataSegment> unusedSegments = new ArrayList<>();
-    final SegmentTimeline timeline = SegmentTimeline.forSegments(Collections.emptyIterator());
+    final SegmentTimeline timeline = SegmentTimeline.forSegments(emptyIterator());
 
     connector.inReadOnlyTransaction(
         (handle, status) -> {
@@ -576,7 +577,7 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
               SqlSegmentsMetadataQuery.forHandle(handle, connector, dbTables.get(), jsonMapper);
 
           final List<Interval> intervals =
-              interval == null ? Intervals.ONLY_ETERNITY : Collections.singletonList(interval);
+              interval == null ? Intervals.ONLY_ETERNITY : singletonList(interval);
 
           try (final CloseableIterator<DataSegment> iterator =
               queryTool.retrieveUsedSegments(dataSourceName, intervals)) {
@@ -622,9 +623,7 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
                     retrieveUnusedSegments(dataSource, segmentIds, handle);
                 List<Interval> unusedSegmentsIntervals =
                     JodaUtils.condenseIntervals(
-                        unusedSegments.stream()
-                            .map(DataSegment::getInterval)
-                            .collect(Collectors.toList()));
+                        unusedSegments.stream().map(DataSegment::getInterval).collect(toList()));
                 try (CloseableIterator<DataSegment> usedSegmentsOverlappingUnusedSegmentsIntervals =
                     retrieveUsedSegmentsOverlappingIntervals(
                         dataSource, unusedSegmentsIntervals, handle)) {
@@ -696,7 +695,7 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
                   }
                 })
             .filter(Objects::nonNull) // Filter nulls corresponding to used segments.
-            .collect(Collectors.toList());
+            .collect(toList());
     if (!unknownSegmentIds.isEmpty()) {
       throw new UnknownSegmentIdsException(unknownSegmentIds);
     }
@@ -753,7 +752,7 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
                   handle ->
                       SqlSegmentsMetadataQuery.forHandle(
                               handle, connector, dbTables.get(), jsonMapper)
-                          .markSegments(Collections.singletonList(segmentId), false));
+                          .markSegments(singletonList(segmentId), false));
 
       return numSegments > 0;
     } catch (RuntimeException e) {
@@ -786,8 +785,8 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
     }
   }
 
-  @Override
-  public @Nullable ImmutableDruidDataSource getImmutableDataSourceWithUsedSegments(
+  @Nullable
+  public @Override ImmutableDruidDataSource getImmutableDataSourceWithUsedSegments(
       String dataSourceName) {
     return getSnapshotOfDataSourcesWithAllUsedSegments().getDataSource(dataSourceName);
   }
@@ -906,7 +905,7 @@ public class SqlSegmentsMetadataManager implements SegmentsMetadataManager {
               }
             });
 
-    Preconditions.checkNotNull(
+    requireNonNull(
         segments, "Unexpected 'null' when polling segments from the db, aborting snapshot update.");
 
     // dataSourcesSnapshot is updated only here and the DataSourcesSnapshot object is immutable. If
diff --git a/server/src/main/java/org/apache/druid/metadata/SqlSegmentsMetadataQuery.java b/server/src/main/java/org/apache/druid/metadata/SqlSegmentsMetadataQuery.java
index 77f5d881d5..fd8aef366f 100644
--- a/server/src/main/java/org/apache/druid/metadata/SqlSegmentsMetadataQuery.java
+++ b/server/src/main/java/org/apache/druid/metadata/SqlSegmentsMetadataQuery.java
@@ -19,15 +19,16 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterators;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.stream.Collectors;
 import org.apache.druid.java.util.common.CloseableIterators;
 import org.apache.druid.java.util.common.IAE;
 import org.apache.druid.java.util.common.Intervals;
@@ -141,7 +142,7 @@ public class SqlSegmentsMetadataQuery {
 
     final int[] segmentChanges = batch.execute();
     return computeNumChangedSegments(
-        segmentIds.stream().map(SegmentId::toString).collect(Collectors.toList()), segmentChanges);
+        segmentIds.stream().map(SegmentId::toString).collect(toList()), segmentChanges);
   }
 
   /**
@@ -184,7 +185,7 @@ public class SqlSegmentsMetadataQuery {
           ImmutableList.copyOf(
               Iterators.transform(
                   retrieveSegments(
-                      dataSource, Collections.singletonList(interval), IntervalMode.CONTAINS, true),
+                      dataSource, singletonList(interval), IntervalMode.CONTAINS, true),
                   DataSegment::getId));
       return markSegments(segments, false);
     }
@@ -215,7 +216,7 @@ public class SqlSegmentsMetadataQuery {
         // finite value. Since
         // we are using string comparison, a segment with this start or end will not be returned
         // otherwise.
-        if (matchMode.equals(IntervalMode.OVERLAPS)) {
+        if (matchMode == IntervalMode.OVERLAPS) {
           sb.append(
               StringUtils.format(
                   " OR (start = '%s' AND \"end\" != '%s' AND \"end\" > :start%d)",
@@ -235,7 +236,7 @@ public class SqlSegmentsMetadataQuery {
       // comparison, a segment with
       // this start and end will not be returned otherwise.
       // Known Issue: https://github.com/apache/druid/issues/12860
-      if (matchMode.equals(IntervalMode.OVERLAPS)) {
+      if (matchMode == IntervalMode.OVERLAPS) {
         sb.append(
             StringUtils.format(
                 " OR (start = '%s' AND \"end\" = '%s')",
diff --git a/server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java b/server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java
index cd5ad8c758..7a87bf446f 100644
--- a/server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java
+++ b/server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.metadata.input;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.SerializerProvider;
-import com.google.common.base.Preconditions;
 import java.io.File;
 import java.io.FileOutputStream;
 import java.io.IOException;
@@ -61,8 +62,7 @@ public class SqlEntity implements InputEntity {
       ObjectMapper objectMapper) {
     this.sql = sql;
     this.sqlFirehoseDatabaseConnector =
-        Preconditions.checkNotNull(
-            sqlFirehoseDatabaseConnector, "SQL Metadata Connector not configured!");
+        requireNonNull(sqlFirehoseDatabaseConnector, "SQL Metadata Connector not configured!");
     this.foldCase = foldCase;
     this.objectMapper = objectMapper;
   }
diff --git a/server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java b/server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java
index f8712c5c84..fabf83b013 100644
--- a/server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java
+++ b/server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java
@@ -19,14 +19,17 @@
 
 package org.apache.druid.metadata.input;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Collections.singleton;
+import static java.util.Collections.singletonList;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonIgnore;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import java.io.File;
-import java.util.Collections;
 import java.util.List;
 import java.util.Objects;
 import java.util.Set;
@@ -57,13 +60,12 @@ public class SqlInputSource extends AbstractInputSource implements SplittableInp
       @JsonProperty("foldCase") boolean foldCase,
       @JsonProperty("database") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,
       @JacksonInject @Smile ObjectMapper objectMapper) {
-    Preconditions.checkArgument(sqls.size() > 0, "No SQL queries provided");
+    checkArgument(!sqls.isEmpty(), "No SQL queries provided");
 
     this.sqls = sqls;
     this.foldCase = foldCase;
     this.sqlFirehoseDatabaseConnector =
-        Preconditions.checkNotNull(
-            sqlFirehoseDatabaseConnector, "SQL Metadata Connector not configured!");
+        requireNonNull(sqlFirehoseDatabaseConnector, "SQL Metadata Connector not configured!");
     this.objectMapper = objectMapper;
   }
 
@@ -71,7 +73,7 @@ public class SqlInputSource extends AbstractInputSource implements SplittableInp
   @Nonnull
   @Override
   public Set<String> getTypes() {
-    return Collections.singleton(TYPE_KEY);
+    return singleton(TYPE_KEY);
   }
 
   @JsonProperty
@@ -103,10 +105,7 @@ public class SqlInputSource extends AbstractInputSource implements SplittableInp
   @Override
   public SplittableInputSource<String> withSplit(InputSplit<String> split) {
     return new SqlInputSource(
-        Collections.singletonList(split.get()),
-        foldCase,
-        sqlFirehoseDatabaseConnector,
-        objectMapper);
+        singletonList(split.get()), foldCase, sqlFirehoseDatabaseConnector, objectMapper);
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/metadata/input/SqlReader.java b/server/src/main/java/org/apache/druid/metadata/input/SqlReader.java
index 8888ddbc80..581cc87d6d 100644
--- a/server/src/main/java/org/apache/druid/metadata/input/SqlReader.java
+++ b/server/src/main/java/org/apache/druid/metadata/input/SqlReader.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.metadata.input;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import org.apache.druid.data.input.InputEntity;
@@ -63,10 +64,8 @@ public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>
     final InputEntity.CleanableFile resultFile =
         closer.register(source.fetch(temporaryDirectory, null));
     FileInputStream inputStream = new FileInputStream(resultFile.file());
-    JsonIterator<Map<String, Object>> jsonIterator =
-        new JsonIterator<>(
-            new TypeReference<Map<String, Object>>() {}, inputStream, closer, objectMapper);
-    return jsonIterator;
+    return new JsonIterator<>(
+        new TypeReference<Map<String, Object>>() {}, inputStream, closer, objectMapper);
   }
 
   @Override
@@ -77,11 +76,11 @@ public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>
   @Override
   protected List<InputRow> parseInputRows(Map<String, Object> intermediateRow)
       throws ParseException {
-    return Collections.singletonList(MapInputRowParser.parse(inputRowSchema, intermediateRow));
+    return singletonList(MapInputRowParser.parse(inputRowSchema, intermediateRow));
   }
 
   @Override
   protected List<Map<String, Object>> toMap(Map<String, Object> intermediateRow) {
-    return Collections.singletonList(intermediateRow);
+    return singletonList(intermediateRow);
   }
 }
diff --git a/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java b/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java
index 9a9a184b45..4144f39b42 100644
--- a/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java
+++ b/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.query;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.SerializerProvider;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
@@ -119,7 +120,7 @@ public class ResultLevelCachingQueryRunner<T> implements QueryRunner<T> {
             new SequenceWrapper() {
               @Override
               public void after(boolean isDone, Throwable thrown) {
-                Preconditions.checkNotNull(
+                requireNonNull(
                     resultLevelCachePopulator,
                     "ResultLevelCachePopulator cannot be null during cache population");
                 if (thrown != null) {
@@ -244,7 +245,7 @@ public class ResultLevelCachingQueryRunner<T> implements QueryRunner<T> {
     }
 
     private void cacheResultEntry(T resultEntry, Function<T, Object> cacheFn) {
-      Preconditions.checkNotNull(cacheObjectStream, "cacheObjectStream");
+      requireNonNull(cacheObjectStream, "cacheObjectStream");
       int cacheLimit = cacheConfig.getResultLevelCacheLimit();
       try (JsonGenerator gen = mapper.getFactory().createGenerator(cacheObjectStream)) {
         JacksonUtils.writeObjectUsingSerializerProvider(
@@ -262,9 +263,7 @@ public class ResultLevelCachingQueryRunner<T> implements QueryRunner<T> {
 
     public void populateResults() {
       CacheUtil.populateResultCache(
-          cache,
-          key,
-          Preconditions.checkNotNull(cacheObjectStream, "cacheObjectStream").toByteArray());
+          cache, key, requireNonNull(cacheObjectStream, "cacheObjectStream").toByteArray());
     }
   }
 }
diff --git a/server/src/main/java/org/apache/druid/query/RetryQueryRunner.java b/server/src/main/java/org/apache/druid/query/RetryQueryRunner.java
index 4820ff24b9..b41c2f5175 100644
--- a/server/src/main/java/org/apache/druid/query/RetryQueryRunner.java
+++ b/server/src/main/java/org/apache/druid/query/RetryQueryRunner.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.query;
 
+import static com.google.common.base.Preconditions.checkNotNull;
+import static java.util.Collections.emptyList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.NoSuchElementException;
@@ -131,13 +132,13 @@ public class RetryQueryRunner<T> implements QueryRunner<T> {
     // the broker.
     // The remainingResponses MUST be not null but 0 in the responseContext at this point.
     final ConcurrentHashMap<String, Integer> idToRemainingResponses =
-        Preconditions.checkNotNull(
+        checkNotNull(
             context.getRemainingResponses(),
             "%s in responseContext",
             Keys.REMAINING_RESPONSES_FROM_QUERY_SERVERS.getName());
 
     final int remainingResponses =
-        Preconditions.checkNotNull(
+        checkNotNull(
             idToRemainingResponses.get(queryPlus.getQuery().getMostSpecificId()),
             "Number of remaining responses for query[%s]",
             queryPlus.getQuery().getMostSpecificId());
@@ -153,7 +154,7 @@ public class RetryQueryRunner<T> implements QueryRunner<T> {
     // missing segments.
     final List<SegmentDescriptor> maybeMissingSegments = context.getMissingSegments();
     if (maybeMissingSegments == null) {
-      return Collections.emptyList();
+      return emptyList();
     }
 
     return jsonMapper.convertValue(
diff --git a/server/src/main/java/org/apache/druid/query/lookup/LookupListeningAnnouncerConfig.java b/server/src/main/java/org/apache/druid/query/lookup/LookupListeningAnnouncerConfig.java
index b720b8a4b0..23be9b3604 100644
--- a/server/src/main/java/org/apache/druid/query/lookup/LookupListeningAnnouncerConfig.java
+++ b/server/src/main/java/org/apache/druid/query/lookup/LookupListeningAnnouncerConfig.java
@@ -19,10 +19,12 @@
 
 package org.apache.druid.query.lookup;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import org.apache.druid.java.util.common.StringUtils;
 import org.apache.druid.server.metrics.DataSourceTaskIdHolder;
 
@@ -43,13 +45,13 @@ class LookupListeningAnnouncerConfig {
   }
 
   public String getLookupTier() {
-    Preconditions.checkArgument(
-        !(lookupTierIsDatasource && null != lookupTier),
+    checkArgument(
+        !(lookupTierIsDatasource && lookupTier != null),
         "Cannot specify both `lookupTier` and `lookupTierIsDatasource`");
     final String lookupTier =
         lookupTierIsDatasource ? dataSourceTaskIdHolder.getDataSource() : this.lookupTier;
 
-    return Preconditions.checkNotNull(
+    return checkNotNull(
         lookupTier == null ? DEFAULT_TIER : StringUtils.emptyToNullNonDruidDataString(lookupTier),
         "Cannot have empty lookup tier from %s",
         lookupTierIsDatasource ? "bound value" : LookupModule.PROPERTY_BASE);
diff --git a/server/src/main/java/org/apache/druid/query/lookup/LookupModule.java b/server/src/main/java/org/apache/druid/query/lookup/LookupModule.java
index 37320ae6e0..36867a7205 100644
--- a/server/src/main/java/org/apache/druid/query/lookup/LookupModule.java
+++ b/server/src/main/java/org/apache/druid/query/lookup/LookupModule.java
@@ -70,8 +70,8 @@ public class LookupModule implements DruidModule {
         );
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public LookupNodeService getLookupNodeService(
       LookupListeningAnnouncerConfig lookupListeningAnnouncerConfig) {
     return new LookupNodeService(lookupListeningAnnouncerConfig.getLookupTier());
diff --git a/server/src/main/java/org/apache/druid/query/lookup/LookupReferencesManager.java b/server/src/main/java/org/apache/druid/query/lookup/LookupReferencesManager.java
index 6466ca9297..f2acf86c6f 100644
--- a/server/src/main/java/org/apache/druid/query/lookup/LookupReferencesManager.java
+++ b/server/src/main/java/org/apache/druid/query/lookup/LookupReferencesManager.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.query.lookup;
 
+import static com.google.common.base.Preconditions.checkState;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Strings;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
@@ -265,14 +266,14 @@ public class LookupReferencesManager implements LookupExtractorFactoryContainerP
 
   public void add(
       String lookupName, LookupExtractorFactoryContainer lookupExtractorFactoryContainer) {
-    Preconditions.checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
+    checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
     addNotice(
         new LoadNotice(
             lookupName, lookupExtractorFactoryContainer, lookupConfig.getLookupStartRetries()));
   }
 
   public void remove(String lookupName) {
-    Preconditions.checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
+    checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
     addNotice(new DropNotice(lookupName));
   }
 
@@ -297,7 +298,7 @@ public class LookupReferencesManager implements LookupExtractorFactoryContainerP
 
   @Override
   public Optional<LookupExtractorFactoryContainer> get(String lookupName) {
-    Preconditions.checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
+    checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
     return Optional.ofNullable(stateRef.get().lookupMap.get(lookupName));
   }
 
@@ -308,7 +309,7 @@ public class LookupReferencesManager implements LookupExtractorFactoryContainerP
 
   // Note that this should ensure that "toLoad" and "toDrop" are disjoint.
   LookupsState<LookupExtractorFactoryContainer> getAllLookupsState() {
-    Preconditions.checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
+    checkState(lifecycleLock.awaitStarted(1, TimeUnit.MILLISECONDS));
 
     LookupUpdateState lookupUpdateState = stateRef.get();
 
diff --git a/server/src/main/java/org/apache/druid/query/lookup/LookupSerdeModule.java b/server/src/main/java/org/apache/druid/query/lookup/LookupSerdeModule.java
index a8cb993ccd..8a672a1aae 100644
--- a/server/src/main/java/org/apache/druid/query/lookup/LookupSerdeModule.java
+++ b/server/src/main/java/org/apache/druid/query/lookup/LookupSerdeModule.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.query.lookup;
 
+import static java.util.Collections.emptySet;
+
 import com.fasterxml.jackson.databind.Module;
 import com.fasterxml.jackson.databind.jsontype.NamedType;
 import com.fasterxml.jackson.databind.module.SimpleModule;
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Binder;
-import java.util.Collections;
 import java.util.List;
 import java.util.Optional;
 import java.util.Set;
@@ -69,7 +70,7 @@ public class LookupSerdeModule implements DruidModule {
       implements LookupExtractorFactoryContainerProvider {
     @Override
     public Set<String> getAllLookupNames() {
-      return Collections.emptySet();
+      return emptySet();
     }
 
     @Override
diff --git a/server/src/main/java/org/apache/druid/query/lookup/MapLookupExtractorFactory.java b/server/src/main/java/org/apache/druid/query/lookup/MapLookupExtractorFactory.java
index 965bb62804..180a9f5ffd 100644
--- a/server/src/main/java/org/apache/druid/query/lookup/MapLookupExtractorFactory.java
+++ b/server/src/main/java/org/apache/druid/query/lookup/MapLookupExtractorFactory.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.query.lookup;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.annotation.JsonTypeName;
-import com.google.common.base.Preconditions;
 import java.util.Map;
 import javax.annotation.Nullable;
 import javax.ws.rs.GET;
@@ -43,7 +44,7 @@ public class MapLookupExtractorFactory implements LookupExtractorFactory {
   public MapLookupExtractorFactory(
       @JsonProperty("map") Map<String, String> map,
       @JsonProperty("isOneToOne") boolean isOneToOne) {
-    this.map = Preconditions.checkNotNull(map, "map cannot be null");
+    this.map = requireNonNull(map, "map cannot be null");
     this.isOneToOne = isOneToOne;
     this.lookupExtractor = new MapLookupExtractor(map, isOneToOne);
     this.lookupIntrospectHandler = new MapLookupIntrospectionHandler(this.map);
@@ -102,8 +103,7 @@ public class MapLookupExtractorFactory implements LookupExtractorFactory {
   @Override
   public int hashCode() {
     int result = map.hashCode();
-    result = 31 * result + (isOneToOne ? 1 : 0);
-    return result;
+    return 31 * result + (isOneToOne ? 1 : 0);
   }
 
   public static class MapLookupIntrospectionHandler implements LookupIntrospectHandler {
diff --git a/server/src/main/java/org/apache/druid/rpc/DiscoveryServiceLocator.java b/server/src/main/java/org/apache/druid/rpc/DiscoveryServiceLocator.java
index 12074c6d9b..ef11370ceb 100644
--- a/server/src/main/java/org/apache/druid/rpc/DiscoveryServiceLocator.java
+++ b/server/src/main/java/org/apache/druid/rpc/DiscoveryServiceLocator.java
@@ -19,7 +19,6 @@
 
 package org.apache.druid.rpc;
 
-import com.google.common.collect.ImmutableSet;
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.common.util.concurrent.SettableFuture;
@@ -72,8 +71,7 @@ public class DiscoveryServiceLocator implements ServiceLocator {
       if (closed) {
         return Futures.immediateFuture(ServiceLocations.closed());
       } else if (initialized) {
-        return Futures.immediateFuture(
-            ServiceLocations.forLocations(ImmutableSet.copyOf(locations)));
+        return Futures.immediateFuture(ServiceLocations.forLocations(locations));
       } else {
         if (pendingFuture == null) {
           pendingFuture = SettableFuture.create();
@@ -97,8 +95,8 @@ public class DiscoveryServiceLocator implements ServiceLocator {
     }
   }
 
-  @Override
   @LifecycleStop
+  @Override
   public void close() {
     synchronized (this) {
       // Idempotent: can call close() multiple times so long as start() has already been called.
@@ -142,7 +140,7 @@ public class DiscoveryServiceLocator implements ServiceLocator {
         initialized = true;
 
         if (pendingFuture != null) {
-          pendingFuture.set(ServiceLocations.forLocations(ImmutableSet.copyOf(locations)));
+          pendingFuture.set(ServiceLocations.forLocations(locations));
           pendingFuture = null;
         }
       }
diff --git a/server/src/main/java/org/apache/druid/rpc/RequestBuilder.java b/server/src/main/java/org/apache/druid/rpc/RequestBuilder.java
index 7ca7d0419e..e02930574a 100644
--- a/server/src/main/java/org/apache/druid/rpc/RequestBuilder.java
+++ b/server/src/main/java/org/apache/druid/rpc/RequestBuilder.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.rpc;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.jaxrs.smile.SmileMediaTypes;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.HashMultimap;
 import com.google.common.collect.Multimap;
 import java.net.MalformedURLException;
@@ -52,9 +53,9 @@ public class RequestBuilder {
   private Duration timeout = DEFAULT_TIMEOUT;
 
   public RequestBuilder(final HttpMethod method, final String encodedPathAndQueryString) {
-    this.method = Preconditions.checkNotNull(method, "method");
+    this.method = requireNonNull(method, "method");
     this.encodedPathAndQueryString =
-        Preconditions.checkNotNull(encodedPathAndQueryString, "encodedPathAndQueryString");
+        requireNonNull(encodedPathAndQueryString, "encodedPathAndQueryString");
 
     if (!encodedPathAndQueryString.startsWith("/")) {
       throw new IAE("Path must start with '/'");
@@ -67,15 +68,15 @@ public class RequestBuilder {
   }
 
   public RequestBuilder content(final String contentType, final byte[] content) {
-    this.contentType = Preconditions.checkNotNull(contentType, "contentType");
-    this.content = Preconditions.checkNotNull(content, "content");
+    this.contentType = requireNonNull(contentType, "contentType");
+    this.content = requireNonNull(content, "content");
     return this;
   }
 
   public RequestBuilder jsonContent(final ObjectMapper jsonMapper, final Object content) {
     try {
       this.contentType = MediaType.APPLICATION_JSON;
-      this.content = jsonMapper.writeValueAsBytes(Preconditions.checkNotNull(content, "content"));
+      this.content = jsonMapper.writeValueAsBytes(requireNonNull(content, "content"));
       return this;
     } catch (JsonProcessingException e) {
       throw new RuntimeException(e);
@@ -85,7 +86,7 @@ public class RequestBuilder {
   public RequestBuilder smileContent(final ObjectMapper smileMapper, final Object content) {
     try {
       this.contentType = SmileMediaTypes.APPLICATION_JACKSON_SMILE;
-      this.content = smileMapper.writeValueAsBytes(Preconditions.checkNotNull(content, "content"));
+      this.content = smileMapper.writeValueAsBytes(requireNonNull(content, "content"));
       return this;
     } catch (JsonProcessingException e) {
       throw new RuntimeException(e);
@@ -93,7 +94,7 @@ public class RequestBuilder {
   }
 
   public RequestBuilder timeout(final Duration timeout) {
-    this.timeout = Preconditions.checkNotNull(timeout, "timeout");
+    this.timeout = requireNonNull(timeout, "timeout");
     return this;
   }
 
@@ -166,8 +167,7 @@ public class RequestBuilder {
   @Override
   public int hashCode() {
     int result = Objects.hash(method, encodedPathAndQueryString, headers, contentType, timeout);
-    result = 31 * result + Arrays.hashCode(content);
-    return result;
+    return 31 * result + Arrays.hashCode(content);
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/rpc/ServiceClientImpl.java b/server/src/main/java/org/apache/druid/rpc/ServiceClientImpl.java
index 05dae12259..327734e0f2 100644
--- a/server/src/main/java/org/apache/druid/rpc/ServiceClientImpl.java
+++ b/server/src/main/java/org/apache/druid/rpc/ServiceClientImpl.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.rpc;
 
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
 import com.google.common.util.concurrent.FutureCallback;
@@ -66,11 +67,11 @@ public class ServiceClientImpl implements ServiceClient {
       final ServiceLocator serviceLocator,
       final ServiceRetryPolicy retryPolicy,
       final ScheduledExecutorService connectExec) {
-    this.serviceName = Preconditions.checkNotNull(serviceName, "serviceName");
-    this.httpClient = Preconditions.checkNotNull(httpClient, "httpClient");
-    this.serviceLocator = Preconditions.checkNotNull(serviceLocator, "serviceLocator");
-    this.retryPolicy = Preconditions.checkNotNull(retryPolicy, "retryPolicy");
-    this.connectExec = Preconditions.checkNotNull(connectExec, "connectExec");
+    this.serviceName = requireNonNull(serviceName, "serviceName");
+    this.httpClient = requireNonNull(httpClient, "httpClient");
+    this.serviceLocator = requireNonNull(serviceLocator, "serviceLocator");
+    this.retryPolicy = requireNonNull(retryPolicy, "retryPolicy");
+    this.connectExec = requireNonNull(connectExec, "connectExec");
 
     if (retryPolicy.maxAttempts() == 0) {
       throw new IAE("Invalid maxAttempts[%d] in retry policy", retryPolicy.maxAttempts());
diff --git a/server/src/main/java/org/apache/druid/rpc/ServiceLocation.java b/server/src/main/java/org/apache/druid/rpc/ServiceLocation.java
index 38eb8bedf9..8e17ee971e 100644
--- a/server/src/main/java/org/apache/druid/rpc/ServiceLocation.java
+++ b/server/src/main/java/org/apache/druid/rpc/ServiceLocation.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.rpc;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import java.util.Objects;
 import org.apache.druid.server.DruidNode;
 
@@ -32,10 +33,10 @@ public class ServiceLocation {
 
   public ServiceLocation(
       final String host, final int plaintextPort, final int tlsPort, final String basePath) {
-    this.host = Preconditions.checkNotNull(host, "host");
+    this.host = requireNonNull(host, "host");
     this.plaintextPort = plaintextPort;
     this.tlsPort = tlsPort;
-    this.basePath = Preconditions.checkNotNull(basePath, "basePath");
+    this.basePath = requireNonNull(basePath, "basePath");
   }
 
   public static ServiceLocation fromDruidNode(final DruidNode druidNode) {
diff --git a/server/src/main/java/org/apache/druid/rpc/ServiceLocations.java b/server/src/main/java/org/apache/druid/rpc/ServiceLocations.java
index e4c51f31c9..2ac20cc1f2 100644
--- a/server/src/main/java/org/apache/druid/rpc/ServiceLocations.java
+++ b/server/src/main/java/org/apache/druid/rpc/ServiceLocations.java
@@ -19,8 +19,10 @@
 
 package org.apache.druid.rpc;
 
-import com.google.common.base.Preconditions;
-import java.util.Collections;
+import static java.util.Collections.emptySet;
+import static java.util.Collections.singleton;
+import static java.util.Objects.requireNonNull;
+
 import java.util.Objects;
 import java.util.Set;
 import org.apache.druid.java.util.common.IAE;
@@ -31,7 +33,7 @@ public class ServiceLocations {
   private final boolean closed;
 
   private ServiceLocations(final Set<ServiceLocation> locations, final boolean closed) {
-    this.locations = Preconditions.checkNotNull(locations, "locations");
+    this.locations = requireNonNull(locations, "locations");
     this.closed = closed;
 
     if (closed && !locations.isEmpty()) {
@@ -40,7 +42,7 @@ public class ServiceLocations {
   }
 
   public static ServiceLocations forLocation(final ServiceLocation location) {
-    return new ServiceLocations(Collections.singleton(Preconditions.checkNotNull(location)), false);
+    return new ServiceLocations(singleton(requireNonNull(location)), false);
   }
 
   public static ServiceLocations forLocations(final Set<ServiceLocation> locations) {
@@ -48,7 +50,7 @@ public class ServiceLocations {
   }
 
   public static ServiceLocations closed() {
-    return new ServiceLocations(Collections.emptySet(), true);
+    return new ServiceLocations(emptySet(), true);
   }
 
   public Set<ServiceLocation> getLocations() {
diff --git a/server/src/main/java/org/apache/druid/rpc/guice/ServiceClientModule.java b/server/src/main/java/org/apache/druid/rpc/guice/ServiceClientModule.java
index 9fe9ed889c..a5700f3077 100644
--- a/server/src/main/java/org/apache/druid/rpc/guice/ServiceClientModule.java
+++ b/server/src/main/java/org/apache/druid/rpc/guice/ServiceClientModule.java
@@ -50,9 +50,9 @@ public class ServiceClientModule implements DruidModule {
     // Nothing to do.
   }
 
-  @Provides
-  @LazySingleton
   @EscalatedGlobal
+  @LazySingleton
+  @Provides
   public ServiceClientFactory makeServiceClientFactory(
       @EscalatedGlobal final HttpClient httpClient) {
     final ScheduledExecutorService connectExec =
@@ -60,9 +60,9 @@ public class ServiceClientModule implements DruidModule {
     return new ServiceClientFactoryImpl(httpClient, connectExec);
   }
 
-  @Provides
-  @ManageLifecycle
   @IndexingService
+  @ManageLifecycle
+  @Provides
   public ServiceLocator makeOverlordServiceLocator(
       final DruidNodeDiscoveryProvider discoveryProvider) {
     return new DiscoveryServiceLocator(discoveryProvider, NodeRole.OVERLORD);
diff --git a/server/src/main/java/org/apache/druid/rpc/indexing/OverlordClientImpl.java b/server/src/main/java/org/apache/druid/rpc/indexing/OverlordClientImpl.java
index ae58cd8b6e..744ba1558b 100644
--- a/server/src/main/java/org/apache/druid/rpc/indexing/OverlordClientImpl.java
+++ b/server/src/main/java/org/apache/druid/rpc/indexing/OverlordClientImpl.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.rpc.indexing;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.util.concurrent.ListenableFuture;
 import java.io.IOException;
 import java.util.Map;
@@ -45,8 +47,8 @@ public class OverlordClientImpl implements OverlordClient {
   private final ObjectMapper jsonMapper;
 
   public OverlordClientImpl(final ServiceClient client, final ObjectMapper jsonMapper) {
-    this.client = Preconditions.checkNotNull(client, "client");
-    this.jsonMapper = Preconditions.checkNotNull(jsonMapper, "jsonMapper");
+    this.client = requireNonNull(client, "client");
+    this.jsonMapper = requireNonNull(jsonMapper, "jsonMapper");
   }
 
   @Override
@@ -61,7 +63,7 @@ public class OverlordClientImpl implements OverlordClient {
               deserialize(holder, JacksonUtils.TYPE_REFERENCE_MAP_STRING_OBJECT);
           final String returnedTaskId = (String) map.get("task");
 
-          Preconditions.checkState(
+          checkState(
               taskId.equals(returnedTaskId),
               "Got a different taskId[%s]. Expected taskId[%s]",
               returnedTaskId,
diff --git a/server/src/main/java/org/apache/druid/rpc/indexing/SpecificTaskRetryPolicy.java b/server/src/main/java/org/apache/druid/rpc/indexing/SpecificTaskRetryPolicy.java
index d47045b162..1bbba85546 100644
--- a/server/src/main/java/org/apache/druid/rpc/indexing/SpecificTaskRetryPolicy.java
+++ b/server/src/main/java/org/apache/druid/rpc/indexing/SpecificTaskRetryPolicy.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.rpc.indexing;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import org.apache.druid.java.util.common.StringUtils;
 import org.apache.druid.rpc.ServiceRetryPolicy;
 import org.apache.druid.rpc.StandardRetryPolicy;
@@ -41,8 +42,8 @@ public class SpecificTaskRetryPolicy implements ServiceRetryPolicy {
   private final ServiceRetryPolicy baseRetryPolicy;
 
   public SpecificTaskRetryPolicy(final String taskId, final ServiceRetryPolicy baseRetryPolicy) {
-    this.taskId = Preconditions.checkNotNull(taskId, "taskId");
-    this.baseRetryPolicy = Preconditions.checkNotNull(baseRetryPolicy, "baseRetryPolicy");
+    this.taskId = requireNonNull(taskId, "taskId");
+    this.baseRetryPolicy = requireNonNull(baseRetryPolicy, "baseRetryPolicy");
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/rpc/indexing/SpecificTaskServiceLocator.java b/server/src/main/java/org/apache/druid/rpc/indexing/SpecificTaskServiceLocator.java
index 8aa739edc6..808eeb91ad 100644
--- a/server/src/main/java/org/apache/druid/rpc/indexing/SpecificTaskServiceLocator.java
+++ b/server/src/main/java/org/apache/druid/rpc/indexing/SpecificTaskServiceLocator.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.rpc.indexing;
 
+import static java.util.Collections.emptySet;
+
 import com.google.common.util.concurrent.FutureCallback;
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.common.util.concurrent.SettableFuture;
 import com.google.errorprone.annotations.concurrent.GuardedBy;
-import java.util.Collections;
 import org.apache.druid.client.indexing.TaskStatusResponse;
 import org.apache.druid.indexer.TaskLocation;
 import org.apache.druid.indexer.TaskState;
@@ -140,7 +141,7 @@ public class SpecificTaskServiceLocator implements ServiceLocator {
                     if (lastKnownState != TaskState.RUNNING) {
                       pendingFuture.set(ServiceLocations.closed());
                     } else if (lastKnownLocation == null) {
-                      pendingFuture.set(ServiceLocations.forLocations(Collections.emptySet()));
+                      pendingFuture.set(ServiceLocations.forLocations(emptySet()));
                     } else {
                       pendingFuture.set(ServiceLocations.forLocation(lastKnownLocation));
                     }
diff --git a/server/src/main/java/org/apache/druid/segment/InlineSegmentWrangler.java b/server/src/main/java/org/apache/druid/segment/InlineSegmentWrangler.java
index dc82a78244..7bf306a538 100644
--- a/server/src/main/java/org/apache/druid/segment/InlineSegmentWrangler.java
+++ b/server/src/main/java/org/apache/druid/segment/InlineSegmentWrangler.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.segment;
 
+import static java.util.Collections.singletonList;
+
 import java.util.ArrayList;
-import java.util.Collections;
 import org.apache.druid.java.util.common.guava.Sequences;
 import org.apache.druid.query.DataSource;
 import org.apache.druid.query.InlineDataSource;
@@ -42,7 +43,7 @@ public class InlineSegmentWrangler implements SegmentWrangler {
     final InlineDataSource inlineDataSource = (InlineDataSource) dataSource;
 
     if (inlineDataSource.rowsAreArrayList()) {
-      return Collections.singletonList(
+      return singletonList(
           new ArrayListSegment<>(
               SegmentId.dummy(SEGMENT_ID),
               (ArrayList<Object[]>) inlineDataSource.getRowsAsList(),
@@ -50,7 +51,7 @@ public class InlineSegmentWrangler implements SegmentWrangler {
               inlineDataSource.getRowSignature()));
     }
 
-    return Collections.singletonList(
+    return singletonList(
         new RowBasedSegment<>(
             SegmentId.dummy(SEGMENT_ID),
             Sequences.simple(inlineDataSource.getRows()),
diff --git a/server/src/main/java/org/apache/druid/segment/LookupSegmentWrangler.java b/server/src/main/java/org/apache/druid/segment/LookupSegmentWrangler.java
index 6ae28c8a18..4025cd98a6 100644
--- a/server/src/main/java/org/apache/druid/segment/LookupSegmentWrangler.java
+++ b/server/src/main/java/org/apache/druid/segment/LookupSegmentWrangler.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.segment;
 
+import static java.util.Collections.emptyList;
+
 import com.google.inject.Inject;
 import java.util.Collections;
 import java.util.Optional;
@@ -57,6 +59,6 @@ public class LookupSegmentWrangler implements SegmentWrangler {
                 Collections.<Segment>singletonList(
                     new LookupSegment(
                         lookupDataSource.getLookupName(), container.getLookupExtractorFactory())))
-        .orElse(Collections.emptyList());
+        .orElse(emptyList());
   }
 }
diff --git a/server/src/main/java/org/apache/druid/segment/indexing/DataSchema.java b/server/src/main/java/org/apache/druid/segment/indexing/DataSchema.java
index 8256f36856..e18d85159d 100644
--- a/server/src/main/java/org/apache/druid/segment/indexing/DataSchema.java
+++ b/server/src/main/java/org/apache/druid/segment/indexing/DataSchema.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.segment.indexing;
 
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.joining;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonInclude;
@@ -26,7 +29,6 @@ import com.fasterxml.jackson.annotation.JsonInclude.Include;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Strings;
 import com.google.common.collect.Multiset;
 import com.google.common.collect.TreeMultiset;
@@ -37,7 +39,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 import java.util.TreeMap;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.common.utils.IdUtils;
 import org.apache.druid.data.input.impl.DimensionsSpec;
@@ -91,9 +92,7 @@ public class DataSchema {
         dimensionsSpec == null
             ? null
             : computeDimensionsSpec(
-                Preconditions.checkNotNull(timestampSpec, "timestampSpec"),
-                dimensionsSpec,
-                this.aggregators);
+                requireNonNull(timestampSpec, "timestampSpec"), dimensionsSpec, this.aggregators);
 
     if (granularitySpec == null) {
       log.warn("No granularitySpec has been specified. Using UniformGranularitySpec as default.");
@@ -260,7 +259,7 @@ public class DataSchema {
                                 entry.getCount() == 1
                                     ? ""
                                     : StringUtils.format(" (%d occurrences)", entry.getCount())))
-                    .collect(Collectors.joining(", "))));
+                    .collect(joining(", "))));
       }
     }
 
@@ -276,8 +275,8 @@ public class DataSchema {
     return dataSource;
   }
 
-  @Nullable
   @JsonProperty("timestampSpec")
+  @Nullable
   private TimestampSpec getGivenTimestampSpec() {
     return timestampSpec;
   }
@@ -285,15 +284,13 @@ public class DataSchema {
   public TimestampSpec getTimestampSpec() {
     if (timestampSpec == null) {
       timestampSpec =
-          Preconditions.checkNotNull(getParser(), "inputRowParser")
-              .getParseSpec()
-              .getTimestampSpec();
+          requireNonNull(getParser(), "inputRowParser").getParseSpec().getTimestampSpec();
     }
     return timestampSpec;
   }
 
-  @Nullable
   @JsonProperty("dimensionsSpec")
+  @Nullable
   private DimensionsSpec getGivenDimensionsSpec() {
     return dimensionsSpec;
   }
@@ -303,9 +300,7 @@ public class DataSchema {
       dimensionsSpec =
           computeDimensionsSpec(
               getTimestampSpec(),
-              Preconditions.checkNotNull(getParser(), "inputRowParser")
-                  .getParseSpec()
-                  .getDimensionsSpec(),
+              requireNonNull(getParser(), "inputRowParser").getParseSpec().getDimensionsSpec(),
               aggregators);
     }
     return dimensionsSpec;
@@ -327,9 +322,9 @@ public class DataSchema {
   }
 
   @Deprecated
+  @JsonInclude(Include.NON_NULL)
   @JsonProperty("parser")
   @Nullable
-  @JsonInclude(Include.NON_NULL)
   public Map<String, Object> getParserMap() {
     return parserMap;
   }
diff --git a/server/src/main/java/org/apache/druid/segment/indexing/IOConfig.java b/server/src/main/java/org/apache/druid/segment/indexing/IOConfig.java
index 8cdbbc9fb5..8ef0748b12 100644
--- a/server/src/main/java/org/apache/druid/segment/indexing/IOConfig.java
+++ b/server/src/main/java/org/apache/druid/segment/indexing/IOConfig.java
@@ -24,5 +24,5 @@ import com.fasterxml.jackson.annotation.JsonTypeInfo;
 
 /** */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(value = {@JsonSubTypes.Type(name = "realtime", value = RealtimeIOConfig.class)})
+@JsonSubTypes(@JsonSubTypes.Type(name = "realtime", value = RealtimeIOConfig.class))
 public interface IOConfig {}
diff --git a/server/src/main/java/org/apache/druid/segment/indexing/RealtimeTuningConfig.java b/server/src/main/java/org/apache/druid/segment/indexing/RealtimeTuningConfig.java
index a8be43c176..e3bd9f444f 100644
--- a/server/src/main/java/org/apache/druid/segment/indexing/RealtimeTuningConfig.java
+++ b/server/src/main/java/org/apache/druid/segment/indexing/RealtimeTuningConfig.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.segment.indexing;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.io.File;
 import javax.annotation.Nullable;
 import org.apache.druid.indexer.partitions.PartitionsSpec;
@@ -157,11 +159,10 @@ public class RealtimeTuningConfig implements AppenderatorConfig {
         handoffConditionTimeout == null
             ? DEFAULT_HANDOFF_CONDITION_TIMEOUT
             : handoffConditionTimeout;
-    Preconditions.checkArgument(
-        this.handoffConditionTimeout >= 0, "handoffConditionTimeout must be >= 0");
+    checkArgument(this.handoffConditionTimeout >= 0, "handoffConditionTimeout must be >= 0");
 
     this.alertTimeout = alertTimeout == null ? DEFAULT_ALERT_TIMEOUT : alertTimeout;
-    Preconditions.checkArgument(this.alertTimeout >= 0, "alertTimeout must be >= 0");
+    checkArgument(this.alertTimeout >= 0, "alertTimeout must be >= 0");
     this.segmentWriteOutMediumFactory = segmentWriteOutMediumFactory;
     this.dedupColumn = dedupColumn == null ? DEFAULT_DEDUP_COLUMN : dedupColumn;
   }
@@ -212,20 +213,20 @@ public class RealtimeTuningConfig implements AppenderatorConfig {
         dedupColumn);
   }
 
-  @Override
   @JsonProperty
+  @Override
   public AppendableIndexSpec getAppendableIndexSpec() {
     return appendableIndexSpec;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public int getMaxRowsInMemory() {
     return maxRowsInMemory;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public long getMaxBytesInMemory() {
     return maxBytesInMemory;
   }
@@ -236,8 +237,8 @@ public class RealtimeTuningConfig implements AppenderatorConfig {
     return skipBytesInMemoryOverheadCheck;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public Period getIntermediatePersistPeriod() {
     return intermediatePersistPeriod;
   }
@@ -249,11 +250,11 @@ public class RealtimeTuningConfig implements AppenderatorConfig {
 
   @Override
   public File getBasePersistDirectory() {
-    return Preconditions.checkNotNull(basePersistDirectory, "basePersistDirectory not set");
+    return requireNonNull(basePersistDirectory, "basePersistDirectory not set");
   }
 
   public VersioningPolicy getVersioningPolicy() {
-    return Preconditions.checkNotNull(versioningPolicy, "versioningPolicy not set");
+    return requireNonNull(versioningPolicy, "versioningPolicy not set");
   }
 
   @JsonProperty("rejectionPolicy")
@@ -261,8 +262,8 @@ public class RealtimeTuningConfig implements AppenderatorConfig {
     return rejectionPolicyFactory;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public int getMaxPendingPersists() {
     return maxPendingPersists;
   }
@@ -277,8 +278,8 @@ public class RealtimeTuningConfig implements AppenderatorConfig {
     return shardSpec;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public IndexSpec getIndexSpec() {
     return indexSpec;
   }
@@ -299,8 +300,8 @@ public class RealtimeTuningConfig implements AppenderatorConfig {
     return this.mergeThreadPriority;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public boolean isReportParseExceptions() {
     return reportParseExceptions;
   }
@@ -315,9 +316,9 @@ public class RealtimeTuningConfig implements AppenderatorConfig {
     return alertTimeout;
   }
 
-  @Override
   @JsonProperty
   @Nullable
+  @Override
   public SegmentWriteOutMediumFactory getSegmentWriteOutMediumFactory() {
     return segmentWriteOutMediumFactory;
   }
diff --git a/server/src/main/java/org/apache/druid/segment/indexing/TuningConfig.java b/server/src/main/java/org/apache/druid/segment/indexing/TuningConfig.java
index 2cf57eac30..29adda9a20 100644
--- a/server/src/main/java/org/apache/druid/segment/indexing/TuningConfig.java
+++ b/server/src/main/java/org/apache/druid/segment/indexing/TuningConfig.java
@@ -28,7 +28,7 @@ import org.apache.druid.segment.incremental.OnheapIncrementalIndex;
 
 /** */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(value = {@JsonSubTypes.Type(name = "realtime", value = RealtimeTuningConfig.class)})
+@JsonSubTypes(@JsonSubTypes.Type(name = "realtime", value = RealtimeTuningConfig.class))
 public interface TuningConfig {
   boolean DEFAULT_LOG_PARSE_EXCEPTIONS = false;
   AppendableIndexSpec DEFAULT_APPENDABLE_INDEX = new OnheapIncrementalIndex.Spec();
diff --git a/server/src/main/java/org/apache/druid/segment/indexing/granularity/ArbitraryGranularitySpec.java b/server/src/main/java/org/apache/druid/segment/indexing/granularity/ArbitraryGranularitySpec.java
index 943e956242..639a37b5f9 100644
--- a/server/src/main/java/org/apache/druid/segment/indexing/granularity/ArbitraryGranularitySpec.java
+++ b/server/src/main/java/org/apache/druid/segment/indexing/granularity/ArbitraryGranularitySpec.java
@@ -64,7 +64,7 @@ public class ArbitraryGranularitySpec extends BaseGranularitySpec {
 
   @Override
   public Iterable<Interval> sortedBucketIntervals() {
-    return () -> lookupTableBucketByDateTime.iterator();
+    return lookupTableBucketByDateTime::iterator;
   }
 
   @Override
@@ -72,8 +72,8 @@ public class ArbitraryGranularitySpec extends BaseGranularitySpec {
     throw new UnsupportedOperationException();
   }
 
-  @Override
   @JsonProperty("queryGranularity")
+  @Override
   public Granularity getQueryGranularity() {
     return queryGranularity;
   }
@@ -105,8 +105,7 @@ public class ArbitraryGranularitySpec extends BaseGranularitySpec {
   public int hashCode() {
     int result = inputIntervals().hashCode();
     result = 31 * result + rollup.hashCode();
-    result = 31 * result + (queryGranularity != null ? queryGranularity.hashCode() : 0);
-    return result;
+    return 31 * result + (queryGranularity != null ? queryGranularity.hashCode() : 0);
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/segment/indexing/granularity/BaseGranularitySpec.java b/server/src/main/java/org/apache/druid/segment/indexing/granularity/BaseGranularitySpec.java
index 05017f0a83..3c444cc1b6 100644
--- a/server/src/main/java/org/apache/druid/segment/indexing/granularity/BaseGranularitySpec.java
+++ b/server/src/main/java/org/apache/druid/segment/indexing/granularity/BaseGranularitySpec.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.segment.indexing.granularity;
 
+import static java.util.Collections.emptyList;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Optional;
 import com.google.common.collect.Iterators;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -45,18 +46,18 @@ public abstract class BaseGranularitySpec implements GranularitySpec {
   protected final Boolean rollup;
 
   public BaseGranularitySpec(List<Interval> inputIntervals, Boolean rollup) {
-    this.inputIntervals = inputIntervals == null ? Collections.emptyList() : inputIntervals;
+    this.inputIntervals = inputIntervals == null ? emptyList() : inputIntervals;
     this.rollup = rollup == null ? DEFAULT_ROLLUP : rollup;
   }
 
-  @Override
   @JsonProperty("intervals")
+  @Override
   public List<Interval> inputIntervals() {
     return inputIntervals;
   }
 
-  @Override
   @JsonProperty("rollup")
+  @Override
   public boolean isRollup() {
     return rollup;
   }
diff --git a/server/src/main/java/org/apache/druid/segment/indexing/granularity/GranularitySpec.java b/server/src/main/java/org/apache/druid/segment/indexing/granularity/GranularitySpec.java
index fef7fa3a11..f33bf088b3 100644
--- a/server/src/main/java/org/apache/druid/segment/indexing/granularity/GranularitySpec.java
+++ b/server/src/main/java/org/apache/druid/segment/indexing/granularity/GranularitySpec.java
@@ -38,11 +38,10 @@ import org.joda.time.Interval;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = UniformGranularitySpec.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "uniform", value = UniformGranularitySpec.class),
-      @JsonSubTypes.Type(name = "arbitrary", value = ArbitraryGranularitySpec.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "arbitrary", value = ArbitraryGranularitySpec.class),
+  @JsonSubTypes.Type(name = "uniform", value = UniformGranularitySpec.class)
+})
 public interface GranularitySpec {
   /**
    * Iterable all time groups, broken up on segment boundaries. Should be sorted by interval start
diff --git a/server/src/main/java/org/apache/druid/segment/indexing/granularity/UniformGranularitySpec.java b/server/src/main/java/org/apache/druid/segment/indexing/granularity/UniformGranularitySpec.java
index a0b4ea968d..cf04b18446 100644
--- a/server/src/main/java/org/apache/druid/segment/indexing/granularity/UniformGranularitySpec.java
+++ b/server/src/main/java/org/apache/druid/segment/indexing/granularity/UniformGranularitySpec.java
@@ -53,17 +53,17 @@ public class UniformGranularitySpec extends BaseGranularitySpec {
 
   @Override
   public Iterable<Interval> sortedBucketIntervals() {
-    return () -> intervalsByGranularity.granularityIntervalsIterator();
+    return intervalsByGranularity::granularityIntervalsIterator;
   }
 
-  @Override
   @JsonProperty("segmentGranularity")
+  @Override
   public Granularity getSegmentGranularity() {
     return segmentGranularity;
   }
 
-  @Override
   @JsonProperty("queryGranularity")
+  @Override
   public Granularity getQueryGranularity() {
     return queryGranularity;
   }
@@ -103,8 +103,7 @@ public class UniformGranularitySpec extends BaseGranularitySpec {
     int result = segmentGranularity.hashCode();
     result = 31 * result + queryGranularity.hashCode();
     result = 31 * result + rollup.hashCode();
-    result = 31 * result + (inputIntervals != null ? inputIntervals.hashCode() : 0);
-    return result;
+    return 31 * result + (inputIntervals != null ? inputIntervals.hashCode() : 0);
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/segment/loading/LeastBytesUsedStorageLocationSelectorStrategy.java b/server/src/main/java/org/apache/druid/segment/loading/LeastBytesUsedStorageLocationSelectorStrategy.java
index 763dce68b9..4df820ecd1 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/LeastBytesUsedStorageLocationSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/LeastBytesUsedStorageLocationSelectorStrategy.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.Comparator.comparingLong;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.google.common.collect.Ordering;
-import java.util.Comparator;
 import java.util.Iterator;
 import java.util.List;
 
@@ -33,7 +34,7 @@ import java.util.List;
 public class LeastBytesUsedStorageLocationSelectorStrategy
     implements StorageLocationSelectorStrategy {
   private static final Ordering<StorageLocation> ORDERING =
-      Ordering.from(Comparator.comparingLong(StorageLocation::currSizeBytes));
+      Ordering.from(comparingLong(StorageLocation::currSizeBytes));
 
   private final List<StorageLocation> storageLocations;
 
diff --git a/server/src/main/java/org/apache/druid/segment/loading/LocalDataSegmentKiller.java b/server/src/main/java/org/apache/druid/segment/loading/LocalDataSegmentKiller.java
index f5f68017de..2a6ebe4012 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/LocalDataSegmentKiller.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/LocalDataSegmentKiller.java
@@ -60,7 +60,7 @@ public class LocalDataSegmentKiller implements DataSegmentKiller {
         parentDir = parentDir.getParentFile();
         int maxDepth =
             4; // if for some reason there's no datasSource directory, stop recursing somewhere
-               // reasonable
+        // reasonable
         while (parentDir != null && --maxDepth >= 0) {
           // parentDir.listFiles().length > 0 check not strictly necessary, because
           // parentDir.delete() fails on
diff --git a/server/src/main/java/org/apache/druid/segment/loading/LocalDataSegmentPusher.java b/server/src/main/java/org/apache/druid/segment/loading/LocalDataSegmentPusher.java
index 9443de1396..f77aff2c84 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/LocalDataSegmentPusher.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/LocalDataSegmentPusher.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.UUID.randomUUID;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.inject.Inject;
 import java.io.File;
@@ -27,7 +29,6 @@ import java.net.URI;
 import java.nio.file.Files;
 import java.nio.file.StandardCopyOption;
 import java.util.Map;
-import java.util.UUID;
 import org.apache.druid.java.util.common.FileUtils;
 import org.apache.druid.java.util.common.IOE;
 import org.apache.druid.java.util.common.StringUtils;
@@ -104,7 +105,7 @@ public class LocalDataSegmentPusher implements DataSegmentPusher {
   }
 
   private String makeIntermediateDir() {
-    return "intermediate_pushes/" + UUID.randomUUID();
+    return "intermediate_pushes/" + randomUUID();
   }
 
   private DataSegment pushZip(final File inDir, final File outDir, final DataSegment baseSegment)
@@ -170,8 +171,7 @@ public class LocalDataSegmentPusher implements DataSegmentPusher {
           // same directory twice, so behavior is compatible with the zip style of pushing.
           Files.move(
               segmentDir.toPath(),
-              new File(outDir, StringUtils.format("%s_old_%s", INDEX_DIR, UUID.randomUUID()))
-                  .toPath(),
+              new File(outDir, StringUtils.format("%s_old_%s", INDEX_DIR, randomUUID())).toPath(),
               StandardCopyOption.ATOMIC_MOVE);
 
           Files.move(tmpSegmentDir.toPath(), segmentDir.toPath(), StandardCopyOption.ATOMIC_MOVE);
diff --git a/server/src/main/java/org/apache/druid/segment/loading/LocalLoadSpec.java b/server/src/main/java/org/apache/druid/segment/loading/LocalLoadSpec.java
index 4678055be9..fe0eda2100 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/LocalLoadSpec.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/LocalLoadSpec.java
@@ -19,11 +19,13 @@
 
 package org.apache.druid.segment.loading;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.annotation.JsonTypeName;
-import com.google.common.base.Preconditions;
 import java.io.File;
 import java.nio.file.Files;
 import java.nio.file.Path;
@@ -40,7 +42,7 @@ public class LocalLoadSpec implements LoadSpec {
   public LocalLoadSpec(
       @JacksonInject LocalDataSegmentPuller puller,
       @JsonProperty(value = "path", required = true) final String path) {
-    Preconditions.checkNotNull(path);
+    requireNonNull(path);
     this.path = Paths.get(path);
     this.puller = puller;
   }
@@ -52,7 +54,7 @@ public class LocalLoadSpec implements LoadSpec {
 
   @Override
   public LoadSpecResult loadSegment(final File outDir) throws SegmentLoadingException {
-    Preconditions.checkArgument(Files.exists(path), "[%s] does not exist", path);
+    checkArgument(Files.exists(path), "[%s] does not exist", path);
     return new LoadSpecResult(puller.getSegmentFiles(path.toFile(), outDir).size());
   }
 }
diff --git a/server/src/main/java/org/apache/druid/segment/loading/MostAvailableSizeStorageLocationSelectorStrategy.java b/server/src/main/java/org/apache/druid/segment/loading/MostAvailableSizeStorageLocationSelectorStrategy.java
index ffe3b70e1c..5a69cb5e88 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/MostAvailableSizeStorageLocationSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/MostAvailableSizeStorageLocationSelectorStrategy.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.Comparator.comparingLong;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.google.common.collect.Ordering;
-import java.util.Comparator;
 import java.util.Iterator;
 import java.util.List;
 
@@ -33,7 +34,7 @@ import java.util.List;
 public class MostAvailableSizeStorageLocationSelectorStrategy
     implements StorageLocationSelectorStrategy {
   private static final Ordering<StorageLocation> ORDERING =
-      Ordering.from(Comparator.comparingLong(StorageLocation::availableSizeBytes).reversed());
+      Ordering.from(comparingLong(StorageLocation::availableSizeBytes).reversed());
 
   private final List<StorageLocation> storageLocations;
 
diff --git a/server/src/main/java/org/apache/druid/segment/loading/RandomStorageLocationSelectorStrategy.java b/server/src/main/java/org/apache/druid/segment/loading/RandomStorageLocationSelectorStrategy.java
index 6bac4c95bd..633afb8a8c 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/RandomStorageLocationSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/RandomStorageLocationSelectorStrategy.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.Collections.shuffle;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 
@@ -43,7 +44,7 @@ public class RandomStorageLocationSelectorStrategy implements StorageLocationSel
   @Override
   public Iterator<StorageLocation> getLocations() {
     List<StorageLocation> copyLocation = new ArrayList<>(storageLocations);
-    Collections.shuffle(copyLocation);
+    shuffle(copyLocation);
     return copyLocation.iterator();
   }
 }
diff --git a/server/src/main/java/org/apache/druid/segment/loading/SegmentLoaderConfig.java b/server/src/main/java/org/apache/druid/segment/loading/SegmentLoaderConfig.java
index 9adda82e0b..087011a736 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/SegmentLoaderConfig.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/SegmentLoaderConfig.java
@@ -19,19 +19,20 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.Collections.emptyList;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.Lists;
 import java.io.File;
-import java.util.Collections;
+import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
-import java.util.stream.Collectors;
 import org.apache.druid.utils.JvmUtils;
 
 /** */
 public class SegmentLoaderConfig {
-  @JsonProperty private List<StorageLocationConfig> locations = Collections.emptyList();
+  @JsonProperty private List<StorageLocationConfig> locations = emptyList();
 
   @JsonProperty("lazyLoadOnStart")
   private boolean lazyLoadOnStart = false;
@@ -122,7 +123,7 @@ public class SegmentLoaderConfig {
 
   public SegmentLoaderConfig withLocations(List<StorageLocationConfig> locations) {
     SegmentLoaderConfig retVal = new SegmentLoaderConfig();
-    retVal.locations = Lists.newArrayList(locations);
+    retVal.locations = new ArrayList<>(locations);
     retVal.deleteOnRemove = this.deleteOnRemove;
     retVal.infoDir = this.infoDir;
     return retVal;
@@ -151,7 +152,7 @@ public class SegmentLoaderConfig {
                     locationConfig.getPath(),
                     locationConfig.getMaxSize(),
                     locationConfig.getFreeSpacePercent()))
-        .collect(Collectors.toList());
+        .collect(toList());
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/segment/loading/SegmentLocalCacheManager.java b/server/src/main/java/org/apache/druid/segment/loading/SegmentLocalCacheManager.java
index f9d27ad480..f789e17f71 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/SegmentLocalCacheManager.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/SegmentLocalCacheManager.java
@@ -345,7 +345,7 @@ public class SegmentLocalCacheManager implements SegmentCacheManager {
       try {
         // May be the segment was already loaded [This check is required to account for restart
         // scenarios]
-        if (null != findStoragePathIfCached(segment)) {
+        if (findStoragePathIfCached(segment) != null) {
           return true;
         }
 
@@ -361,7 +361,7 @@ public class SegmentLocalCacheManager implements SegmentCacheManager {
         // Not found in any location, reserve now
         for (Iterator<StorageLocation> it = strategy.getLocations(); it.hasNext(); ) {
           StorageLocation location = it.next();
-          if (null != location.reserve(storageDirStr, segment)) {
+          if (location.reserve(storageDirStr, segment) != null) {
             return true;
           }
         }
diff --git a/server/src/main/java/org/apache/druid/segment/loading/StorageLocation.java b/server/src/main/java/org/apache/druid/segment/loading/StorageLocation.java
index 7810a75530..21c1ba2bf6 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/StorageLocation.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/StorageLocation.java
@@ -168,8 +168,8 @@ public class StorageLocation {
    * This method is only package-private to use it in unit tests. Production code must not call this
    * method directly. Use {@link #reserve} instead.
    */
-  @VisibleForTesting
   @GuardedBy("this")
+  @VisibleForTesting
   boolean canHandle(String segmentId, long segmentSize) {
     if (availableSizeBytes() < segmentSize) {
       log.warn(
diff --git a/server/src/main/java/org/apache/druid/segment/loading/StorageLocationConfig.java b/server/src/main/java/org/apache/druid/segment/loading/StorageLocationConfig.java
index ae0a7c33d8..67993a054f 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/StorageLocationConfig.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/StorageLocationConfig.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.segment.loading;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.io.File;
 import javax.annotation.Nullable;
 import org.apache.druid.java.util.common.HumanReadableBytes;
@@ -41,11 +43,11 @@ public class StorageLocationConfig {
   }
 
   public StorageLocationConfig(File path, long maxSize, Double freeSpacePercent) {
-    this.path = Preconditions.checkNotNull(path, "path");
+    this.path = requireNonNull(path, "path");
     this.maxSize = maxSize;
     this.freeSpacePercent = freeSpacePercent;
-    Preconditions.checkArgument(this.maxSize > 0, "maxSize[%s] should be positive", this.maxSize);
-    Preconditions.checkArgument(
+    checkArgument(this.maxSize > 0, "maxSize[%s] should be positive", this.maxSize);
+    checkArgument(
         this.freeSpacePercent == null || this.freeSpacePercent >= 0,
         "freeSpacePercent[%s] should be 0 or a positive double",
         this.freeSpacePercent);
diff --git a/server/src/main/java/org/apache/druid/segment/loading/StorageLocationSelectorStrategy.java b/server/src/main/java/org/apache/druid/segment/loading/StorageLocationSelectorStrategy.java
index e7be5f939b..9db0bca77d 100644
--- a/server/src/main/java/org/apache/druid/segment/loading/StorageLocationSelectorStrategy.java
+++ b/server/src/main/java/org/apache/druid/segment/loading/StorageLocationSelectorStrategy.java
@@ -37,19 +37,16 @@ import org.apache.druid.timeline.DataSegment;
     use = JsonTypeInfo.Id.NAME,
     property = "strategy",
     defaultImpl = LeastBytesUsedStorageLocationSelectorStrategy.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(
-          name = "leastBytesUsed",
-          value = LeastBytesUsedStorageLocationSelectorStrategy.class),
-      @JsonSubTypes.Type(
-          name = "roundRobin",
-          value = RoundRobinStorageLocationSelectorStrategy.class),
-      @JsonSubTypes.Type(name = "random", value = RandomStorageLocationSelectorStrategy.class),
-      @JsonSubTypes.Type(
-          name = "mostAvailableSize",
-          value = MostAvailableSizeStorageLocationSelectorStrategy.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(
+      name = "leastBytesUsed",
+      value = LeastBytesUsedStorageLocationSelectorStrategy.class),
+  @JsonSubTypes.Type(
+      name = "mostAvailableSize",
+      value = MostAvailableSizeStorageLocationSelectorStrategy.class),
+  @JsonSubTypes.Type(name = "random", value = RandomStorageLocationSelectorStrategy.class),
+  @JsonSubTypes.Type(name = "roundRobin", value = RoundRobinStorageLocationSelectorStrategy.class)
+})
 public interface StorageLocationSelectorStrategy {
   /**
    * Finds the best ordering of the {@link StorageLocation}s to load a {@link DataSegment} according
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/FireDepartment.java b/server/src/main/java/org/apache/druid/segment/realtime/FireDepartment.java
index 75c2378bc9..29e8ccb878 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/FireDepartment.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/FireDepartment.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.segment.realtime;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.io.IOException;
 import org.apache.druid.data.input.Firehose;
 import org.apache.druid.segment.indexing.DataSchema;
@@ -48,8 +49,8 @@ public class FireDepartment extends IngestionSpec<RealtimeIOConfig, RealtimeTuni
       @JsonProperty("ioConfig") RealtimeIOConfig ioConfig,
       @JsonProperty("tuningConfig") RealtimeTuningConfig tuningConfig) {
     super(dataSchema, ioConfig, tuningConfig);
-    Preconditions.checkNotNull(dataSchema, "dataSchema");
-    Preconditions.checkNotNull(ioConfig, "ioConfig");
+    requireNonNull(dataSchema, "dataSchema");
+    requireNonNull(ioConfig, "ioConfig");
 
     this.dataSchema = dataSchema;
     this.ioConfig = ioConfig;
@@ -83,7 +84,7 @@ public class FireDepartment extends IngestionSpec<RealtimeIOConfig, RealtimeTuni
   public Firehose connect() throws IOException {
     return ioConfig
         .getFirehoseFactory()
-        .connect(Preconditions.checkNotNull(dataSchema.getParser(), "inputRowParser"), null);
+        .connect(requireNonNull(dataSchema.getParser(), "inputRowParser"), null);
   }
 
   public FireDepartmentMetrics getMetrics() {
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/FireDepartmentConfig.java b/server/src/main/java/org/apache/druid/segment/realtime/FireDepartmentConfig.java
index dfd7f51686..ed01f81dbb 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/FireDepartmentConfig.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/FireDepartmentConfig.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.segment.realtime;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import org.joda.time.Period;
 
 /** */
@@ -36,9 +38,9 @@ public class FireDepartmentConfig {
     this.maxRowsInMemory = maxRowsInMemory;
     this.intermediatePersistPeriod = intermediatePersistPeriod;
 
-    Preconditions.checkArgument(
+    checkArgument(
         maxRowsInMemory > 0, "maxRowsInMemory[%s] should be greater than 0", maxRowsInMemory);
-    Preconditions.checkNotNull(intermediatePersistPeriod, "intermediatePersistPeriod");
+    requireNonNull(intermediatePersistPeriod, "intermediatePersistPeriod");
   }
 
   @JsonProperty
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/RealtimeMetricsMonitor.java b/server/src/main/java/org/apache/druid/segment/realtime/RealtimeMetricsMonitor.java
index 30b27de755..161f71d1f2 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/RealtimeMetricsMonitor.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/RealtimeMetricsMonitor.java
@@ -53,7 +53,7 @@ public class RealtimeMetricsMonitor extends AbstractMonitor {
       List<FireDepartment> fireDepartments, Map<String, String[]> dimensions) {
     this.fireDepartments = fireDepartments;
     this.previousValues = new HashMap<>();
-    this.dimensions = ImmutableMap.copyOf(dimensions);
+    this.dimensions = dimensions;
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorDriverMetadata.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorDriverMetadata.java
index 202878bede..c2973ab0b0 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorDriverMetadata.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorDriverMetadata.java
@@ -19,16 +19,17 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.segment.realtime.appenderator.SegmentWithState.SegmentState;
 import org.apache.druid.timeline.SegmentId;
 
@@ -47,7 +48,7 @@ public class AppenderatorDriverMetadata {
       @JsonProperty("activeSegments") Map<String, List<SegmentIdWithShardSpec>> activeSegments,
       @JsonProperty("publishPendingSegments")
           Map<String, List<SegmentIdWithShardSpec>> publishPendingSegments) {
-    Preconditions.checkState(
+    checkState(
         segments != null || (activeSegments != null && publishPendingSegments != null),
         "Metadata should either have segments with state information or both active segments and publish pending "
             + "segments information. segments [%s], activeSegments [%s], publishPendingSegments [%s]",
@@ -69,7 +70,7 @@ public class AppenderatorDriverMetadata {
                             activeSegmentsAlreadySeen.add(segmentIdentifier.asSegmentId());
                             return SegmentWithState.newSegment(segmentIdentifier);
                           })
-                      .collect(Collectors.toList())));
+                      .collect(toList())));
       // publishPendingSegments is a superset of activeSegments
       publishPendingSegments.forEach(
           (sequence, sequenceSegments) -> {
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorFactory.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorFactory.java
index ab6acf16d0..6724758dfa 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorFactory.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorFactory.java
@@ -26,11 +26,10 @@ import org.apache.druid.segment.indexing.RealtimeTuningConfig;
 import org.apache.druid.segment.realtime.FireDepartmentMetrics;
 
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "default", value = DefaultRealtimeAppenderatorFactory.class),
-      @JsonSubTypes.Type(name = "offline", value = DefaultOfflineAppenderatorFactory.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "default", value = DefaultRealtimeAppenderatorFactory.class),
+  @JsonSubTypes.Type(name = "offline", value = DefaultOfflineAppenderatorFactory.class)
+})
 public interface AppenderatorFactory {
   Appenderator build(DataSchema schema, RealtimeTuningConfig config, FireDepartmentMetrics metrics);
 }
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorImpl.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorImpl.java
index 753455eccb..8d02f1ba57 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorImpl.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorImpl.java
@@ -19,11 +19,15 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Collections.synchronizedMap;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.joining;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Function;
-import com.google.common.base.Joiner;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Stopwatch;
 import com.google.common.base.Supplier;
 import com.google.common.collect.ImmutableList;
@@ -45,7 +49,6 @@ import java.nio.file.StandardOpenOption;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.IdentityHashMap;
 import java.util.List;
@@ -61,7 +64,6 @@ import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.commons.lang.mutable.MutableLong;
 import org.apache.druid.client.cache.Cache;
@@ -185,7 +187,7 @@ public class AppenderatorImpl implements Appenderator {
    * key needs to be interpreted with reference semantics.
    */
   private final Map<FireHydrant, Pair<File, SegmentId>> persistedHydrantMetadata =
-      Collections.synchronizedMap(new IdentityHashMap<>());
+      synchronizedMap(new IdentityHashMap<>());
 
   /**
    * This constructor allows the caller to provide its own SinkQuerySegmentWalker.
@@ -213,19 +215,18 @@ public class AppenderatorImpl implements Appenderator {
       boolean isOpenSegments,
       boolean useMaxMemoryEstimates) {
     this.myId = id;
-    this.schema = Preconditions.checkNotNull(schema, "schema");
-    this.tuningConfig = Preconditions.checkNotNull(tuningConfig, "tuningConfig");
-    this.metrics = Preconditions.checkNotNull(metrics, "metrics");
-    this.dataSegmentPusher = Preconditions.checkNotNull(dataSegmentPusher, "dataSegmentPusher");
-    this.objectMapper = Preconditions.checkNotNull(objectMapper, "objectMapper");
-    this.segmentAnnouncer = Preconditions.checkNotNull(segmentAnnouncer, "segmentAnnouncer");
-    this.indexIO = Preconditions.checkNotNull(indexIO, "indexIO");
-    this.indexMerger = Preconditions.checkNotNull(indexMerger, "indexMerger");
+    this.schema = requireNonNull(schema, "schema");
+    this.tuningConfig = requireNonNull(tuningConfig, "tuningConfig");
+    this.metrics = requireNonNull(metrics, "metrics");
+    this.dataSegmentPusher = requireNonNull(dataSegmentPusher, "dataSegmentPusher");
+    this.objectMapper = requireNonNull(objectMapper, "objectMapper");
+    this.segmentAnnouncer = requireNonNull(segmentAnnouncer, "segmentAnnouncer");
+    this.indexIO = requireNonNull(indexIO, "indexIO");
+    this.indexMerger = requireNonNull(indexMerger, "indexMerger");
     this.cache = cache;
     this.texasRanger = sinkQuerySegmentWalker;
-    this.rowIngestionMeters = Preconditions.checkNotNull(rowIngestionMeters, "rowIngestionMeters");
-    this.parseExceptionHandler =
-        Preconditions.checkNotNull(parseExceptionHandler, "parseExceptionHandler");
+    this.rowIngestionMeters = requireNonNull(rowIngestionMeters, "rowIngestionMeters");
+    this.parseExceptionHandler = requireNonNull(parseExceptionHandler, "parseExceptionHandler");
     this.isOpenSegments = isOpenSegments;
     this.useMaxMemoryEstimates = useMaxMemoryEstimates;
 
@@ -504,9 +505,7 @@ public class AppenderatorImpl implements Appenderator {
   @Override
   public <T> QueryRunner<T> getQueryRunnerForIntervals(
       final Query<T> query, final Iterable<Interval> intervals) {
-    if (texasRanger == null) {
-      throw new IllegalStateException("Don't query me, bro.");
-    }
+    checkState(texasRanger != null, "Don't query me, bro.");
 
     return texasRanger.getQueryRunnerForIntervals(query, intervals);
   }
@@ -514,9 +513,7 @@ public class AppenderatorImpl implements Appenderator {
   @Override
   public <T> QueryRunner<T> getQueryRunnerForSegments(
       final Query<T> query, final Iterable<SegmentDescriptor> specs) {
-    if (texasRanger == null) {
-      throw new IllegalStateException("Don't query me, bro.");
-    }
+    checkState(texasRanger != null, "Don't query me, bro.");
 
     return texasRanger.getQueryRunnerForSegments(query, specs);
   }
@@ -631,14 +628,14 @@ public class AppenderatorImpl implements Appenderator {
                     log.debug(
                         "Committing metadata[%s] for sinks[%s].",
                         commitMetadata,
-                        Joiner.on(", ")
-                            .join(
-                                currentHydrants.entrySet().stream()
-                                    .map(
-                                        entry ->
-                                            StringUtils.format(
-                                                "%s:%d", entry.getKey(), entry.getValue()))
-                                    .collect(Collectors.toList())));
+                        String.join(
+                            ", ",
+                            currentHydrants.entrySet().stream()
+                                .map(
+                                    entry ->
+                                        StringUtils.format(
+                                            "%s:%d", entry.getKey(), entry.getValue()))
+                                .collect(toList())));
 
                     committer.run();
 
@@ -663,7 +660,7 @@ public class AppenderatorImpl implements Appenderator {
                       indexesToPersist.stream()
                           .map(itp -> itp.rhs.asSegmentId().toString())
                           .distinct()
-                          .collect(Collectors.joining(", ")));
+                          .collect(joining(", ")));
                   log.info(
                       "Persisted stats: processed rows: [%d], persisted rows[%d], sinks: [%d], total fireHydrants (across sinks): [%d], persisted fireHydrants (across sinks): [%d]",
                       rowIngestionMeters.getProcessed(),
@@ -742,7 +739,7 @@ public class AppenderatorImpl implements Appenderator {
                   "Building and pushing segments: %s",
                   theSinks.keySet().stream()
                       .map(SegmentIdWithShardSpec::toString)
-                      .collect(Collectors.joining(", ")));
+                      .collect(joining(", ")));
 
               for (Map.Entry<SegmentIdWithShardSpec, Sink> entry : theSinks.entrySet()) {
                 if (droppingSinks.contains(entry.getKey())) {
@@ -918,7 +915,7 @@ public class AppenderatorImpl implements Appenderator {
               // maintain exactly-once
               // semantics.
               () -> dataSegmentPusher.push(mergedFile, segmentToPush, useUniquePath),
-              exception -> exception instanceof Exception,
+              Exception.class::isInstance,
               5);
 
       if (!isOpenSegments()) {
@@ -979,13 +976,13 @@ public class AppenderatorImpl implements Appenderator {
 
     try {
       shutdownExecutors();
-      Preconditions.checkState(
+      checkState(
           persistExecutor == null || persistExecutor.awaitTermination(365, TimeUnit.DAYS),
           "persistExecutor not terminated");
-      Preconditions.checkState(
+      checkState(
           pushExecutor == null || pushExecutor.awaitTermination(365, TimeUnit.DAYS),
           "pushExecutor not terminated");
-      Preconditions.checkState(
+      checkState(
           intermediateTempExecutor == null
               || intermediateTempExecutor.awaitTermination(365, TimeUnit.DAYS),
           "intermediateTempExecutor not terminated");
@@ -1029,10 +1026,10 @@ public class AppenderatorImpl implements Appenderator {
     try {
       shutdownExecutors();
       // We don't wait for pushExecutor to be terminated. See Javadoc for more details.
-      Preconditions.checkState(
+      checkState(
           persistExecutor == null || persistExecutor.awaitTermination(365, TimeUnit.DAYS),
           "persistExecutor not terminated");
-      Preconditions.checkState(
+      checkState(
           intermediateTempExecutor == null
               || intermediateTempExecutor.awaitTermination(365, TimeUnit.DAYS),
           "intermediateTempExecutor not terminated");
@@ -1134,7 +1131,7 @@ public class AppenderatorImpl implements Appenderator {
    * @return persisted commit metadata
    */
   private Object bootstrapSinksFromDisk() {
-    Preconditions.checkState(sinks.isEmpty(), "Already bootstrapped?!");
+    checkState(sinks.isEmpty(), "Already bootstrapped?!");
 
     final File baseDir = tuningConfig.getBasePersistDirectory();
     if (!baseDir.exists()) {
@@ -1194,12 +1191,12 @@ public class AppenderatorImpl implements Appenderator {
 
         // To avoid reading and listing of "merged" dir and other special files
         final File[] sinkFiles =
-            sinkDir.listFiles((dir, fileName) -> !(Ints.tryParse(fileName) == null));
+            sinkDir.listFiles((dir, fileName) -> Ints.tryParse(fileName) != null);
 
         Arrays.sort(
             sinkFiles,
             (o1, o2) ->
-                Ints.compare(Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName())));
+                Integer.compare(Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName())));
 
         List<FireHydrant> hydrants = new ArrayList<>();
         for (File hydrantDir : sinkFiles) {
@@ -1259,7 +1256,7 @@ public class AppenderatorImpl implements Appenderator {
         Sets.newHashSet(Iterables.transform(sinks.keySet(), SegmentIdWithShardSpec::toString));
     final Set<String> missingSinks = Sets.difference(committed.getHydrants().keySet(), loadedSinks);
     if (!missingSinks.isEmpty()) {
-      throw new ISE("Missing committed sinks [%s]", Joiner.on(", ").join(missingSinks));
+      throw new ISE("Missing committed sinks [%s]", String.join(", ", missingSinks));
     }
 
     totalRows.set(rowsSoFar);
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumber.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumber.java
index 68afaaa752..9c0ec791e2 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumber.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumber.java
@@ -112,7 +112,7 @@ public class AppenderatorPlumber implements Plumber {
   }
 
   public Map<Long, SegmentIdWithShardSpec> getSegmentsView() {
-    return ImmutableMap.copyOf(segments);
+    return segments;
   }
 
   public DataSchema getSchema() {
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/Appenderators.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/Appenderators.java
index 5909cb93c1..1628e53922 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/Appenderators.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/Appenderators.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import org.apache.druid.client.cache.Cache;
 import org.apache.druid.client.cache.CacheConfig;
 import org.apache.druid.client.cache.CachePopulatorStats;
@@ -77,7 +78,7 @@ public class Appenderators {
             conglomerate,
             queryProcessingPool,
             new JoinableFactoryWrapper(joinableFactory),
-            Preconditions.checkNotNull(cache, "cache"),
+            requireNonNull(cache, "cache"),
             cacheConfig,
             cachePopulatorStats),
         indexIO,
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BaseAppenderatorDriver.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BaseAppenderatorDriver.java
index d7cabac34f..738a1ef1ef 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BaseAppenderatorDriver.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BaseAppenderatorDriver.java
@@ -19,13 +19,17 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Collections.disjoint;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toSet;
+
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Supplier;
 import com.google.common.base.Throwables;
-import com.google.common.collect.ImmutableList;
-import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Maps;
 import com.google.common.collect.Sets;
@@ -47,7 +51,6 @@ import java.util.Objects;
 import java.util.Set;
 import java.util.TreeMap;
 import java.util.function.Consumer;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 import javax.annotation.Nullable;
 import org.apache.druid.data.input.Committer;
@@ -105,7 +108,7 @@ public abstract class BaseAppenderatorDriver implements Closeable {
       this.appendFinishedSegments.addAll(appendFinishedSegments);
 
       if (appendingSegment != null) {
-        Preconditions.checkArgument(
+        checkArgument(
             appendingSegment.getState() == SegmentState.APPENDING,
             "appendingSegment[%s] is not in the APPENDING state",
             appendingSegment.getSegmentIdentifier());
@@ -118,12 +121,12 @@ public abstract class BaseAppenderatorDriver implements Closeable {
     }
 
     void setAppendingSegment(SegmentWithState appendingSegment) {
-      Preconditions.checkArgument(
+      checkArgument(
           appendingSegment.getState() == SegmentState.APPENDING,
           "segment[%s] is not in the APPENDING state",
           appendingSegment.getSegmentIdentifier());
       // There should be only one appending segment at any time
-      Preconditions.checkState(
+      checkState(
           this.appendingSegment == null,
           "Current appendingSegment[%s] is not null. "
               + "Its state must be changed before setting a new appendingSegment[%s]",
@@ -133,7 +136,7 @@ public abstract class BaseAppenderatorDriver implements Closeable {
     }
 
     void finishAppendingToCurrentActiveSegment(Consumer<SegmentWithState> stateTransitionFn) {
-      Preconditions.checkNotNull(appendingSegment, "appendingSegment");
+      requireNonNull(appendingSegment, "appendingSegment");
       stateTransitionFn.accept(appendingSegment);
       appendFinishedSegments.add(appendingSegment);
       appendingSegment = null;
@@ -226,10 +229,10 @@ public abstract class BaseAppenderatorDriver implements Closeable {
       SegmentAllocator segmentAllocator,
       UsedSegmentChecker usedSegmentChecker,
       DataSegmentKiller dataSegmentKiller) {
-    this.appenderator = Preconditions.checkNotNull(appenderator, "appenderator");
-    this.segmentAllocator = Preconditions.checkNotNull(segmentAllocator, "segmentAllocator");
-    this.usedSegmentChecker = Preconditions.checkNotNull(usedSegmentChecker, "usedSegmentChecker");
-    this.dataSegmentKiller = Preconditions.checkNotNull(dataSegmentKiller, "dataSegmentKiller");
+    this.appenderator = requireNonNull(appenderator, "appenderator");
+    this.segmentAllocator = requireNonNull(segmentAllocator, "segmentAllocator");
+    this.usedSegmentChecker = requireNonNull(usedSegmentChecker, "usedSegmentChecker");
+    this.dataSegmentKiller = requireNonNull(dataSegmentKiller, "dataSegmentKiller");
     this.executor =
         MoreExecutors.listeningDecorator(
             Execs.singleThreaded(
@@ -364,8 +367,8 @@ public abstract class BaseAppenderatorDriver implements Closeable {
       final boolean skipSegmentLineageCheck,
       final boolean allowIncrementalPersists)
       throws IOException {
-    Preconditions.checkNotNull(row, "row");
-    Preconditions.checkNotNull(sequenceName, "sequenceName");
+    requireNonNull(row, "row");
+    requireNonNull(sequenceName, "sequenceName");
 
     final SegmentIdWithShardSpec identifier =
         getSegment(row, sequenceName, skipSegmentLineageCheck);
@@ -401,7 +404,7 @@ public abstract class BaseAppenderatorDriver implements Closeable {
               segmentsForSequence -> segmentsForSequence.intervalToSegmentStates.values().stream())
           .flatMap(segmentsOfInterval -> segmentsOfInterval.getAllSegments().stream())
           .map(SegmentWithState::getSegmentIdentifier)
-          .collect(Collectors.toList());
+          .collect(toList());
     }
   }
 
@@ -415,7 +418,7 @@ public abstract class BaseAppenderatorDriver implements Closeable {
           .map(segmentsOfInterval -> segmentsOfInterval.appendingSegment)
           .filter(Objects::nonNull)
           .map(SegmentWithState::getSegmentIdentifier)
-          .collect(Collectors.toSet());
+          .collect(toSet());
     }
   }
 
@@ -443,7 +446,7 @@ public abstract class BaseAppenderatorDriver implements Closeable {
               final Set<SegmentIdWithShardSpec> pushedSegments =
                   segmentsAndMetadata.getSegments().stream()
                       .map(SegmentIdWithShardSpec::fromDataSegment)
-                      .collect(Collectors.toSet());
+                      .collect(toSet());
 
               if (!pushedSegments.equals(Sets.newHashSet(segmentIdentifiers))) {
                 log.warn(
@@ -480,7 +483,7 @@ public abstract class BaseAppenderatorDriver implements Closeable {
         Futures.allAsList(
             segmentsAndCommitMetadata.getSegments().stream()
                 .map(segment -> appenderator.drop(SegmentIdWithShardSpec.fromDataSegment(segment)))
-                .collect(Collectors.toList()));
+                .collect(toList()));
 
     return Futures.transform(
         dropFuture,
@@ -576,7 +579,7 @@ public abstract class BaseAppenderatorDriver implements Closeable {
               final Set<SegmentIdWithShardSpec> segmentsIdentifiers =
                   segmentsAndCommitMetadata.getSegments().stream()
                       .map(SegmentIdWithShardSpec::fromDataSegment)
-                      .collect(Collectors.toSet());
+                      .collect(toSet());
 
               final Set<DataSegment> activeSegments =
                   usedSegmentChecker.findUsedSegments(segmentsIdentifiers);
@@ -592,14 +595,9 @@ public abstract class BaseAppenderatorDriver implements Closeable {
                 // (this means
                 // they were probably pushed by a replica, and with the unique paths option).
                 final boolean physicallyDisjoint =
-                    Sets.intersection(
-                            activeSegments.stream()
-                                .map(DataSegment::getLoadSpec)
-                                .collect(Collectors.toSet()),
-                            ourSegments.stream()
-                                .map(DataSegment::getLoadSpec)
-                                .collect(Collectors.toSet()))
-                        .isEmpty();
+                    disjoint(
+                        activeSegments.stream().map(DataSegment::getLoadSpec).collect(toSet()),
+                        ourSegments.stream().map(DataSegment::getLoadSpec).collect(toSet()));
 
                 if (physicallyDisjoint) {
                   segmentsAndCommitMetadata.getSegments().forEach(dataSegmentKiller::killQuietly);
@@ -680,22 +678,19 @@ public abstract class BaseAppenderatorDriver implements Closeable {
     final AppenderatorDriverMetadata wrappedMetadata;
     final Map<String, SegmentsForSequence> snapshot;
     synchronized (segments) {
-      snapshot = ImmutableMap.copyOf(segments);
+      snapshot = segments;
     }
 
     wrappedMetadata =
         new AppenderatorDriverMetadata(
-            ImmutableMap.copyOf(
-                Maps.transformValues(
-                    snapshot,
-                    (Function<SegmentsForSequence, List<SegmentWithState>>)
-                        input ->
-                            ImmutableList.copyOf(
-                                input.intervalToSegmentStates.values().stream()
-                                    .flatMap(
-                                        segmentsOfInterval ->
-                                            segmentsOfInterval.getAllSegments().stream())
-                                    .collect(Collectors.toList())))),
+            Maps.transformValues(
+                snapshot,
+                (Function<SegmentsForSequence, List<SegmentWithState>>)
+                    input ->
+                        input.intervalToSegmentStates.values().stream()
+                            .flatMap(
+                                segmentsOfInterval -> segmentsOfInterval.getAllSegments().stream())
+                            .collect(toList())),
             CollectionUtils.mapValues(
                 snapshot, segmentsForSequence -> segmentsForSequence.lastSegmentId),
             committer.getMetadata());
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BatchAppenderator.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BatchAppenderator.java
index 6adc8245bc..70e5a7e24e 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BatchAppenderator.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BatchAppenderator.java
@@ -19,14 +19,16 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.joining;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Stopwatch;
 import com.google.common.base.Supplier;
 import com.google.common.collect.ImmutableList;
-import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import com.google.common.primitives.Ints;
@@ -51,7 +53,6 @@ import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.data.input.Committer;
 import org.apache.druid.data.input.InputRow;
@@ -163,16 +164,15 @@ public class BatchAppenderator implements Appenderator {
       ParseExceptionHandler parseExceptionHandler,
       boolean useMaxMemoryEstimates) {
     this.myId = id;
-    this.schema = Preconditions.checkNotNull(schema, "schema");
-    this.tuningConfig = Preconditions.checkNotNull(tuningConfig, "tuningConfig");
-    this.metrics = Preconditions.checkNotNull(metrics, "metrics");
-    this.dataSegmentPusher = Preconditions.checkNotNull(dataSegmentPusher, "dataSegmentPusher");
-    this.objectMapper = Preconditions.checkNotNull(objectMapper, "objectMapper");
-    this.indexIO = Preconditions.checkNotNull(indexIO, "indexIO");
-    this.indexMerger = Preconditions.checkNotNull(indexMerger, "indexMerger");
-    this.rowIngestionMeters = Preconditions.checkNotNull(rowIngestionMeters, "rowIngestionMeters");
-    this.parseExceptionHandler =
-        Preconditions.checkNotNull(parseExceptionHandler, "parseExceptionHandler");
+    this.schema = requireNonNull(schema, "schema");
+    this.tuningConfig = requireNonNull(tuningConfig, "tuningConfig");
+    this.metrics = requireNonNull(metrics, "metrics");
+    this.dataSegmentPusher = requireNonNull(dataSegmentPusher, "dataSegmentPusher");
+    this.objectMapper = requireNonNull(objectMapper, "objectMapper");
+    this.indexIO = requireNonNull(indexIO, "indexIO");
+    this.indexMerger = requireNonNull(indexMerger, "indexMerger");
+    this.rowIngestionMeters = requireNonNull(rowIngestionMeters, "rowIngestionMeters");
+    this.parseExceptionHandler = requireNonNull(parseExceptionHandler, "parseExceptionHandler");
 
     maxBytesTuningConfig = tuningConfig.getMaxBytesInMemoryOrDefault();
     skipBytesInMemoryOverheadCheck = tuningConfig.isSkipBytesInMemoryOverheadCheck();
@@ -243,10 +243,9 @@ public class BatchAppenderator implements Appenderator {
 
     throwPersistErrorIfExists();
 
-    Preconditions.checkArgument(
-        committerSupplier == null, "Batch appenderator does not need a committer!");
+    checkArgument(committerSupplier == null, "Batch appenderator does not need a committer!");
 
-    Preconditions.checkArgument(
+    checkArgument(
         allowIncrementalPersists, "Batch appenderator should always allow incremental persists!");
 
     if (!identifier.getDataSource().equals(schema.getDataSource())) {
@@ -573,7 +572,7 @@ public class BatchAppenderator implements Appenderator {
                         .filter(itp -> itp.rhs != null)
                         .map(itp -> itp.rhs.asSegmentId().toString())
                         .distinct()
-                        .collect(Collectors.joining(", ")));
+                        .collect(joining(", ")));
                 log.info(
                     "Persisted stats: processed rows: [%d], persisted rows[%d], persisted sinks: [%d], persisted fireHydrants (across sinks): [%d]",
                     rowIngestionMeters.getProcessed(),
@@ -618,7 +617,7 @@ public class BatchAppenderator implements Appenderator {
    *     complete since we will not be keeping a reference to it...
    */
   Map<SegmentIdWithShardSpec, Sink> swapSinks() {
-    Map<SegmentIdWithShardSpec, Sink> retVal = ImmutableMap.copyOf(sinks);
+    Map<SegmentIdWithShardSpec, Sink> retVal = sinks;
     sinks.clear();
     resetSinkMetadata();
     return retVal;
@@ -798,7 +797,7 @@ public class BatchAppenderator implements Appenderator {
                               IndexMerger.getMergedDimensionsFromQueryableIndexes(
                                   indexes, schema.getDimensionsSpec())),
                       false),
-              exception -> exception instanceof Exception,
+              Exception.class::isInstance,
               5);
 
       // Drop the queryable indexes behind the hydrants... they are not needed anymore and their
@@ -925,8 +924,8 @@ public class BatchAppenderator implements Appenderator {
     }
   }
 
-  @VisibleForTesting
   @Nullable
+  @VisibleForTesting
   public List<File> getPersistedidentifierPaths() {
 
     ArrayList<File> retVal = new ArrayList<>();
@@ -957,14 +956,15 @@ public class BatchAppenderator implements Appenderator {
       throws IOException {
     // To avoid reading and listing of "merged" dir and other special files
     final File[] sinkFiles =
-        identifierPath.listFiles((dir, fileName) -> !(Ints.tryParse(fileName) == null));
+        identifierPath.listFiles((dir, fileName) -> Ints.tryParse(fileName) != null);
     if (sinkFiles == null) {
       throw new ISE("Problem reading persisted sinks in path[%s]", identifierPath);
     }
 
     Arrays.sort(
         sinkFiles,
-        (o1, o2) -> Ints.compare(Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName())));
+        (o1, o2) ->
+            Integer.compare(Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName())));
 
     List<FireHydrant> hydrants = new ArrayList<>();
     for (File hydrantDir : sinkFiles) {
@@ -1141,8 +1141,7 @@ public class BatchAppenderator implements Appenderator {
     }
     // These calculations are approximated from actual heap dumps.
     int total;
-    total = Integer.BYTES + (4 * Short.BYTES) + ROUGH_OVERHEAD_PER_HYDRANT;
-    return total;
+    return Integer.BYTES + (4 * Short.BYTES) + ROUGH_OVERHEAD_PER_HYDRANT;
   }
 
   private int calculateSinkMemoryInUsed() {
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BatchAppenderatorDriver.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BatchAppenderatorDriver.java
index 6ebd897960..1303abb07b 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BatchAppenderatorDriver.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/BatchAppenderatorDriver.java
@@ -19,21 +19,23 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static java.util.Collections.emptySet;
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toMap;
+
 import com.google.common.collect.ImmutableList;
-import com.google.common.collect.ImmutableMap;
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
 import java.io.IOException;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.data.input.InputRow;
 import org.apache.druid.java.util.common.ISE;
@@ -86,8 +88,8 @@ public class BatchAppenderatorDriver extends BaseAppenderatorDriver {
    *
    * @return always null
    */
-  @Override
   @Nullable
+  @Override
   public Object startJob(AppenderatorDriverSegmentLockHelper lockHelper) {
     final Object metadata = appenderator.startJob();
     if (metadata != null) {
@@ -144,8 +146,7 @@ public class BatchAppenderatorDriver extends BaseAppenderatorDriver {
     // Sanity check
     final Map<SegmentIdWithShardSpec, DataSegment> pushedSegmentIdToSegmentMap =
         segmentsAndCommitMetadata.getSegments().stream()
-            .collect(
-                Collectors.toMap(SegmentIdWithShardSpec::fromDataSegment, Function.identity()));
+            .collect(toMap(SegmentIdWithShardSpec::fromDataSegment, identity()));
 
     if (!pushedSegmentIdToSegmentMap.keySet().equals(requestedSegmentIdsForSequences)) {
       throw new ISE(
@@ -202,23 +203,23 @@ public class BatchAppenderatorDriver extends BaseAppenderatorDriver {
       final Function<Set<DataSegment>, Set<DataSegment>> outputSegmentsAnnotateFunction) {
     final Map<String, SegmentsForSequence> snapshot;
     synchronized (segments) {
-      snapshot = ImmutableMap.copyOf(segments);
+      snapshot = segments;
     }
 
     return publishInBackground(
         segmentsToBeOverwritten,
         segmentsToBeDropped,
-        tombstones == null ? Collections.emptySet() : tombstones,
+        tombstones == null ? emptySet() : tombstones,
         new SegmentsAndCommitMetadata(
             snapshot.values().stream()
                 .flatMap(SegmentsForSequence::allSegmentStateStream)
                 .map(
                     segmentWithState ->
-                        Preconditions.checkNotNull(
+                        checkNotNull(
                             segmentWithState.getDataSegment(),
                             "dataSegment for segmentId[%s]",
                             segmentWithState.getSegmentIdentifier()))
-                .collect(Collectors.toList()),
+                .collect(toList()),
             null),
         publisher,
         outputSegmentsAnnotateFunction);
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentIdWithShardSpec.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentIdWithShardSpec.java
index b10fc0d224..501adcf159 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentIdWithShardSpec.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentIdWithShardSpec.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import org.apache.druid.timeline.DataSegment;
 import org.apache.druid.timeline.SegmentId;
 import org.apache.druid.timeline.partition.ShardSpec;
@@ -48,7 +49,7 @@ public final class SegmentIdWithShardSpec implements Comparable<SegmentIdWithSha
       @JsonProperty("version") String version,
       @JsonProperty("shardSpec") ShardSpec shardSpec) {
     this.id = SegmentId.of(dataSource, interval, version, shardSpec.getPartitionNum());
-    this.shardSpec = Preconditions.checkNotNull(shardSpec, "shardSpec");
+    this.shardSpec = requireNonNull(shardSpec, "shardSpec");
     this.asString = id.toString();
   }
 
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentPublisherHelper.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentPublisherHelper.java
index d074cb7b2c..8d67764a40 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentPublisherHelper.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentPublisherHelper.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toSet;
+
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashMap;
@@ -27,7 +30,6 @@ import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 import org.apache.druid.java.util.common.ISE;
 import org.apache.druid.java.util.common.logger.Logger;
 import org.apache.druid.timeline.DataSegment;
@@ -102,13 +104,11 @@ public final class SegmentPublisherHelper {
 
       if (annotateFn != null) {
         intervalToSegments.put(
-            interval, segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));
+            interval, segmentsPerInterval.stream().map(annotateFn).collect(toList()));
       }
     }
 
-    return intervalToSegments.values().stream()
-        .flatMap(Collection::stream)
-        .collect(Collectors.toSet());
+    return intervalToSegments.values().stream().flatMap(Collection::stream).collect(toSet());
   }
 
   private static Function<DataSegment, DataSegment> annotateAtomicUpdateGroupFn(
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentWithState.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentWithState.java
index 79c8187e3b..51a6cb0207 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentWithState.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentWithState.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static com.google.common.base.Preconditions.checkState;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.Collection;
 import java.util.List;
 import javax.annotation.Nullable;
@@ -48,7 +49,7 @@ public class SegmentWithState {
     APPEND_FINISHED, // only used in StreamAppenderatorDriver
     PUSHED_AND_DROPPED; // only used in BatchAppenderatorDriver
 
-    @JsonCreator
+    @JsonCreator(mode = JsonCreator.Mode.DELEGATING)
     public static SegmentState fromString(@JsonProperty String name) {
       if ("ACTIVE".equalsIgnoreCase(name)) {
         return APPENDING;
@@ -131,7 +132,7 @@ public class SegmentWithState {
 
   private static void checkStateTransition(
       SegmentState actualFrom, SegmentState expectedFrom, SegmentState to) {
-    Preconditions.checkState(
+    checkState(
         actualFrom == expectedFrom, "Wrong state transition from [%s] to [%s]", actualFrom, to);
   }
 
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentsAndCommitMetadata.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentsAndCommitMetadata.java
index ecdd9f7531..a3e2b30938 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentsAndCommitMetadata.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentsAndCommitMetadata.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Collections.emptyList;
+
 import com.google.common.collect.ImmutableList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Objects;
 import javax.annotation.Nullable;
@@ -29,7 +30,7 @@ import org.apache.druid.timeline.DataSegment;
 
 public class SegmentsAndCommitMetadata {
   private static final SegmentsAndCommitMetadata NIL =
-      new SegmentsAndCommitMetadata(Collections.emptyList(), null);
+      new SegmentsAndCommitMetadata(emptyList(), null);
 
   private final Object commitMetadata;
   private final ImmutableList<DataSegment> segments;
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java
index f943460e81..057284c200 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.Iterables;
 import java.io.Closeable;
 import java.util.Optional;
@@ -100,18 +101,16 @@ public class SinkQuerySegmentWalker implements QuerySegmentWalker {
       Cache cache,
       CacheConfig cacheConfig,
       CachePopulatorStats cachePopulatorStats) {
-    this.dataSource = Preconditions.checkNotNull(dataSource, "dataSource");
-    this.sinkTimeline = Preconditions.checkNotNull(sinkTimeline, "sinkTimeline");
-    this.objectMapper = Preconditions.checkNotNull(objectMapper, "objectMapper");
-    this.emitter = Preconditions.checkNotNull(emitter, "emitter");
-    this.conglomerate = Preconditions.checkNotNull(conglomerate, "conglomerate");
-    this.queryProcessingPool =
-        Preconditions.checkNotNull(queryProcessingPool, "queryProcessingPool");
+    this.dataSource = requireNonNull(dataSource, "dataSource");
+    this.sinkTimeline = requireNonNull(sinkTimeline, "sinkTimeline");
+    this.objectMapper = requireNonNull(objectMapper, "objectMapper");
+    this.emitter = requireNonNull(emitter, "emitter");
+    this.conglomerate = requireNonNull(conglomerate, "conglomerate");
+    this.queryProcessingPool = requireNonNull(queryProcessingPool, "queryProcessingPool");
     this.joinableFactoryWrapper = joinableFactoryWrapper;
-    this.cache = Preconditions.checkNotNull(cache, "cache");
-    this.cacheConfig = Preconditions.checkNotNull(cacheConfig, "cacheConfig");
-    this.cachePopulatorStats =
-        Preconditions.checkNotNull(cachePopulatorStats, "cachePopulatorStats");
+    this.cache = requireNonNull(cache, "cache");
+    this.cacheConfig = requireNonNull(cacheConfig, "cacheConfig");
+    this.cachePopulatorStats = requireNonNull(cachePopulatorStats, "cachePopulatorStats");
 
     if (!cache.isLocal()) {
       log.warn(
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderator.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderator.java
index d6d7e5c3a5..004b1d0b66 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderator.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderator.java
@@ -19,11 +19,14 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.joining;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Function;
-import com.google.common.base.Joiner;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Stopwatch;
 import com.google.common.base.Supplier;
 import com.google.common.collect.ImmutableList;
@@ -59,7 +62,6 @@ import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.commons.lang.mutable.MutableLong;
 import org.apache.druid.client.cache.Cache;
@@ -189,19 +191,18 @@ public class StreamAppenderator implements Appenderator {
       ParseExceptionHandler parseExceptionHandler,
       boolean useMaxMemoryEstimates) {
     this.myId = id;
-    this.schema = Preconditions.checkNotNull(schema, "schema");
-    this.tuningConfig = Preconditions.checkNotNull(tuningConfig, "tuningConfig");
-    this.metrics = Preconditions.checkNotNull(metrics, "metrics");
-    this.dataSegmentPusher = Preconditions.checkNotNull(dataSegmentPusher, "dataSegmentPusher");
-    this.objectMapper = Preconditions.checkNotNull(objectMapper, "objectMapper");
-    this.segmentAnnouncer = Preconditions.checkNotNull(segmentAnnouncer, "segmentAnnouncer");
-    this.indexIO = Preconditions.checkNotNull(indexIO, "indexIO");
-    this.indexMerger = Preconditions.checkNotNull(indexMerger, "indexMerger");
+    this.schema = requireNonNull(schema, "schema");
+    this.tuningConfig = requireNonNull(tuningConfig, "tuningConfig");
+    this.metrics = requireNonNull(metrics, "metrics");
+    this.dataSegmentPusher = requireNonNull(dataSegmentPusher, "dataSegmentPusher");
+    this.objectMapper = requireNonNull(objectMapper, "objectMapper");
+    this.segmentAnnouncer = requireNonNull(segmentAnnouncer, "segmentAnnouncer");
+    this.indexIO = requireNonNull(indexIO, "indexIO");
+    this.indexMerger = requireNonNull(indexMerger, "indexMerger");
     this.cache = cache;
     this.texasRanger = sinkQuerySegmentWalker;
-    this.rowIngestionMeters = Preconditions.checkNotNull(rowIngestionMeters, "rowIngestionMeters");
-    this.parseExceptionHandler =
-        Preconditions.checkNotNull(parseExceptionHandler, "parseExceptionHandler");
+    this.rowIngestionMeters = requireNonNull(rowIngestionMeters, "rowIngestionMeters");
+    this.parseExceptionHandler = requireNonNull(parseExceptionHandler, "parseExceptionHandler");
 
     if (sinkQuerySegmentWalker == null) {
       this.sinkTimeline = new VersionedIntervalTimeline<>(String.CASE_INSENSITIVE_ORDER);
@@ -473,9 +474,7 @@ public class StreamAppenderator implements Appenderator {
   @Override
   public <T> QueryRunner<T> getQueryRunnerForIntervals(
       final Query<T> query, final Iterable<Interval> intervals) {
-    if (texasRanger == null) {
-      throw new IllegalStateException("Don't query me, bro.");
-    }
+    checkState(texasRanger != null, "Don't query me, bro.");
 
     return texasRanger.getQueryRunnerForIntervals(query, intervals);
   }
@@ -483,9 +482,7 @@ public class StreamAppenderator implements Appenderator {
   @Override
   public <T> QueryRunner<T> getQueryRunnerForSegments(
       final Query<T> query, final Iterable<SegmentDescriptor> specs) {
-    if (texasRanger == null) {
-      throw new IllegalStateException("Don't query me, bro.");
-    }
+    checkState(texasRanger != null, "Don't query me, bro.");
 
     return texasRanger.getQueryRunnerForSegments(query, specs);
   }
@@ -597,14 +594,14 @@ public class StreamAppenderator implements Appenderator {
                     log.debug(
                         "Committing metadata[%s] for sinks[%s].",
                         commitMetadata,
-                        Joiner.on(", ")
-                            .join(
-                                currentHydrants.entrySet().stream()
-                                    .map(
-                                        entry ->
-                                            StringUtils.format(
-                                                "%s:%d", entry.getKey(), entry.getValue()))
-                                    .collect(Collectors.toList())));
+                        String.join(
+                            ", ",
+                            currentHydrants.entrySet().stream()
+                                .map(
+                                    entry ->
+                                        StringUtils.format(
+                                            "%s:%d", entry.getKey(), entry.getValue()))
+                                .collect(toList())));
 
                     committer.run();
 
@@ -629,7 +626,7 @@ public class StreamAppenderator implements Appenderator {
                       indexesToPersist.stream()
                           .map(itp -> itp.rhs.asSegmentId().toString())
                           .distinct()
-                          .collect(Collectors.joining(", ")));
+                          .collect(joining(", ")));
                   log.info(
                       "Persisted stats: processed rows: [%d], persisted rows[%d], sinks: [%d], total fireHydrants (across sinks): [%d], persisted fireHydrants (across sinks): [%d]",
                       rowIngestionMeters.getProcessed(),
@@ -708,7 +705,7 @@ public class StreamAppenderator implements Appenderator {
                   "Building and pushing segments: %s",
                   theSinks.keySet().stream()
                       .map(SegmentIdWithShardSpec::toString)
-                      .collect(Collectors.joining(", ")));
+                      .collect(joining(", ")));
 
               for (Map.Entry<SegmentIdWithShardSpec, Sink> entry : theSinks.entrySet()) {
                 if (droppingSinks.contains(entry.getKey())) {
@@ -859,7 +856,7 @@ public class StreamAppenderator implements Appenderator {
               // maintain exactly-once
               // semantics.
               () -> dataSegmentPusher.push(mergedFile, segmentToPush, useUniquePath),
-              exception -> exception instanceof Exception,
+              Exception.class::isInstance,
               5);
 
       final long pushFinishTime = System.nanoTime();
@@ -911,13 +908,13 @@ public class StreamAppenderator implements Appenderator {
 
     try {
       shutdownExecutors();
-      Preconditions.checkState(
+      checkState(
           persistExecutor == null || persistExecutor.awaitTermination(365, TimeUnit.DAYS),
           "persistExecutor not terminated");
-      Preconditions.checkState(
+      checkState(
           pushExecutor == null || pushExecutor.awaitTermination(365, TimeUnit.DAYS),
           "pushExecutor not terminated");
-      Preconditions.checkState(
+      checkState(
           intermediateTempExecutor == null
               || intermediateTempExecutor.awaitTermination(365, TimeUnit.DAYS),
           "intermediateTempExecutor not terminated");
@@ -961,10 +958,10 @@ public class StreamAppenderator implements Appenderator {
     try {
       shutdownExecutors();
       // We don't wait for pushExecutor to be terminated. See Javadoc for more details.
-      Preconditions.checkState(
+      checkState(
           persistExecutor == null || persistExecutor.awaitTermination(365, TimeUnit.DAYS),
           "persistExecutor not terminated");
-      Preconditions.checkState(
+      checkState(
           intermediateTempExecutor == null
               || intermediateTempExecutor.awaitTermination(365, TimeUnit.DAYS),
           "intermediateTempExecutor not terminated");
@@ -1062,7 +1059,7 @@ public class StreamAppenderator implements Appenderator {
    * @return persisted commit metadata
    */
   private Object bootstrapSinksFromDisk() {
-    Preconditions.checkState(sinks.isEmpty(), "Already bootstrapped?!");
+    checkState(sinks.isEmpty(), "Already bootstrapped?!");
 
     final File baseDir = tuningConfig.getBasePersistDirectory();
     if (!baseDir.exists()) {
@@ -1122,12 +1119,12 @@ public class StreamAppenderator implements Appenderator {
 
         // To avoid reading and listing of "merged" dir and other special files
         final File[] sinkFiles =
-            sinkDir.listFiles((dir, fileName) -> !(Ints.tryParse(fileName) == null));
+            sinkDir.listFiles((dir, fileName) -> Ints.tryParse(fileName) != null);
 
         Arrays.sort(
             sinkFiles,
             (o1, o2) ->
-                Ints.compare(Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName())));
+                Integer.compare(Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName())));
 
         List<FireHydrant> hydrants = new ArrayList<>();
         for (File hydrantDir : sinkFiles) {
@@ -1187,7 +1184,7 @@ public class StreamAppenderator implements Appenderator {
         Sets.newHashSet(Iterables.transform(sinks.keySet(), SegmentIdWithShardSpec::toString));
     final Set<String> missingSinks = Sets.difference(committed.getHydrants().keySet(), loadedSinks);
     if (!missingSinks.isEmpty()) {
-      throw new ISE("Missing committed sinks [%s]", Joiner.on(", ").join(missingSinks));
+      throw new ISE("Missing committed sinks [%s]", String.join(", ", missingSinks));
     }
 
     totalRows.set(rowsSoFar);
@@ -1406,14 +1403,12 @@ public class StreamAppenderator implements Appenderator {
     // Memory footprint includes count integer in FireHydrant, shorts in ReferenceCountingSegment,
     // Objects in SimpleQueryableIndex (such as SmooshedFileMapper, each ColumnHolder in column map,
     // etc.)
-    int total =
-        Integer.BYTES
-            + (4 * Short.BYTES)
-            + ROUGH_OVERHEAD_PER_HYDRANT
-            + (hydrant.getSegmentNumDimensionColumns() * ROUGH_OVERHEAD_PER_DIMENSION_COLUMN_HOLDER)
-            + (hydrant.getSegmentNumMetricColumns() * ROUGH_OVERHEAD_PER_METRIC_COLUMN_HOLDER)
-            + ROUGH_OVERHEAD_PER_TIME_COLUMN_HOLDER;
-    return total;
+    return Integer.BYTES
+        + (4 * Short.BYTES)
+        + ROUGH_OVERHEAD_PER_HYDRANT
+        + (hydrant.getSegmentNumDimensionColumns() * ROUGH_OVERHEAD_PER_DIMENSION_COLUMN_HOLDER)
+        + (hydrant.getSegmentNumMetricColumns() * ROUGH_OVERHEAD_PER_METRIC_COLUMN_HOLDER)
+        + ROUGH_OVERHEAD_PER_TIME_COLUMN_HOLDER;
   }
 
   private int calculateSinkMemoryInUsed(Sink sink) {
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriver.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriver.java
index dc3ae451ff..fe06f8b9b0 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriver.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriver.java
@@ -19,9 +19,14 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Comparator.comparing;
+import static java.util.Objects.requireNonNull;
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Supplier;
 import com.google.common.util.concurrent.FutureCallback;
 import com.google.common.util.concurrent.Futures;
@@ -30,14 +35,12 @@ import com.google.common.util.concurrent.SettableFuture;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Comparator;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.NavigableMap;
 import java.util.TreeMap;
 import java.util.concurrent.atomic.AtomicInteger;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.data.input.Committer;
 import org.apache.druid.data.input.InputRow;
@@ -100,14 +103,14 @@ public class StreamAppenderatorDriver extends BaseAppenderatorDriver {
     super(appenderator, segmentAllocator, usedSegmentChecker, dataSegmentKiller);
 
     this.handoffNotifier =
-        Preconditions.checkNotNull(handoffNotifierFactory, "handoffNotifierFactory")
+        requireNonNull(handoffNotifierFactory, "handoffNotifierFactory")
             .createSegmentHandoffNotifier(appenderator.getDataSource());
-    this.metrics = Preconditions.checkNotNull(metrics, "metrics");
-    this.objectMapper = Preconditions.checkNotNull(objectMapper, "objectMapper");
+    this.metrics = requireNonNull(metrics, "metrics");
+    this.objectMapper = requireNonNull(objectMapper, "objectMapper");
   }
 
-  @Override
   @Nullable
+  @Override
   public Object startJob(AppenderatorDriverSegmentLockHelper lockHelper) {
     handoffNotifier.start();
 
@@ -117,7 +120,7 @@ public class StreamAppenderatorDriver extends BaseAppenderatorDriver {
     if (metadata != null) {
       synchronized (segments) {
         final Map<String, String> lastSegmentIds = metadata.getLastSegmentIds();
-        Preconditions.checkState(
+        checkState(
             metadata.getSegments().keySet().equals(lastSegmentIds.keySet()),
             "Sequences for segment states and last segment IDs are not same");
 
@@ -275,9 +278,7 @@ public class StreamAppenderatorDriver extends BaseAppenderatorDriver {
             // version of a segment with the same identifier containing different data; see
             // DataSegmentPusher.push() docs
             pushInBackground(wrapCommitter(committer), theSegments, true),
-            sam ->
-                publishInBackground(
-                    null, null, null, sam, publisher, java.util.function.Function.identity()),
+            sam -> publishInBackground(null, null, null, sam, publisher, identity()),
             executor);
     return Futures.transform(
         publishFuture,
@@ -311,10 +312,9 @@ public class StreamAppenderatorDriver extends BaseAppenderatorDriver {
       final List<SegmentIdWithShardSpec> waitingSegmentIdList =
           segmentsAndCommitMetadata.getSegments().stream()
               .map(SegmentIdWithShardSpec::fromDataSegment)
-              .collect(Collectors.toList());
+              .collect(toList());
       final Object metadata =
-          Preconditions.checkNotNull(
-              segmentsAndCommitMetadata.getCommitMetadata(), "commitMetadata");
+          requireNonNull(segmentsAndCommitMetadata.getCommitMetadata(), "commitMetadata");
 
       if (waitingSegmentIdList.isEmpty()) {
         return Futures.immediateFuture(
@@ -399,7 +399,7 @@ public class StreamAppenderatorDriver extends BaseAppenderatorDriver {
             SegmentIdWithShardSpec, Pair<SegmentWithState, List<SegmentWithState>>>
         intervalToSegments =
             new TreeMap<>(
-                Comparator.comparing(
+                comparing(
                     SegmentIdWithShardSpec::getInterval, Comparators.intervalsByStartThenEnd()));
     private final String lastSegmentId;
 
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/UnifiedIndexerAppenderatorsManager.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/UnifiedIndexerAppenderatorsManager.java
index c7e46c03c8..0f4f502e0e 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/UnifiedIndexerAppenderatorsManager.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/UnifiedIndexerAppenderatorsManager.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.common.util.concurrent.ListeningExecutorService;
 import com.google.common.util.concurrent.MoreExecutors;
@@ -402,7 +403,7 @@ public class UnifiedIndexerAppenderatorsManager implements AppenderatorsManager
               queryRunnerFactoryConglomerateProvider.get(),
               queryProcessingPool,
               joinableFactoryWrapper,
-              Preconditions.checkNotNull(cache, "cache"),
+              requireNonNull(cache, "cache"),
               cacheConfig,
               cachePopulatorStats);
     }
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseFactory.java b/server/src/main/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseFactory.java
index 09a95443cc..4256be8664 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseFactory.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseFactory.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.segment.realtime.firehose;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
@@ -27,7 +30,6 @@ import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.jaxrs.smile.SmileMediaTypes;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.io.CountingInputStream;
 import com.google.common.util.concurrent.Uninterruptibles;
@@ -122,7 +124,7 @@ public class EventReceiverFirehoseFactory
       @JacksonInject @Smile ObjectMapper smileMapper,
       @JacksonInject EventReceiverFirehoseRegister eventReceiverFirehoseRegister,
       @JacksonInject AuthorizerMapper authorizerMapper) {
-    Preconditions.checkNotNull(serviceName, "serviceName");
+    requireNonNull(serviceName, "serviceName");
 
     this.serviceName = serviceName;
     this.bufferSize = bufferSize == null || bufferSize <= 0 ? DEFAULT_BUFFER_SIZE : bufferSize;
@@ -257,8 +259,8 @@ public class EventReceiverFirehoseFactory
       }
     }
 
-    @VisibleForTesting
-    synchronized @Nullable Thread getDelayedCloseExecutor() {
+    @Nullable
+    synchronized @VisibleForTesting Thread getDelayedCloseExecutor() {
       return delayedCloseExecutor;
     }
 
@@ -345,9 +347,9 @@ public class EventReceiverFirehoseFactory
      * large batches are sent with little interval, the events from the batches might be mixed up in
      * {@link #buffer} (if two {@link #addRows(Iterable)} are executed concurrently).
      */
+    @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
     @POST
     @Path("/push-events")
-    @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
     @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
     public Response addAll(InputStream in, @Context final HttpServletRequest req)
         throws JsonProcessingException {
@@ -502,9 +504,7 @@ public class EventReceiverFirehoseFactory
           }
         }
 
-        if (!added) {
-          throw new IllegalStateException("Cannot add events to closed firehose!");
-        }
+        checkState(added, "Cannot add events to closed firehose!");
       }
     }
 
@@ -516,9 +516,9 @@ public class EventReceiverFirehoseFactory
      * shutdown times jumping in arbitrary directions. But once a shutdown request is made, it can't
      * be cancelled entirely, the shutdown time could only be rescheduled with a new request.
      */
+    @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
     @POST
     @Path("/shutdown")
-    @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
     @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
     public Response shutdown(
         @QueryParam("shutoffTime") final String shutoffTimeMillis,
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/firehose/FixedCountFirehoseFactory.java b/server/src/main/java/org/apache/druid/segment/realtime/firehose/FixedCountFirehoseFactory.java
index ce36ec7c06..f6dcb368e9 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/firehose/FixedCountFirehoseFactory.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/firehose/FixedCountFirehoseFactory.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.segment.realtime.firehose;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.io.File;
 import java.io.IOException;
 import javax.annotation.Nullable;
@@ -66,7 +67,7 @@ public class FixedCountFirehoseFactory implements FirehoseFactory {
       @Nullable
       @Override
       public InputRow nextRow() throws IOException {
-        Preconditions.checkArgument(i++ < count, "Max events limit reached.");
+        checkArgument(i++ < count, "Max events limit reached.");
         return delegateFirehose.nextRow();
       }
 
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/plumber/FlushingPlumberSchool.java b/server/src/main/java/org/apache/druid/segment/realtime/plumber/FlushingPlumberSchool.java
index 4adb8c8c2e..9172081109 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/plumber/FlushingPlumberSchool.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/plumber/FlushingPlumberSchool.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.segment.realtime.plumber;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import org.apache.druid.client.cache.Cache;
 import org.apache.druid.client.cache.CacheConfig;
 import org.apache.druid.client.cache.CachePopulatorStats;
@@ -96,8 +97,8 @@ public class FlushingPlumberSchool extends RealtimePlumberSchool {
     this.segmentAnnouncer = segmentAnnouncer;
     this.queryProcessingPool = queryProcessingPool;
     this.joinableFactory = joinableFactory;
-    this.indexMergerV9 = Preconditions.checkNotNull(indexMergerV9, "Null IndexMergerV9");
-    this.indexIO = Preconditions.checkNotNull(indexIO, "Null IndexIO");
+    this.indexMergerV9 = requireNonNull(indexMergerV9, "Null IndexMergerV9");
+    this.indexIO = requireNonNull(indexIO, "Null IndexIO");
     this.cache = cache;
     this.cacheConfig = cacheConfig;
     this.cachePopulatorStats = cachePopulatorStats;
@@ -130,10 +131,9 @@ public class FlushingPlumberSchool extends RealtimePlumberSchool {
   }
 
   private void verifyState() {
-    Preconditions.checkNotNull(
+    requireNonNull(
         conglomerate, "must specify a queryRunnerFactoryConglomerate to do this action.");
-    Preconditions.checkNotNull(
-        segmentAnnouncer, "must specify a segmentAnnouncer to do this action.");
-    Preconditions.checkNotNull(emitter, "must specify a serviceEmitter to do this action.");
+    requireNonNull(segmentAnnouncer, "must specify a segmentAnnouncer to do this action.");
+    requireNonNull(emitter, "must specify a serviceEmitter to do this action.");
   }
 }
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/plumber/PlumberSchool.java b/server/src/main/java/org/apache/druid/segment/realtime/plumber/PlumberSchool.java
index 1857a86b1e..947fb2ba25 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/plumber/PlumberSchool.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/plumber/PlumberSchool.java
@@ -30,11 +30,10 @@ import org.apache.druid.segment.realtime.FireDepartmentMetrics;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = RealtimePlumberSchool.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "realtime", value = RealtimePlumberSchool.class),
-      @JsonSubTypes.Type(name = "flushing", value = FlushingPlumberSchool.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "flushing", value = FlushingPlumberSchool.class),
+  @JsonSubTypes.Type(name = "realtime", value = RealtimePlumberSchool.class)
+})
 public interface PlumberSchool {
   /**
    * Creates a Plumber
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumber.java b/server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumber.java
index 4092b69540..1d7647f1e9 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumber.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumber.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.segment.realtime.plumber;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Stopwatch;
 import com.google.common.base.Supplier;
 import com.google.common.collect.Collections2;
@@ -155,8 +156,8 @@ public class RealtimePlumber implements Plumber {
     this.dataSegmentPusher = dataSegmentPusher;
     this.segmentPublisher = segmentPublisher;
     this.handoffNotifier = handoffNotifier;
-    this.indexMerger = Preconditions.checkNotNull(indexMerger, "Null IndexMerger");
-    this.indexIO = Preconditions.checkNotNull(indexIO, "Null IndexIO");
+    this.indexMerger = requireNonNull(indexMerger, "Null IndexMerger");
+    this.indexIO = requireNonNull(indexIO, "Null IndexIO");
     this.cache = cache;
     this.texasRanger =
         new SinkQuerySegmentWalker(
@@ -610,7 +611,7 @@ public class RealtimePlumber implements Plumber {
               new FilenameFilter() {
                 @Override
                 public boolean accept(File dir, String fileName) {
-                  return !(Ints.tryParse(fileName) == null);
+                  return Ints.tryParse(fileName) != null;
                 }
               });
       Arrays.sort(
@@ -619,7 +620,8 @@ public class RealtimePlumber implements Plumber {
             @Override
             public int compare(File o1, File o2) {
               try {
-                return Ints.compare(Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName()));
+                return Integer.compare(
+                    Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName()));
               } catch (NumberFormatException e) {
                 log.error(e, "Couldn't compare as numbers? [%s][%s]", o1, o2);
                 return o1.compareTo(o2);
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumberSchool.java b/server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumberSchool.java
index e16891275f..318a972c8d 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumberSchool.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumberSchool.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.segment.realtime.plumber;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import org.apache.druid.client.cache.Cache;
 import org.apache.druid.client.cache.CacheConfig;
 import org.apache.druid.client.cache.CachePopulatorStats;
@@ -81,8 +82,8 @@ public class RealtimePlumberSchool implements PlumberSchool {
     this.handoffNotifierFactory = handoffNotifierFactory;
     this.queryProcessingPool = queryProcessingPool;
     this.joinableFactory = joinableFactory;
-    this.indexMergerV9 = Preconditions.checkNotNull(indexMergerV9, "Null IndexMergerV9");
-    this.indexIO = Preconditions.checkNotNull(indexIO, "Null IndexIO");
+    this.indexMergerV9 = requireNonNull(indexMergerV9, "Null IndexMergerV9");
+    this.indexIO = requireNonNull(indexIO, "Null IndexIO");
 
     this.cache = cache;
     this.cacheConfig = cacheConfig;
@@ -118,16 +119,13 @@ public class RealtimePlumberSchool implements PlumberSchool {
   }
 
   private void verifyState() {
-    Preconditions.checkNotNull(
+    requireNonNull(
         conglomerate, "must specify a queryRunnerFactoryConglomerate to do this action.");
-    Preconditions.checkNotNull(
-        dataSegmentPusher, "must specify a segmentPusher to do this action.");
-    Preconditions.checkNotNull(
-        segmentAnnouncer, "must specify a segmentAnnouncer to do this action.");
-    Preconditions.checkNotNull(
-        segmentPublisher, "must specify a segmentPublisher to do this action.");
-    Preconditions.checkNotNull(
+    requireNonNull(dataSegmentPusher, "must specify a segmentPusher to do this action.");
+    requireNonNull(segmentAnnouncer, "must specify a segmentAnnouncer to do this action.");
+    requireNonNull(segmentPublisher, "must specify a segmentPublisher to do this action.");
+    requireNonNull(
         handoffNotifierFactory, "must specify a handoffNotifierFactory to do this action.");
-    Preconditions.checkNotNull(emitter, "must specify a serviceEmitter to do this action.");
+    requireNonNull(emitter, "must specify a serviceEmitter to do this action.");
   }
 }
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/plumber/RejectionPolicyFactory.java b/server/src/main/java/org/apache/druid/segment/realtime/plumber/RejectionPolicyFactory.java
index 3be7f68728..83d452509c 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/plumber/RejectionPolicyFactory.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/plumber/RejectionPolicyFactory.java
@@ -24,12 +24,11 @@ import com.fasterxml.jackson.annotation.JsonTypeInfo;
 import org.joda.time.Period;
 
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "serverTime", value = ServerTimeRejectionPolicyFactory.class),
-      @JsonSubTypes.Type(name = "messageTime", value = MessageTimeRejectionPolicyFactory.class),
-      @JsonSubTypes.Type(name = "none", value = NoopRejectionPolicyFactory.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "messageTime", value = MessageTimeRejectionPolicyFactory.class),
+  @JsonSubTypes.Type(name = "none", value = NoopRejectionPolicyFactory.class),
+  @JsonSubTypes.Type(name = "serverTime", value = ServerTimeRejectionPolicyFactory.class)
+})
 public interface RejectionPolicyFactory {
   RejectionPolicy create(Period windowPeriod);
 }
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/plumber/Sink.java b/server/src/main/java/org/apache/druid/segment/realtime/plumber/Sink.java
index 3041b669ab..7adf16f607 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/plumber/Sink.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/plumber/Sink.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.segment.realtime.plumber;
 
+import static java.util.Collections.emptyList;
+
 import com.google.common.base.Predicate;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
@@ -95,7 +96,7 @@ public class Sink implements Iterable<FireHydrant>, Overshadowable<Sink> {
         maxBytesInMemory,
         useMaxMemoryEstimates,
         dedupColumn,
-        Collections.emptyList());
+        emptyList());
   }
 
   public Sink(
@@ -231,7 +232,7 @@ public class Sink implements Iterable<FireHydrant>, Overshadowable<Sink> {
         interval,
         version,
         ImmutableMap.of(),
-        Collections.emptyList(),
+        emptyList(),
         Lists.transform(Arrays.asList(schema.getAggregators()), AggregatorFactory::getName),
         shardSpec,
         null,
diff --git a/server/src/main/java/org/apache/druid/segment/realtime/plumber/VersioningPolicy.java b/server/src/main/java/org/apache/druid/segment/realtime/plumber/VersioningPolicy.java
index da97bf7bd2..b45cd4bbac 100644
--- a/server/src/main/java/org/apache/druid/segment/realtime/plumber/VersioningPolicy.java
+++ b/server/src/main/java/org/apache/druid/segment/realtime/plumber/VersioningPolicy.java
@@ -24,11 +24,10 @@ import com.fasterxml.jackson.annotation.JsonTypeInfo;
 import org.joda.time.Interval;
 
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "intervalStart", value = IntervalStartVersioningPolicy.class),
-      @JsonSubTypes.Type(name = "custom", value = CustomVersioningPolicy.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "custom", value = CustomVersioningPolicy.class),
+  @JsonSubTypes.Type(name = "intervalStart", value = IntervalStartVersioningPolicy.class)
+})
 public interface VersioningPolicy {
   String getVersion(Interval interval);
 }
diff --git a/server/src/main/java/org/apache/druid/server/BrokerQueryResource.java b/server/src/main/java/org/apache/druid/server/BrokerQueryResource.java
index 0656dcbef6..68c56835d8 100644
--- a/server/src/main/java/org/apache/druid/server/BrokerQueryResource.java
+++ b/server/src/main/java/org/apache/druid/server/BrokerQueryResource.java
@@ -73,14 +73,14 @@ public class BrokerQueryResource extends QueryResource {
     this.brokerServerView = brokerServerView;
   }
 
-  @POST
-  @Path("/candidates")
-  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @Consumes({
+    APPLICATION_SMILE,
     MediaType.APPLICATION_JSON,
-    SmileMediaTypes.APPLICATION_JACKSON_SMILE,
-    APPLICATION_SMILE
+    SmileMediaTypes.APPLICATION_JACKSON_SMILE
   })
+  @POST
+  @Path("/candidates")
+  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @ResourceFilters(StateResourceFilter.class)
   public Response getQueryTargets(
       InputStream in,
diff --git a/server/src/main/java/org/apache/druid/server/ClientInfoResource.java b/server/src/main/java/org/apache/druid/server/ClientInfoResource.java
index a6b7494627..5fb8c4e58b 100644
--- a/server/src/main/java/org/apache/druid/server/ClientInfoResource.java
+++ b/server/src/main/java/org/apache/druid/server/ClientInfoResource.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toSet;
+
 import com.google.common.base.Function;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
@@ -26,7 +30,6 @@ import com.google.common.collect.Maps;
 import com.google.inject.Inject;
 import com.sun.jersey.spi.container.ResourceFilters;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashSet;
 import java.util.List;
@@ -34,7 +37,6 @@ import java.util.Map;
 import java.util.Optional;
 import java.util.Set;
 import java.util.TreeMap;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 import javax.servlet.http.HttpServletRequest;
 import javax.ws.rs.DefaultValue;
@@ -105,7 +107,7 @@ public class ClientInfoResource {
   public Iterable<String> getDataSources(@Context final HttpServletRequest request) {
     Function<String, Iterable<ResourceAction>> raGenerator =
         datasourceName -> {
-          return Collections.singletonList(
+          return singletonList(
               AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(datasourceName));
         };
 
@@ -115,8 +117,9 @@ public class ClientInfoResource {
 
   private Set<String> getAllDataSources() {
     return serverInventoryView.getInventory().stream()
-        .flatMap(server -> server.getDataSources().stream().map(DruidDataSource::getName))
-        .collect(Collectors.toSet());
+        .flatMap(server -> server.getDataSources().stream())
+        .map(DruidDataSource::getName)
+        .collect(toSet());
   }
 
   @GET
@@ -146,7 +149,7 @@ public class ClientInfoResource {
     final Optional<Iterable<TimelineObjectHolder<String, ServerSelector>>> maybeServersLookup =
         maybeTimeline.map(timeline -> timeline.lookup(theInterval));
     if (!maybeServersLookup.isPresent() || Iterables.isEmpty(maybeServersLookup.get())) {
-      return Collections.emptyMap();
+      return emptyMap();
     }
     Map<Interval, Object> servedIntervals =
         new TreeMap<>(
@@ -264,7 +267,7 @@ public class ClientInfoResource {
               }
               return dataSource.getSegments().stream();
             })
-        .collect(Collectors.toSet());
+        .collect(toSet());
   }
 
   @GET
diff --git a/server/src/main/java/org/apache/druid/server/ClientQuerySegmentWalker.java b/server/src/main/java/org/apache/druid/server/ClientQuerySegmentWalker.java
index d92d30f2f4..fcfa182bf9 100644
--- a/server/src/main/java/org/apache/druid/server/ClientQuerySegmentWalker.java
+++ b/server/src/main/java/org/apache/druid/server/ClientQuerySegmentWalker.java
@@ -19,20 +19,21 @@
 
 package org.apache.druid.server;
 
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.Iterables;
 import com.google.inject.Inject;
 import java.util.ArrayDeque;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Queue;
 import java.util.Stack;
 import java.util.concurrent.atomic.AtomicInteger;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.commons.lang.StringUtils;
 import org.apache.druid.client.CachingClusteredClient;
@@ -333,7 +334,7 @@ public class ClientQuerySegmentWalker implements QuerySegmentWalker {
             inlineIfNecessary(current, null, subqueryRowLimitAccumulator, maxSubqueryRows, dryRun);
 
         while (!stack.isEmpty()) {
-          current = stack.pop().withChildren(Collections.singletonList(current));
+          current = stack.pop().withChildren(singletonList(current));
         }
 
         if (!(current instanceof QueryDataSource)) {
@@ -372,7 +373,7 @@ public class ClientQuerySegmentWalker implements QuerySegmentWalker {
         // Cannot inline subquery. Attempt to inline one level deeper, and then try again.
         return inlineIfNecessary(
             dataSource.withChildren(
-                Collections.singletonList(
+                singletonList(
                     inlineIfNecessary(
                         Iterables.getOnlyElement(dataSource.getChildren()),
                         null,
@@ -392,7 +393,7 @@ public class ClientQuerySegmentWalker implements QuerySegmentWalker {
                   child ->
                       inlineIfNecessary(
                           child, null, subqueryRowLimitAccumulator, maxSubqueryRows, dryRun))
-              .collect(Collectors.toList()));
+              .collect(toList()));
     }
   }
 
@@ -524,7 +525,7 @@ public class ClientQuerySegmentWalker implements QuerySegmentWalker {
                         queryDataSourceToSubqueryIds,
                         parentQueryId,
                         parentSqlQueryId))
-            .collect(Collectors.toList()));
+            .collect(toList()));
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/server/DruidNode.java b/server/src/main/java/org/apache/druid/server/DruidNode.java
index 55b8062857..7202de9a42 100644
--- a/server/src/main/java/org/apache/druid/server/DruidNode.java
+++ b/server/src/main/java/org/apache/druid/server/DruidNode.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import com.google.common.net.HostAndPort;
 import com.google.inject.name.Named;
 import java.net.InetAddress;
@@ -133,7 +134,7 @@ public class DruidNode {
       Integer tlsPort,
       boolean enablePlaintextPort,
       boolean enableTlsPort) {
-    Preconditions.checkNotNull(serviceName);
+    requireNonNull(serviceName);
 
     if (!enableTlsPort && !enablePlaintextPort) {
       throw new IAE(
diff --git a/server/src/main/java/org/apache/druid/server/QueryLaningStrategy.java b/server/src/main/java/org/apache/druid/server/QueryLaningStrategy.java
index 88803a4d49..fbdaf8d9dd 100644
--- a/server/src/main/java/org/apache/druid/server/QueryLaningStrategy.java
+++ b/server/src/main/java/org/apache/druid/server/QueryLaningStrategy.java
@@ -21,7 +21,6 @@ package org.apache.druid.server;
 
 import com.fasterxml.jackson.annotation.JsonSubTypes;
 import com.fasterxml.jackson.annotation.JsonTypeInfo;
-import com.google.common.primitives.Ints;
 import it.unimi.dsi.fastutil.objects.Object2IntMap;
 import java.util.Optional;
 import java.util.Set;
@@ -35,12 +34,11 @@ import org.apache.druid.server.scheduling.NoQueryLaningStrategy;
     use = JsonTypeInfo.Id.NAME,
     property = "strategy",
     defaultImpl = NoQueryLaningStrategy.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "none", value = NoQueryLaningStrategy.class),
-      @JsonSubTypes.Type(name = "hilo", value = HiLoQueryLaningStrategy.class),
-      @JsonSubTypes.Type(name = "manual", value = ManualQueryLaningStrategy.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "hilo", value = HiLoQueryLaningStrategy.class),
+  @JsonSubTypes.Type(name = "manual", value = ManualQueryLaningStrategy.class),
+  @JsonSubTypes.Type(name = "none", value = NoQueryLaningStrategy.class)
+})
 public interface QueryLaningStrategy {
   /**
    * Provide a map of lane names to the limit on the number of concurrent queries for that lane
@@ -58,6 +56,6 @@ public interface QueryLaningStrategy {
   <T> Optional<String> computeLane(QueryPlus<T> query, Set<SegmentServerSelector> segments);
 
   default int computeLimitFromPercent(int totalLimit, int value) {
-    return Ints.checkedCast((long) Math.ceil(totalLimit * ((double) value / 100)));
+    return Math.toIntExact((long) Math.ceil(totalLimit * ((double) value / 100)));
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/QueryLifecycle.java b/server/src/main/java/org/apache/druid/server/QueryLifecycle.java
index 465c786aff..d2fb13f99b 100644
--- a/server/src/main/java/org/apache/druid/server/QueryLifecycle.java
+++ b/server/src/main/java/org/apache/druid/server/QueryLifecycle.java
@@ -19,15 +19,16 @@
 
 package org.apache.druid.server;
 
+import static java.util.Objects.requireNonNull;
+import static java.util.UUID.randomUUID;
+
 import com.fasterxml.jackson.databind.ObjectWriter;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Strings;
 import com.google.common.collect.Iterables;
 import java.util.HashSet;
 import java.util.LinkedHashMap;
 import java.util.Map;
 import java.util.Set;
-import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 import javax.annotation.Nullable;
 import javax.servlet.http.HttpServletRequest;
@@ -186,7 +187,7 @@ public class QueryLifecycle {
     userContextKeys = new HashSet<>(baseQuery.getContext().keySet());
     String queryId = baseQuery.getId();
     if (Strings.isNullOrEmpty(queryId)) {
-      queryId = UUID.randomUUID().toString();
+      queryId = randomUUID().toString();
     }
 
     Map<String, Object> mergedUserAndConfigContext =
@@ -230,8 +231,8 @@ public class QueryLifecycle {
 
   private Access doAuthorize(
       final AuthenticationResult authenticationResult, final Access authorizationResult) {
-    Preconditions.checkNotNull(authenticationResult, "authenticationResult");
-    Preconditions.checkNotNull(authorizationResult, "authorizationResult");
+    requireNonNull(authenticationResult, "authenticationResult");
+    requireNonNull(authorizationResult, "authorizationResult");
 
     if (!authorizationResult.isAllowed()) {
       // Not authorized; go straight to Jail, do not pass Go.
diff --git a/server/src/main/java/org/apache/druid/server/QueryPrioritizationStrategy.java b/server/src/main/java/org/apache/druid/server/QueryPrioritizationStrategy.java
index 69d889bc07..fbd8a4e50b 100644
--- a/server/src/main/java/org/apache/druid/server/QueryPrioritizationStrategy.java
+++ b/server/src/main/java/org/apache/druid/server/QueryPrioritizationStrategy.java
@@ -32,13 +32,10 @@ import org.apache.druid.server.scheduling.ThresholdBasedQueryPrioritizationStrat
     use = JsonTypeInfo.Id.NAME,
     property = "strategy",
     defaultImpl = ManualQueryPrioritizationStrategy.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "manual", value = ManualQueryPrioritizationStrategy.class),
-      @JsonSubTypes.Type(
-          name = "threshold",
-          value = ThresholdBasedQueryPrioritizationStrategy.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "manual", value = ManualQueryPrioritizationStrategy.class),
+  @JsonSubTypes.Type(name = "threshold", value = ThresholdBasedQueryPrioritizationStrategy.class)
+})
 public interface QueryPrioritizationStrategy {
   <T> Optional<Integer> computePriority(QueryPlus<T> query, Set<SegmentServerSelector> segments);
 }
diff --git a/server/src/main/java/org/apache/druid/server/QueryResource.java b/server/src/main/java/org/apache/druid/server/QueryResource.java
index 14e0a50fc2..f873013454 100644
--- a/server/src/main/java/org/apache/druid/server/QueryResource.java
+++ b/server/src/main/java/org/apache/druid/server/QueryResource.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.core.JsonParseException;
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
@@ -28,7 +30,6 @@ import com.fasterxml.jackson.databind.module.SimpleModule;
 import com.fasterxml.jackson.datatype.joda.ser.DateTimeSerializer;
 import com.fasterxml.jackson.jaxrs.smile.SmileMediaTypes;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Strings;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
@@ -160,14 +161,14 @@ public class QueryResource implements QueryCountStatsProvider {
     return Response.status(Response.Status.ACCEPTED).build();
   }
 
-  @POST
-  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @Consumes({
+    APPLICATION_SMILE,
     MediaType.APPLICATION_JSON,
-    SmileMediaTypes.APPLICATION_JACKSON_SMILE,
-    APPLICATION_SMILE
+    SmileMediaTypes.APPLICATION_JACKSON_SMILE
   })
   @Nullable
+  @POST
+  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response doPost(
       final InputStream in,
       @QueryParam("pretty") final String pretty,
@@ -410,8 +411,7 @@ public class QueryResource implements QueryCountStatsProvider {
           serializeDateTimeAsLong ? serializeDateTimeAsLongInputMapper : inputMapper;
       final ObjectMapper decoratedMapper;
       if (toolChest != null) {
-        decoratedMapper =
-            toolChest.decorateObjectMapper(mapper, Preconditions.checkNotNull(query, "query"));
+        decoratedMapper = toolChest.decorateObjectMapper(mapper, requireNonNull(query, "query"));
       } else {
         decoratedMapper = mapper;
       }
diff --git a/server/src/main/java/org/apache/druid/server/QueryResultPusher.java b/server/src/main/java/org/apache/druid/server/QueryResultPusher.java
index 036496939d..2f1bf8db64 100644
--- a/server/src/main/java/org/apache/druid/server/QueryResultPusher.java
+++ b/server/src/main/java/org/apache/druid/server/QueryResultPusher.java
@@ -434,8 +434,8 @@ public abstract class QueryResultPusher {
       }
     }
 
-    @Override
     @Nullable
+    @Override
     public Response accumulate(Response retVal, Object in) {
       if (!initialized) {
         initialize();
diff --git a/server/src/main/java/org/apache/druid/server/QueryScheduler.java b/server/src/main/java/org/apache/druid/server/QueryScheduler.java
index e1850d52bb..dbedaa3f85 100644
--- a/server/src/main/java/org/apache/druid/server/QueryScheduler.java
+++ b/server/src/main/java/org/apache/druid/server/QueryScheduler.java
@@ -162,7 +162,7 @@ public class QueryScheduler implements QueryWatcher {
             .setDimension("lane", lane.orElse("default"))
             .setDimension("dataSource", query.getDataSource().getTableNames())
             .setDimension("type", query.getType());
-    emitter.emit(builderUsr.build("query/priority", priority.orElse(Integer.valueOf(0))));
+    emitter.emit(builderUsr.build("query/priority", priority.orElseGet(() -> 0)));
     return lane.map(query::withLane).orElse(query);
   }
 
diff --git a/server/src/main/java/org/apache/druid/server/RequestLogLine.java b/server/src/main/java/org/apache/druid/server/RequestLogLine.java
index e1f61b22b4..24a6cca8b0 100644
--- a/server/src/main/java/org/apache/druid/server/RequestLogLine.java
+++ b/server/src/main/java/org/apache/druid/server/RequestLogLine.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Joiner;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import java.util.Arrays;
 import java.util.Map;
@@ -53,9 +54,9 @@ public class RequestLogLine {
     this.query = query;
     this.sql = sql;
     this.sqlQueryContext = sqlQueryContext != null ? sqlQueryContext : ImmutableMap.of();
-    this.timestamp = Preconditions.checkNotNull(timestamp, "timestamp");
+    this.timestamp = requireNonNull(timestamp, "timestamp");
     this.remoteAddr = StringUtils.nullToEmptyNonDruidDataString(remoteAddr);
-    this.queryStats = Preconditions.checkNotNull(queryStats, "queryStats");
+    this.queryStats = requireNonNull(queryStats, "queryStats");
   }
 
   public static RequestLogLine forNative(
@@ -93,20 +94,20 @@ public class RequestLogLine {
                     "query", sql == null ? "<unavailable>" : sql, "context", sqlQueryContext))));
   }
 
-  @Nullable
   @JsonProperty("query")
+  @Nullable
   public Query<?> getQuery() {
     return query;
   }
 
-  @Nullable
   @JsonProperty("sql")
+  @Nullable
   public String getSql() {
     return sql;
   }
 
-  @Nullable
   @JsonProperty
+  @Nullable
   public Map<String, Object> getSqlQueryContext() {
     return sqlQueryContext;
   }
@@ -116,8 +117,8 @@ public class RequestLogLine {
     return timestamp;
   }
 
-  @Nullable
   @JsonProperty("remoteAddr")
+  @Nullable
   public String getRemoteAddr() {
     return remoteAddr;
   }
diff --git a/server/src/main/java/org/apache/druid/server/StatusResource.java b/server/src/main/java/org/apache/druid/server/StatusResource.java
index bd43e88ae3..092c7d15fd 100644
--- a/server/src/main/java/org/apache/druid/server/StatusResource.java
+++ b/server/src/main/java/org/apache/druid/server/StatusResource.java
@@ -66,8 +66,8 @@ public class StatusResource {
 
   @GET
   @Path("/properties")
-  @ResourceFilters(ConfigResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(ConfigResourceFilter.class)
   public Map<String, String> getProperties() {
     Map<String, String> allProperties = Maps.fromProperties(properties);
     Set<String> hiddenProperties = druidServerConfig.getHiddenProperties();
@@ -103,8 +103,8 @@ public class StatusResource {
   }
 
   @GET
-  @ResourceFilters(StateResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(StateResourceFilter.class)
   public Status doGet(@Context final HttpServletRequest req) {
     return new Status(extnLoader.getLoadedModules());
   }
@@ -159,7 +159,7 @@ public class StatusResource {
           .append(lineSeparator)
           .append(lineSeparator);
 
-      if (modules.size() > 0) {
+      if (!modules.isEmpty()) {
         output.append("Registered Druid Modules").append(lineSeparator);
       } else {
         output.append("No Druid Modules loaded !");
diff --git a/server/src/main/java/org/apache/druid/server/audit/SQLAuditManager.java b/server/src/main/java/org/apache/druid/server/audit/SQLAuditManager.java
index a82c9be92e..7f6272eecf 100644
--- a/server/src/main/java/org/apache/druid/server/audit/SQLAuditManager.java
+++ b/server/src/main/java/org/apache/druid/server/audit/SQLAuditManager.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.audit;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Supplier;
@@ -180,9 +182,7 @@ public class SQLAuditManager implements AuditManager {
   }
 
   private int getLimit(int limit) throws IllegalArgumentException {
-    if (limit < 1) {
-      throw new IllegalArgumentException("Limit must be greater than zero!");
-    }
+    checkArgument(limit >= 1, "Limit must be greater than zero!");
     return limit;
   }
 
diff --git a/server/src/main/java/org/apache/druid/server/coordination/ChangeRequestsSnapshot.java b/server/src/main/java/org/apache/druid/server/coordination/ChangeRequestsSnapshot.java
index 6bf0ce358e..f3973bddb7 100644
--- a/server/src/main/java/org/apache/druid/server/coordination/ChangeRequestsSnapshot.java
+++ b/server/src/main/java/org/apache/druid/server/coordination/ChangeRequestsSnapshot.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.coordination;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.List;
 import javax.annotation.Nullable;
 
@@ -58,7 +59,7 @@ public final class ChangeRequestsSnapshot<T> {
     this.resetCause = resetCause;
 
     if (resetCounter) {
-      Preconditions.checkNotNull(resetCause, "NULL resetCause when resetCounter is true.");
+      requireNonNull(resetCause, "NULL resetCause when resetCounter is true.");
     }
 
     this.counter = counter;
@@ -70,20 +71,20 @@ public final class ChangeRequestsSnapshot<T> {
     return resetCounter;
   }
 
-  @Nullable
   @JsonProperty
+  @Nullable
   public String getResetCause() {
     return resetCause;
   }
 
-  @Nullable
   @JsonProperty
+  @Nullable
   public ChangeRequestHistory.Counter getCounter() {
     return counter;
   }
 
-  @Nullable
   @JsonProperty
+  @Nullable
   public List<T> getRequests() {
     return requests;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordination/DataSegmentAnnouncerProvider.java b/server/src/main/java/org/apache/druid/server/coordination/DataSegmentAnnouncerProvider.java
index a5d5b62b84..d8821ee586 100644
--- a/server/src/main/java/org/apache/druid/server/coordination/DataSegmentAnnouncerProvider.java
+++ b/server/src/main/java/org/apache/druid/server/coordination/DataSegmentAnnouncerProvider.java
@@ -28,6 +28,5 @@ import com.google.inject.Provider;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = BatchDataSegmentAnnouncerProvider.class)
-@JsonSubTypes(
-    value = {@JsonSubTypes.Type(name = "batch", value = BatchDataSegmentAnnouncerProvider.class)})
+@JsonSubTypes(@JsonSubTypes.Type(name = "batch", value = BatchDataSegmentAnnouncerProvider.class))
 public interface DataSegmentAnnouncerProvider extends Provider<DataSegmentAnnouncer> {}
diff --git a/server/src/main/java/org/apache/druid/server/coordination/DataSegmentChangeRequest.java b/server/src/main/java/org/apache/druid/server/coordination/DataSegmentChangeRequest.java
index 432096b15b..d1154981b4 100644
--- a/server/src/main/java/org/apache/druid/server/coordination/DataSegmentChangeRequest.java
+++ b/server/src/main/java/org/apache/druid/server/coordination/DataSegmentChangeRequest.java
@@ -25,12 +25,11 @@ import javax.annotation.Nullable;
 
 /** */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "action")
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "load", value = SegmentChangeRequestLoad.class),
-      @JsonSubTypes.Type(name = "drop", value = SegmentChangeRequestDrop.class),
-      @JsonSubTypes.Type(name = "noop", value = SegmentChangeRequestNoop.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "drop", value = SegmentChangeRequestDrop.class),
+  @JsonSubTypes.Type(name = "load", value = SegmentChangeRequestLoad.class),
+  @JsonSubTypes.Type(name = "noop", value = SegmentChangeRequestNoop.class)
+})
 public interface DataSegmentChangeRequest {
   void go(DataSegmentChangeHandler handler, @Nullable DataSegmentChangeCallback callback);
 
diff --git a/server/src/main/java/org/apache/druid/server/coordination/DruidServerMetadata.java b/server/src/main/java/org/apache/druid/server/coordination/DruidServerMetadata.java
index 4260b683ab..da14b84362 100644
--- a/server/src/main/java/org/apache/druid/server/coordination/DruidServerMetadata.java
+++ b/server/src/main/java/org/apache/druid/server/coordination/DruidServerMetadata.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.coordination;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.Objects;
 import javax.annotation.Nullable;
 
@@ -44,7 +45,7 @@ public class DruidServerMetadata {
       @JsonProperty("type") ServerType type,
       @JsonProperty("tier") String tier,
       @JsonProperty("priority") int priority) {
-    this.name = Preconditions.checkNotNull(name);
+    this.name = requireNonNull(name);
     this.hostAndPort = hostAndPort;
     this.hostAndTlsPort = hostAndTlsPort;
     this.maxSize = maxSize;
@@ -67,8 +68,8 @@ public class DruidServerMetadata {
     return hostAndPort;
   }
 
-  @Nullable
   @JsonProperty
+  @Nullable
   public String getHostAndTlsPort() {
     return hostAndTlsPort;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordination/SegmentLoadDropHandler.java b/server/src/main/java/org/apache/druid/server/coordination/SegmentLoadDropHandler.java
index 21758053e2..9cb15c01a6 100644
--- a/server/src/main/java/org/apache/druid/server/coordination/SegmentLoadDropHandler.java
+++ b/server/src/main/java/org/apache/druid/server/coordination/SegmentLoadDropHandler.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server.coordination;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Throwables;
 import com.google.common.cache.Cache;
 import com.google.common.cache.CacheBuilder;
@@ -337,7 +338,7 @@ public class SegmentLoadDropHandler implements DataSegmentChangeHandler {
       result = Status.failed(e.toString());
     } finally {
       updateRequestStatus(new SegmentChangeRequestLoad(segment), result);
-      if (null != callback) {
+      if (callback != null) {
         callback.execute();
       }
     }
@@ -401,7 +402,7 @@ public class SegmentLoadDropHandler implements DataSegmentChangeHandler {
       try {
         latch.await();
 
-        if (failedSegments.size() > 0) {
+        if (!failedSegments.isEmpty()) {
           log.makeAlert("%,d errors seen while loading segments", failedSegments.size())
               .addData("failedSegments", failedSegments)
               .emit();
@@ -480,7 +481,7 @@ public class SegmentLoadDropHandler implements DataSegmentChangeHandler {
       result = Status.failed(e.getMessage());
     } finally {
       updateRequestStatus(new SegmentChangeRequestDrop(segment), result);
-      if (null != callback) {
+      if (callback != null) {
         callback.execute();
       }
     }
@@ -765,7 +766,7 @@ public class SegmentLoadDropHandler implements DataSegmentChangeHandler {
     Status(
         @JsonProperty("state") STATE state,
         @JsonProperty("failureCause") @Nullable String failureCause) {
-      Preconditions.checkNotNull(state, "state must be non-null");
+      requireNonNull(state, "state must be non-null");
       this.state = state;
       this.failureCause = failureCause;
     }
@@ -779,8 +780,8 @@ public class SegmentLoadDropHandler implements DataSegmentChangeHandler {
       return state;
     }
 
-    @Nullable
     @JsonProperty
+    @Nullable
     public String getFailureCause() {
       return failureCause;
     }
diff --git a/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java b/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java
index 276d34a21c..35e0215ce6 100644
--- a/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java
+++ b/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.coordination;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.Lists;
 import com.google.inject.Inject;
-import java.util.Collections;
 import java.util.Optional;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.function.Function;
@@ -202,7 +203,7 @@ public class ServerManager implements QuerySegmentWalker {
         FunctionalIterable.create(specs)
             .transformCat(
                 descriptor ->
-                    Collections.singletonList(
+                    singletonList(
                         buildQueryRunnerForSegment(
                             query,
                             descriptor,
diff --git a/server/src/main/java/org/apache/druid/server/coordination/ServerType.java b/server/src/main/java/org/apache/druid/server/coordination/ServerType.java
index 3ddefacb76..3815f320c5 100644
--- a/server/src/main/java/org/apache/druid/server/coordination/ServerType.java
+++ b/server/src/main/java/org/apache/druid/server/coordination/ServerType.java
@@ -132,7 +132,7 @@ public enum ServerType {
    */
   public abstract boolean isSegmentServer();
 
-  @JsonCreator
+  @JsonCreator(mode = JsonCreator.Mode.DELEGATING)
   public static ServerType fromString(String type) {
     return ServerType.valueOf(StringUtils.toUpperCase(type).replace('-', '_'));
   }
@@ -148,8 +148,8 @@ public enum ServerType {
     }
   }
 
-  @Override
   @JsonValue
+  @Override
   public String toString() {
     return StringUtils.toLowerCase(name()).replace('_', '-');
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/BalancerStrategyFactory.java b/server/src/main/java/org/apache/druid/server/coordinator/BalancerStrategyFactory.java
index 92dc908a28..d7df4dd892 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/BalancerStrategyFactory.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/BalancerStrategyFactory.java
@@ -27,15 +27,14 @@ import com.google.common.util.concurrent.ListeningExecutorService;
     use = JsonTypeInfo.Id.NAME,
     property = "strategy",
     defaultImpl = CostBalancerStrategyFactory.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(
-          name = "diskNormalized",
-          value = DiskNormalizedCostBalancerStrategyFactory.class),
-      @JsonSubTypes.Type(name = "cost", value = CostBalancerStrategyFactory.class),
-      @JsonSubTypes.Type(name = "cachingCost", value = CachingCostBalancerStrategyFactory.class),
-      @JsonSubTypes.Type(name = "random", value = RandomBalancerStrategyFactory.class),
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "cachingCost", value = CachingCostBalancerStrategyFactory.class),
+  @JsonSubTypes.Type(name = "cost", value = CostBalancerStrategyFactory.class),
+  @JsonSubTypes.Type(
+      name = "diskNormalized",
+      value = DiskNormalizedCostBalancerStrategyFactory.class),
+  @JsonSubTypes.Type(name = "random", value = RandomBalancerStrategyFactory.class)
+})
 public interface BalancerStrategyFactory {
   BalancerStrategy createBalancerStrategy(ListeningExecutorService exec);
 }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/CachingCostBalancerStrategy.java b/server/src/main/java/org/apache/druid/server/coordinator/CachingCostBalancerStrategy.java
index cd27f151d8..35f18b4a07 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/CachingCostBalancerStrategy.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/CachingCostBalancerStrategy.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.coordinator;
 
-import com.google.common.base.Preconditions;
+import static java.util.Collections.singleton;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.util.concurrent.ListeningExecutorService;
-import java.util.Collections;
 import java.util.Set;
 import org.apache.druid.server.coordinator.cost.ClusterCostCache;
 import org.apache.druid.timeline.DataSegment;
@@ -34,7 +35,7 @@ public class CachingCostBalancerStrategy extends CostBalancerStrategy {
   public CachingCostBalancerStrategy(
       ClusterCostCache clusterCostCache, ListeningExecutorService exec) {
     super(exec);
-    this.clusterCostCache = Preconditions.checkNotNull(clusterCostCache);
+    this.clusterCostCache = requireNonNull(clusterCostCache);
   }
 
   @Override
@@ -63,7 +64,7 @@ public class CachingCostBalancerStrategy extends CostBalancerStrategy {
     // minus the cost of the segment itself
     if (server.isServingSegment(proposalSegment)) {
       cost -=
-          costCacheForSegments(server, Collections.singleton(proposalSegment))
+          costCacheForSegments(server, singleton(proposalSegment))
               .computeCost(serverName, proposalSegment);
     }
 
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorCompactionConfig.java b/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorCompactionConfig.java
index ee90b12a4c..23b0c74c42 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorCompactionConfig.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorCompactionConfig.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableList;
 import java.util.List;
 import java.util.Objects;
@@ -98,7 +99,7 @@ public class CoordinatorCompactionConfig {
 
   @Nonnull
   public static CoordinatorCompactionConfig current(final JacksonConfigManager configManager) {
-    return Preconditions.checkNotNull(watch(configManager).get(), "Got null config from watcher?!");
+    return requireNonNull(watch(configManager).get(), "Got null config from watcher?!");
   }
 
   @JsonCreator
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorDynamicConfig.java b/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorDynamicConfig.java
index db915ea067..2d99ffc777 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorDynamicConfig.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorDynamicConfig.java
@@ -19,10 +19,12 @@
 
 package org.apache.druid.server.coordinator;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonIgnore;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableSet;
 import java.util.Collection;
 import java.util.HashSet;
@@ -163,7 +165,7 @@ public class CoordinatorDynamicConfig {
           Builder.DEFAULT_PERCENT_OF_SEGMENTS_TO_CONSIDER_PER_MOVE);
       percentOfSegmentsToConsiderPerMove = Builder.DEFAULT_PERCENT_OF_SEGMENTS_TO_CONSIDER_PER_MOVE;
     }
-    Preconditions.checkArgument(
+    checkArgument(
         percentOfSegmentsToConsiderPerMove > 0 && percentOfSegmentsToConsiderPerMove <= 100,
         "percentOfSegmentsToConsiderPerMove should be between 1 and 100!");
     this.percentOfSegmentsToConsiderPerMove = percentOfSegmentsToConsiderPerMove;
@@ -187,7 +189,7 @@ public class CoordinatorDynamicConfig {
             ? Builder.DEFAULT_MAX_SEGMENTS_IN_NODE_LOADING_QUEUE
             : maxSegmentsInNodeLoadingQueue;
     this.decommissioningNodes = parseJsonStringOrArray(decommissioningNodes);
-    Preconditions.checkArgument(
+    checkArgument(
         decommissioningMaxPercentOfMaxSegmentsToMove >= 0
             && decommissioningMaxPercentOfMaxSegmentsToMove <= 100,
         "decommissioningMaxPercentOfMaxSegmentsToMove should be in range [0, 100]");
@@ -206,7 +208,7 @@ public class CoordinatorDynamicConfig {
           Builder.DEFAULT_MAX_NON_PRIMARY_REPLICANTS_TO_LOAD);
       maxNonPrimaryReplicantsToLoad = Builder.DEFAULT_MAX_NON_PRIMARY_REPLICANTS_TO_LOAD;
     }
-    Preconditions.checkArgument(
+    checkArgument(
         maxNonPrimaryReplicantsToLoad >= 0,
         "maxNonPrimaryReplicantsToLoad must be greater than or equal to 0.");
     this.maxNonPrimaryReplicantsToLoad = maxNonPrimaryReplicantsToLoad;
@@ -246,7 +248,7 @@ public class CoordinatorDynamicConfig {
 
   @Nonnull
   public static CoordinatorDynamicConfig current(final JacksonConfigManager configManager) {
-    return Preconditions.checkNotNull(watch(configManager).get(), "Got null config from watcher?!");
+    return requireNonNull(watch(configManager).get(), "Got null config from watcher?!");
   }
 
   @JsonProperty("millisToWaitBeforeDeleting")
@@ -352,9 +354,9 @@ public class CoordinatorDynamicConfig {
    *
    * @return number in range [0, 100]
    */
-  @Min(0)
-  @Max(100)
   @JsonProperty
+  @Max(100)
+  @Min(0)
   public int getDecommissioningMaxPercentOfMaxSegmentsToMove() {
     return decommissioningMaxPercentOfMaxSegmentsToMove;
   }
@@ -369,8 +371,8 @@ public class CoordinatorDynamicConfig {
     return replicateAfterLoadTimeout;
   }
 
-  @Min(0)
   @JsonProperty
+  @Min(0)
   public int getMaxNonPrimaryReplicantsToLoad() {
     return maxNonPrimaryReplicantsToLoad;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorOverlordServiceConfig.java b/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorOverlordServiceConfig.java
index 3ac2fa25f4..08d5554921 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorOverlordServiceConfig.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorOverlordServiceConfig.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.server.coordinator;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 
 /** */
 public class CoordinatorOverlordServiceConfig {
@@ -34,7 +35,7 @@ public class CoordinatorOverlordServiceConfig {
     this.enabled = enabled == null ? false : enabled.booleanValue();
     this.overlordService = overlordService;
 
-    Preconditions.checkArgument(
+    checkArgument(
         (this.enabled && this.overlordService != null) || !this.enabled,
         "coordinator is enabled to be overlord but overlordService is not specified");
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorStats.java b/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorStats.java
index 14e7bb9e03..8779f5f802 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorStats.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/CoordinatorStats.java
@@ -19,10 +19,12 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptySet;
+import static java.util.Collections.unmodifiableSet;
+
 import it.unimi.dsi.fastutil.objects.Object2LongMap;
 import it.unimi.dsi.fastutil.objects.Object2LongMap.Entry;
 import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
@@ -57,25 +59,25 @@ public class CoordinatorStats {
   public Set<String> getTiers(final String statName) {
     final Object2LongOpenHashMap<String> theStat = perTierStats.get(statName);
     if (theStat == null) {
-      return Collections.emptySet();
+      return emptySet();
     }
-    return Collections.unmodifiableSet(theStat.keySet());
+    return unmodifiableSet(theStat.keySet());
   }
 
   public Set<String> getDataSources(String statName) {
     final Object2LongOpenHashMap<String> stat = perDataSourceStats.get(statName);
     if (stat == null) {
-      return Collections.emptySet();
+      return emptySet();
     }
-    return Collections.unmodifiableSet(stat.keySet());
+    return unmodifiableSet(stat.keySet());
   }
 
   public Set<String> getDuties(String statName) {
     final Object2LongOpenHashMap<String> stat = perDutyStats.get(statName);
     if (stat == null) {
-      return Collections.emptySet();
+      return emptySet();
     }
-    return Collections.unmodifiableSet(stat.keySet());
+    return unmodifiableSet(stat.keySet());
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/CostBalancerStrategy.java b/server/src/main/java/org/apache/druid/server/coordinator/CostBalancerStrategy.java
index 51d4d875c2..674eafd276 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/CostBalancerStrategy.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/CostBalancerStrategy.java
@@ -19,18 +19,19 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyIterator;
+import static java.util.Comparator.comparing;
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.collect.Iterables;
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.common.util.concurrent.ListeningExecutorService;
 import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
 import java.util.Iterator;
 import java.util.List;
 import java.util.NavigableSet;
 import java.util.concurrent.ThreadLocalRandom;
-import java.util.stream.Collectors;
 import org.apache.commons.math3.util.FastMath;
 import org.apache.druid.java.util.common.Pair;
 import org.apache.druid.java.util.emitter.EmittingLogger;
@@ -223,16 +224,16 @@ public class CostBalancerStrategy implements BalancerStrategy {
       return results.stream()
           // Comparator.comapringDouble will order by lowest cost...
           // reverse it because we want to drop from the highest cost servers first
-          .sorted(Comparator.comparingDouble((Pair<Double, ServerHolder> o) -> o.lhs).reversed())
+          .sorted(comparing((Pair<Double, ServerHolder> o) -> o.lhs).reversed())
           .map(x -> x.rhs)
-          .collect(Collectors.toList())
+          .collect(toList())
           .iterator();
     } catch (Exception e) {
       log.makeAlert(
               e, "Cost Balancer Multithread strategy wasn't able to complete cost computation.")
           .emit();
     }
-    return Collections.emptyIterator();
+    return emptyIterator();
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java b/server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java
index 52370e35d4..03648dc616 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java
@@ -19,12 +19,12 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.collect.ImmutableList;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
@@ -167,8 +167,7 @@ public class CuratorLoadQueuePeon extends LoadQueuePeon {
 
   @Override
   public void loadSegment(final DataSegment segment, @Nullable final LoadPeonCallback callback) {
-    SegmentHolder segmentHolder =
-        new SegmentHolder(segment, Action.LOAD, Collections.singletonList(callback));
+    SegmentHolder segmentHolder = new SegmentHolder(segment, Action.LOAD, singletonList(callback));
     final SegmentHolder existingHolder = segmentsToLoad.putIfAbsent(segment, segmentHolder);
     if (existingHolder != null) {
       existingHolder.addCallback(callback);
@@ -181,8 +180,7 @@ public class CuratorLoadQueuePeon extends LoadQueuePeon {
 
   @Override
   public void dropSegment(final DataSegment segment, @Nullable final LoadPeonCallback callback) {
-    SegmentHolder segmentHolder =
-        new SegmentHolder(segment, Action.DROP, Collections.singletonList(callback));
+    SegmentHolder segmentHolder = new SegmentHolder(segment, Action.DROP, singletonList(callback));
     final SegmentHolder existingHolder = segmentsToDrop.putIfAbsent(segment, segmentHolder);
     if (existingHolder != null) {
       existingHolder.addCallback(callback);
@@ -312,7 +310,7 @@ public class CuratorLoadQueuePeon extends LoadQueuePeon {
         // When load failed a segment will be removed from the segmentsToLoad twice and
         // null value will be returned at the second time in which case queueSize may be negative.
         // See https://github.com/apache/druid/pull/10362 for more details.
-        if (null != segmentsToLoad.remove(segmentHolder.getSegment())) {
+        if (segmentsToLoad.remove(segmentHolder.getSegment()) != null) {
           queuedSize.addAndGet(-segmentHolder.getSegmentSize());
           timedOutSegments.remove(segmentHolder.getSegment());
         }
@@ -439,7 +437,7 @@ public class CuratorLoadQueuePeon extends LoadQueuePeon {
       synchronized (callbacks) {
         // Return an immutable copy so that callers don't have to worry about concurrent
         // modification
-        return ImmutableList.copyOf(callbacks);
+        return callbacks;
       }
     }
 
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/DataSourceCompactionConfig.java b/server/src/main/java/org/apache/druid/server/coordinator/DataSourceCompactionConfig.java
index 04c470fece..1beaee51f5 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/DataSourceCompactionConfig.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/DataSourceCompactionConfig.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.Arrays;
 import java.util.Map;
 import java.util.Objects;
@@ -70,7 +71,7 @@ public class DataSourceCompactionConfig {
       @JsonProperty("transformSpec") @Nullable UserCompactionTaskTransformConfig transformSpec,
       @JsonProperty("ioConfig") @Nullable UserCompactionTaskIOConfig ioConfig,
       @JsonProperty("taskContext") @Nullable Map<String, Object> taskContext) {
-    this.dataSource = Preconditions.checkNotNull(dataSource, "dataSource");
+    this.dataSource = requireNonNull(dataSource, "dataSource");
     this.taskPriority = taskPriority == null ? DEFAULT_COMPACTION_TASK_PRIORITY : taskPriority;
     this.inputSegmentSizeBytes =
         inputSegmentSizeBytes == null ? DEFAULT_INPUT_SEGMENT_SIZE_BYTES : inputSegmentSizeBytes;
@@ -193,7 +194,6 @@ public class DataSourceCompactionConfig {
             transformSpec,
             ioConfig,
             taskContext);
-    result = 31 * result + Arrays.hashCode(metricsSpec);
-    return result;
+    return 31 * result + Arrays.hashCode(metricsSpec);
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/DruidCluster.java b/server/src/main/java/org/apache/druid/server/coordinator/DruidCluster.java
index f436e61eb6..fb4facb0ef 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/DruidCluster.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/DruidCluster.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Comparator.reverseOrder;
+
 import com.google.common.annotations.VisibleForTesting;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -101,8 +102,7 @@ public class DruidCluster {
   private void addHistorical(ServerHolder serverHolder) {
     final ImmutableDruidServer server = serverHolder.getServer();
     final NavigableSet<ServerHolder> tierServers =
-        historicals.computeIfAbsent(
-            server.getTier(), k -> new TreeSet<>(Collections.reverseOrder()));
+        historicals.computeIfAbsent(server.getTier(), k -> new TreeSet<>(reverseOrder()));
     tierServers.add(serverHolder);
   }
 
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java b/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java
index 9892279c5e..c8ad0082df 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Ordering;
@@ -43,7 +45,6 @@ import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.TimeUnit;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.client.DataSourcesSnapshot;
 import org.apache.druid.client.DruidDataSource;
@@ -344,9 +345,7 @@ public class DruidCoordinator {
           // This does not use segments.removeAll(loadedView.getSegments()) for performance reasons.
           // Please see https://github.com/apache/druid/pull/5632 and LoadStatusBenchmark for more
           // info.
-          for (DataSegment serverSegment : loadedView.getSegments()) {
-            segments.remove(serverSegment);
-          }
+          segments.removeAll(loadedView.getSegments());
         }
       }
       final int numUnavailableSegments = segments.size();
@@ -669,7 +668,7 @@ public class DruidCoordinator {
             "Done making custom coordinator duties %s for group %s",
             customDutyGroup.getCustomDutyList().stream()
                 .map(duty -> duty.getClass().getName())
-                .collect(Collectors.toList()),
+                .collect(toList()),
             customDutyGroup.getName());
       }
 
@@ -749,8 +748,8 @@ public class DruidCoordinator {
     }
     log.debug(
         "Done making indexing service duties %s",
-        duties.stream().map(duty -> duty.getClass().getName()).collect(Collectors.toList()));
-    return ImmutableList.copyOf(duties);
+        duties.stream().map(duty -> duty.getClass().getName()).collect(toList()));
+    return duties;
   }
 
   private List<CoordinatorDuty> makeMetadataStoreManagementDuties() {
@@ -759,8 +758,8 @@ public class DruidCoordinator {
 
     log.debug(
         "Done making metadata store management duties %s",
-        duties.stream().map(duty -> duty.getClass().getName()).collect(Collectors.toList()));
-    return ImmutableList.copyOf(duties);
+        duties.stream().map(duty -> duty.getClass().getName()).collect(toList()));
+    return duties;
   }
 
   @VisibleForTesting
@@ -784,9 +783,9 @@ public class DruidCoordinator {
     return customDutyGroups.getCoordinatorCustomDutyGroups().stream()
         .flatMap(
             coordinatorCustomDutyGroup -> coordinatorCustomDutyGroup.getCustomDutyList().stream())
-        .filter(duty -> duty instanceof CompactSegments)
+        .filter(CompactSegments.class::isInstance)
         .map(duty -> (CompactSegments) duty)
-        .collect(Collectors.toList());
+        .collect(toList());
   }
 
   private List<CoordinatorDuty> makeCompactSegmentsDuty() {
@@ -807,9 +806,9 @@ public class DruidCoordinator {
       // This is to avoid human coding error (forgetting to add the EmitClusterStatsAndMetrics duty
       // to the group)
       // causing metrics from the duties to not being emitted.
-      if (duties.stream().noneMatch(duty -> duty instanceof EmitClusterStatsAndMetrics)) {
+      if (duties.stream().noneMatch(EmitClusterStatsAndMetrics.class::isInstance)) {
         boolean isContainCompactSegmentDuty =
-            duties.stream().anyMatch(duty -> duty instanceof CompactSegments);
+            duties.stream().anyMatch(CompactSegments.class::isInstance);
         List<CoordinatorDuty> allDuties = new ArrayList<>(duties);
         allDuties.add(
             new EmitClusterStatsAndMetrics(
@@ -987,7 +986,7 @@ public class DruidCoordinator {
           serverInventoryView.getInventory().stream()
               .filter(DruidServer::isSegmentReplicationOrBroadcastTarget)
               .map(DruidServer::toImmutableDruidServer)
-              .collect(Collectors.toList());
+              .collect(toList());
 
       if (log.isDebugEnabled()) {
         // Display info about all segment-replicatable (historical and bridge) servers
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinatorRuntimeParams.java b/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinatorRuntimeParams.java
index 1a645bccda..1fbd52a5f5 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinatorRuntimeParams.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinatorRuntimeParams.java
@@ -19,8 +19,10 @@
 
 package org.apache.druid.server.coordinator;
 
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import java.util.Arrays;
 import java.util.Collection;
@@ -124,14 +126,12 @@ public class DruidCoordinatorRuntimeParams {
    * map with "used" segments.
    */
   public Map<String, SegmentTimeline> getUsedSegmentsTimelinesPerDataSource() {
-    Preconditions.checkState(
-        dataSourcesSnapshot != null, "dataSourcesSnapshot or usedSegments must be set");
+    checkState(dataSourcesSnapshot != null, "dataSourcesSnapshot or usedSegments must be set");
     return dataSourcesSnapshot.getUsedSegmentsTimelinesPerDataSource();
   }
 
   public TreeSet<DataSegment> getUsedSegments() {
-    Preconditions.checkState(
-        usedSegments != null, "usedSegments or dataSourcesSnapshot must be set");
+    checkState(usedSegments != null, "usedSegments or dataSourcesSnapshot must be set");
     return usedSegments;
   }
 
@@ -182,8 +182,7 @@ public class DruidCoordinatorRuntimeParams {
   }
 
   public DataSourcesSnapshot getDataSourcesSnapshot() {
-    Preconditions.checkState(
-        dataSourcesSnapshot != null, "usedSegments or dataSourcesSnapshot must be set");
+    checkState(dataSourcesSnapshot != null, "usedSegments or dataSourcesSnapshot must be set");
     return dataSourcesSnapshot;
   }
 
@@ -297,7 +296,7 @@ public class DruidCoordinatorRuntimeParams {
     }
 
     public DruidCoordinatorRuntimeParams build() {
-      Preconditions.checkNotNull(startTimeNanos, "startTime must be set");
+      requireNonNull(startTimeNanos, "startTime must be set");
       return new DruidCoordinatorRuntimeParams(
           startTimeNanos,
           druidCluster,
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/HttpLoadQueuePeon.java b/server/src/main/java/org/apache/druid/server/coordinator/HttpLoadQueuePeon.java
index 57b9d4293e..7059ed6c55 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/HttpLoadQueuePeon.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/HttpLoadQueuePeon.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptySet;
+import static java.util.Collections.unmodifiableSet;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.ObjectWriter;
@@ -30,7 +33,6 @@ import java.io.InputStream;
 import java.net.MalformedURLException;
 import java.net.URL;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -154,7 +156,7 @@ public class HttpLoadQueuePeon extends LoadQueuePeon {
       }
     }
 
-    if (newRequests.size() == 0) {
+    if (newRequests.isEmpty()) {
       log.trace(
           "[%s]Found no load/drop requests. SegmentsToLoad[%d], SegmentsToDrop[%d], batchSize[%d].",
           serverId,
@@ -385,17 +387,17 @@ public class HttpLoadQueuePeon extends LoadQueuePeon {
 
   @Override
   public Set<DataSegment> getSegmentsToLoad() {
-    return Collections.unmodifiableSet(segmentsToLoad.keySet());
+    return unmodifiableSet(segmentsToLoad.keySet());
   }
 
   @Override
   public Set<DataSegment> getSegmentsToDrop() {
-    return Collections.unmodifiableSet(segmentsToDrop.keySet());
+    return unmodifiableSet(segmentsToDrop.keySet());
   }
 
   @Override
   public Set<DataSegment> getTimedOutSegments() {
-    return Collections.emptySet();
+    return emptySet();
   }
 
   @Override
@@ -425,7 +427,7 @@ public class HttpLoadQueuePeon extends LoadQueuePeon {
 
   @Override
   public Set<DataSegment> getSegmentsMarkedToDrop() {
-    return Collections.unmodifiableSet(segmentsMarkedToDrop);
+    return unmodifiableSet(segmentsMarkedToDrop);
   }
 
   private abstract class SegmentHolder {
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/KillStalePendingSegments.java b/server/src/main/java/org/apache/druid/server/coordinator/KillStalePendingSegments.java
index e8fa07d05a..0cb5379bbd 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/KillStalePendingSegments.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/KillStalePendingSegments.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.coordinator;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkState;
+
 import com.google.inject.Inject;
 import java.util.ArrayList;
 import java.util.List;
@@ -63,7 +64,7 @@ public class KillStalePendingSegments implements CoordinatorDuty {
     // There should be at least one createdTime because the current time is added to the
     // 'createdTimes' list if there
     // is no running/pending/waiting tasks.
-    Preconditions.checkState(!createdTimes.isEmpty(), "Failed to gather createdTimes of tasks");
+    checkState(!createdTimes.isEmpty(), "Failed to gather createdTimes of tasks");
 
     // If there is no running/pending/waiting/complete tasks, stalePendingSegmentsCutoffCreationTime
     // is
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/RandomBalancerStrategy.java b/server/src/main/java/org/apache/druid/server/coordinator/RandomBalancerStrategy.java
index 19bcdeabec..e719b1c609 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/RandomBalancerStrategy.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/RandomBalancerStrategy.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.shuffle;
+import static java.util.stream.Collectors.toList;
+
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.NavigableSet;
 import java.util.concurrent.ThreadLocalRandom;
-import java.util.stream.Collectors;
 import org.apache.druid.timeline.DataSegment;
 
 public class RandomBalancerStrategy implements BalancerStrategy {
@@ -40,8 +41,8 @@ public class RandomBalancerStrategy implements BalancerStrategy {
                 serverHolder ->
                     serverHolder.getAvailableSize() >= proposalSegment.getSize()
                         && !serverHolder.isServingSegment(proposalSegment))
-            .collect(Collectors.toList());
-    if (usableServerHolders.size() == 0) {
+            .collect(toList());
+    if (usableServerHolders.isEmpty()) {
       return null;
     } else {
       return usableServerHolders.get(
@@ -59,7 +60,7 @@ public class RandomBalancerStrategy implements BalancerStrategy {
   public Iterator<ServerHolder> pickServersToDrop(
       DataSegment toDropSegment, NavigableSet<ServerHolder> serverHolders) {
     List<ServerHolder> serverList = new ArrayList<>(serverHolders);
-    Collections.shuffle(serverList);
+    shuffle(serverList);
     return serverList.iterator();
   }
 
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/RoundRobinServerSelector.java b/server/src/main/java/org/apache/druid/server/coordinator/RoundRobinServerSelector.java
index e177c04529..5439ad08f7 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/RoundRobinServerSelector.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/RoundRobinServerSelector.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyIterator;
+
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -56,7 +57,7 @@ public class RoundRobinServerSelector {
   public Iterator<ServerHolder> getServersInTierToLoadSegment(String tier, DataSegment segment) {
     final CircularServerList iterator = tierToServers.get(tier);
     if (iterator == null) {
-      return Collections.emptyIterator();
+      return emptyIterator();
     }
 
     return new EligibleServerIterator(segment, iterator);
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/ServerHolder.java b/server/src/main/java/org/apache/druid/server/coordinator/ServerHolder.java
index 6ac4647845..e1843a144c 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/ServerHolder.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/ServerHolder.java
@@ -190,7 +190,7 @@ public class ServerHolder implements Comparable<ServerHolder> {
       return false;
     }
 
-    return this.server.getType().equals(that.getServer().getType());
+    return this.server.getType() == that.getServer().getType();
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskDimensionsConfig.java b/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskDimensionsConfig.java
index c3c47275c9..34c6c557a6 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskDimensionsConfig.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskDimensionsConfig.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import java.util.List;
 import java.util.Objects;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.data.input.impl.DimensionSchema;
 import org.apache.druid.java.util.common.parsers.ParserUtils;
@@ -43,14 +44,14 @@ public class UserCompactionTaskDimensionsConfig {
       @Nullable @JsonProperty("dimensions") List<DimensionSchema> dimensions) {
     if (dimensions != null) {
       List<String> dimensionNames =
-          dimensions.stream().map(DimensionSchema::getName).collect(Collectors.toList());
+          dimensions.stream().map(DimensionSchema::getName).collect(toList());
       ParserUtils.validateFields(dimensionNames);
     }
     this.dimensions = dimensions;
   }
 
-  @Nullable
   @JsonProperty
+  @Nullable
   public List<DimensionSchema> getDimensions() {
     return dimensions;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskQueryTuningConfig.java b/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskQueryTuningConfig.java
index 9ef10fc558..57ba4c8bb9 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskQueryTuningConfig.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/UserCompactionTaskQueryTuningConfig.java
@@ -78,9 +78,9 @@ public class UserCompactionTaskQueryTuningConfig extends ClientCompactionTaskQue
         maxColumnsToMerge);
   }
 
-  @Override
-  @Nullable
   @JsonIgnore
+  @Nullable
+  @Override
   public Integer getMaxRowsPerSegment() {
     throw new UnsupportedOperationException();
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/cost/ClusterCostCache.java b/server/src/main/java/org/apache/druid/server/coordinator/cost/ClusterCostCache.java
index c5378ed2ab..a0341e51d3 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/cost/ClusterCostCache.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/cost/ClusterCostCache.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.coordinator.cost;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
@@ -30,7 +31,7 @@ public class ClusterCostCache {
   private final Map<String, ServerCostCache> serversCostCache;
 
   ClusterCostCache(Map<String, ServerCostCache> serversCostCache) {
-    this.serversCostCache = Preconditions.checkNotNull(serversCostCache);
+    this.serversCostCache = requireNonNull(serversCostCache);
   }
 
   public double computeCost(String serverName, DataSegment dataSegment) {
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/cost/SegmentsCostCache.java b/server/src/main/java/org/apache/druid/server/coordinator/cost/SegmentsCostCache.java
index 6625a2015a..74ace45d40 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/cost/SegmentsCostCache.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/cost/SegmentsCostCache.java
@@ -19,10 +19,14 @@
 
 package org.apache.druid.server.coordinator.cost;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Collections.binarySearch;
+import static java.util.Comparator.comparing;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.toCollection;
+
 import com.google.common.collect.Ordering;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Comparator;
 import java.util.ListIterator;
 import java.util.NavigableMap;
@@ -30,7 +34,6 @@ import java.util.NavigableSet;
 import java.util.TreeMap;
 import java.util.TreeSet;
 import java.util.concurrent.TimeUnit;
-import java.util.stream.Collectors;
 import org.apache.commons.math3.util.FastMath;
 import org.apache.druid.java.util.common.ISE;
 import org.apache.druid.java.util.common.Intervals;
@@ -111,10 +114,10 @@ public class SegmentsCostCache {
       new DurationGranularity(BUCKET_INTERVAL, 0);
 
   private static final Comparator<DataSegment> SEGMENT_INTERVAL_COMPARATOR =
-      Comparator.comparing(DataSegment::getInterval, Comparators.intervalsByStartThenEnd());
+      comparing(DataSegment::getInterval, Comparators.intervalsByStartThenEnd());
 
   private static final Comparator<Bucket> BUCKET_INTERVAL_COMPARATOR =
-      Comparator.comparing(Bucket::getInterval, Comparators.intervalsByStartThenEnd());
+      comparing(Bucket::getInterval, Comparators.intervalsByStartThenEnd());
 
   private static final Ordering<DataSegment> SEGMENT_ORDERING =
       Ordering.from(SEGMENT_INTERVAL_COMPARATOR);
@@ -124,20 +127,16 @@ public class SegmentsCostCache {
   private final ArrayList<Interval> intervals;
 
   SegmentsCostCache(ArrayList<Bucket> sortedBuckets) {
-    this.sortedBuckets = Preconditions.checkNotNull(sortedBuckets, "buckets should not be null");
+    this.sortedBuckets = requireNonNull(sortedBuckets, "buckets should not be null");
     this.intervals =
-        sortedBuckets.stream()
-            .map(Bucket::getInterval)
-            .collect(Collectors.toCollection(ArrayList::new));
-    Preconditions.checkArgument(
-        BUCKET_ORDERING.isOrdered(sortedBuckets), "buckets must be ordered by interval");
+        sortedBuckets.stream().map(Bucket::getInterval).collect(toCollection(ArrayList::new));
+    checkArgument(BUCKET_ORDERING.isOrdered(sortedBuckets), "buckets must be ordered by interval");
   }
 
   public double cost(DataSegment segment) {
     double cost = 0.0;
     int index =
-        Collections.binarySearch(
-            intervals, segment.getInterval(), Comparators.intervalsByStartThenEnd());
+        binarySearch(intervals, segment.getInterval(), Comparators.intervalsByStartThenEnd());
     index = (index >= 0) ? index : -index - 1;
 
     for (ListIterator<Bucket> it = sortedBuckets.listIterator(index); it.hasNext(); ) {
@@ -192,7 +191,7 @@ public class SegmentsCostCache {
       return new SegmentsCostCache(
           buckets.entrySet().stream()
               .map(entry -> entry.getValue().build())
-              .collect(Collectors.toCollection(ArrayList::new)));
+              .collect(toCollection(ArrayList::new)));
     }
 
     private static Interval getBucketInterval(DataSegment segment) {
@@ -212,13 +211,13 @@ public class SegmentsCostCache {
         ArrayList<DataSegment> sortedSegments,
         double[] leftSum,
         double[] rightSum) {
-      this.interval = Preconditions.checkNotNull(interval, "interval");
-      this.sortedSegments = Preconditions.checkNotNull(sortedSegments, "sortedSegments");
-      this.leftSum = Preconditions.checkNotNull(leftSum, "leftSum");
-      this.rightSum = Preconditions.checkNotNull(rightSum, "rightSum");
-      Preconditions.checkArgument(
+      this.interval = requireNonNull(interval, "interval");
+      this.sortedSegments = requireNonNull(sortedSegments, "sortedSegments");
+      this.leftSum = requireNonNull(leftSum, "leftSum");
+      this.rightSum = requireNonNull(rightSum, "rightSum");
+      checkArgument(
           sortedSegments.size() == leftSum.length && sortedSegments.size() == rightSum.length);
-      Preconditions.checkArgument(SEGMENT_ORDERING.isOrdered(sortedSegments));
+      checkArgument(SEGMENT_ORDERING.isOrdered(sortedSegments));
       this.calculationInterval =
           new Interval(
               interval.getStart().minus(LIFE_THRESHOLD), interval.getEnd().plus(LIFE_THRESHOLD));
@@ -242,8 +241,7 @@ public class SegmentsCostCache {
         throw new ISE("Segment is not within calculation interval");
       }
 
-      int index =
-          Collections.binarySearch(sortedSegments, dataSegment, SEGMENT_INTERVAL_COMPARATOR);
+      int index = binarySearch(sortedSegments, dataSegment, SEGMENT_INTERVAL_COMPARATOR);
       index = (index >= 0) ? index : -index - 1;
       return addLeftCost(dataSegment, t0, t1, index) + rightCost(dataSegment, t0, t1, index);
     }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/cost/ServerCostCache.java b/server/src/main/java/org/apache/druid/server/coordinator/cost/ServerCostCache.java
index 7b64aa15eb..82ed500219 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/cost/ServerCostCache.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/cost/ServerCostCache.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.coordinator.cost;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import java.util.HashMap;
 import java.util.Map;
 import org.apache.druid.timeline.DataSegment;
@@ -32,8 +33,8 @@ public class ServerCostCache {
   ServerCostCache(
       SegmentsCostCache allSegmentsCostCache,
       Map<String, SegmentsCostCache> segmentsCostPerDataSource) {
-    this.allSegmentsCostCache = Preconditions.checkNotNull(allSegmentsCostCache);
-    this.segmentsPerDataSource = Preconditions.checkNotNull(segmentsCostPerDataSource);
+    this.allSegmentsCostCache = requireNonNull(allSegmentsCostCache);
+    this.segmentsPerDataSource = requireNonNull(segmentsCostPerDataSource);
   }
 
   double computeCost(DataSegment segment) {
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/BalanceSegments.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/BalanceSegments.java
index ae803de32a..574148fc70 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/BalanceSegments.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/BalanceSegments.java
@@ -19,7 +19,10 @@
 
 package org.apache.druid.server.coordinator.duty;
 
-import com.google.common.collect.Lists;
+import static java.util.stream.Collectors.partitioningBy;
+import static java.util.stream.Collectors.toList;
+
+import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -28,7 +31,6 @@ import java.util.NavigableSet;
 import java.util.SortedSet;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
-import java.util.stream.Collectors;
 import org.apache.druid.client.ImmutableDruidServer;
 import org.apache.druid.java.util.common.Pair;
 import org.apache.druid.java.util.emitter.EmittingLogger;
@@ -88,7 +90,7 @@ public class BalanceSegments implements CoordinatorDuty {
       CoordinatorStats stats) {
 
     log.info("Balancing segments in tier [%s]", tier);
-    if (params.getUsedSegments().size() == 0) {
+    if (params.getUsedSegments().isEmpty()) {
       log.info("Metadata segments are not available. Cannot balance.");
       // suppress emit zero stats
       return;
@@ -109,7 +111,7 @@ public class BalanceSegments implements CoordinatorDuty {
      the best location for them on active servers. After that, balance segments within active servers pool.
     */
     Map<Boolean, List<ServerHolder>> partitions =
-        servers.stream().collect(Collectors.partitioningBy(ServerHolder::isDecommissioning));
+        servers.stream().collect(partitioningBy(ServerHolder::isDecommissioning));
     final List<ServerHolder> decommissioningServers = partitions.get(true);
     final List<ServerHolder> activeServers = partitions.get(false);
     log.info(
@@ -171,7 +173,7 @@ public class BalanceSegments implements CoordinatorDuty {
 
     if (params.getCoordinatorDynamicConfig().emitBalancingStats()) {
       final BalancerStrategy strategy = params.getBalancerStrategy();
-      strategy.emitStats(tier, stats, Lists.newArrayList(servers));
+      strategy.emitStats(tier, stats, new ArrayList<>(servers));
     }
     log.info("[%s]: Segments Moved: [%d] Segments Let Alone: [%d]", tier, moved, unmoved);
   }
@@ -244,9 +246,9 @@ public class BalanceSegments implements CoordinatorDuty {
                         s.getServer().equals(fromServer)
                             || (!s.isServingSegment(segmentToMove)
                                 && (maxToLoad <= 0 || s.getNumberOfSegmentsInQueue() < maxToLoad)))
-                .collect(Collectors.toList());
+                .collect(toList());
 
-        if (toMoveToWithLoadQueueCapacityAndNotServingSegment.size() > 0) {
+        if (!toMoveToWithLoadQueueCapacityAndNotServingSegment.isEmpty()) {
           final ServerHolder destinationHolder =
               strategy.findNewSegmentHomeBalancer(
                   segmentToMove, toMoveToWithLoadQueueCapacityAndNotServingSegment);
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java
index 7edb3a51c5..4cbe8aef37 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toMap;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.google.common.annotations.VisibleForTesting;
@@ -28,8 +32,6 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicReference;
-import java.util.function.Function;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.client.indexing.ClientCompactionTaskDimensionsSpec;
 import org.apache.druid.client.indexing.ClientCompactionTaskGranularitySpec;
@@ -124,9 +126,7 @@ public class CompactSegments implements CoordinatorCustomDuty {
       if (compactionConfigList != null && !compactionConfigList.isEmpty()) {
         Map<String, DataSourceCompactionConfig> compactionConfigs =
             compactionConfigList.stream()
-                .collect(
-                    Collectors.toMap(
-                        DataSourceCompactionConfig::getDataSource, Function.identity()));
+                .collect(toMap(DataSourceCompactionConfig::getDataSource, identity()));
         final List<TaskStatusPlus> compactionTasks =
             filterNonCompactionTasks(indexingServiceClient.getActiveTasks());
 
@@ -275,7 +275,7 @@ public class CompactSegments implements CoordinatorCustomDuty {
     final Map<String, Integer> minTaskPriority =
         compactionConfigs.stream()
             .collect(
-                Collectors.toMap(
+                toMap(
                     DataSourceCompactionConfig::getDataSource,
                     DataSourceCompactionConfig::getTaskPriority));
     final Map<String, List<Interval>> datasourceToLockedIntervals =
@@ -311,7 +311,7 @@ public class CompactSegments implements CoordinatorCustomDuty {
    */
   @VisibleForTesting
   static boolean isParallelMode(@Nullable ClientCompactionTaskQueryTuningConfig tuningConfig) {
-    if (null == tuningConfig) {
+    if (tuningConfig == null) {
       return false;
     }
     boolean useRangePartitions = useRangePartitions(tuningConfig);
@@ -339,7 +339,7 @@ public class CompactSegments implements CoordinatorCustomDuty {
               // performance.
               return taskType == null || COMPACTION_TASK_TYPE.equals(taskType);
             })
-        .collect(Collectors.toList());
+        .collect(toList());
   }
 
   private CoordinatorStats doRun(
@@ -446,7 +446,7 @@ public class CompactSegments implements CoordinatorCustomDuty {
         // This forcing code should be revised
         // when/if the autocompaction code policy to decide which segments to compact changes
         if (dropExisting == null || !dropExisting) {
-          if (segmentsToCompact.stream().allMatch(dataSegment -> dataSegment.isTombstone())) {
+          if (segmentsToCompact.stream().allMatch(DataSegment::isTombstone)) {
             dropExisting = true;
             LOG.info(
                 "Forcing dropExisting to %s since all segments to compact are tombstones",
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/CoordinatorCustomDuty.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/CoordinatorCustomDuty.java
index 2c798494c8..d40630406a 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/CoordinatorCustomDuty.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/CoordinatorCustomDuty.java
@@ -51,8 +51,8 @@ import org.apache.druid.initialization.DruidModule;
  */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
 @JsonSubTypes({
-  @JsonSubTypes.Type(name = "killSupervisors", value = KillSupervisorsCustomDuty.class),
   @JsonSubTypes.Type(name = "compactSegments", value = CompactSegments.class),
+  @JsonSubTypes.Type(name = "killSupervisors", value = KillSupervisorsCustomDuty.class)
 })
 @ExtensionPoint
 public interface CoordinatorCustomDuty extends CoordinatorDuty {}
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillAuditLog.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillAuditLog.java
index ead6dc6d27..80f793ac7d 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillAuditLog.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillAuditLog.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.coordinator.duty;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.google.inject.Inject;
 import org.apache.druid.audit.AuditManager;
 import org.apache.druid.java.util.common.logger.Logger;
@@ -40,13 +41,12 @@ public class KillAuditLog implements CoordinatorDuty {
   @Inject
   public KillAuditLog(AuditManager auditManager, DruidCoordinatorConfig config) {
     this.period = config.getCoordinatorAuditKillPeriod().getMillis();
-    Preconditions.checkArgument(
+    checkArgument(
         this.period >= config.getCoordinatorMetadataStoreManagementPeriod().getMillis(),
         "coordinator audit kill period must be >= druid.coordinator.period.metadataStoreManagementPeriod");
     this.retainDuration = config.getCoordinatorAuditKillDurationToRetain().getMillis();
-    Preconditions.checkArgument(
-        this.retainDuration >= 0, "coordinator audit kill retainDuration must be >= 0");
-    Preconditions.checkArgument(
+    checkArgument(this.retainDuration >= 0, "coordinator audit kill retainDuration must be >= 0");
+    checkArgument(
         this.retainDuration < System.currentTimeMillis(),
         "Coordinator audit kill retainDuration cannot be greater than current time in ms");
     log.debug(
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillCompactionConfig.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillCompactionConfig.java
index f0bf9dcb94..91be255c2e 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillCompactionConfig.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillCompactionConfig.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.server.coordinator.duty;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toMap;
+
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Inject;
 import java.util.Map;
 import java.util.Set;
-import java.util.function.Function;
-import java.util.stream.Collectors;
 import org.apache.druid.audit.AuditInfo;
 import org.apache.druid.common.config.ConfigManager;
 import org.apache.druid.common.config.JacksonConfigManager;
@@ -73,7 +74,7 @@ public class KillCompactionConfig implements CoordinatorDuty {
     this.period = config.getCoordinatorCompactionKillPeriod().getMillis();
     this.connector = connector;
     this.connectorConfig = connectorConfig;
-    Preconditions.checkArgument(
+    checkArgument(
         this.period >= config.getCoordinatorMetadataStoreManagementPeriod().getMillis(),
         "Coordinator compaction configuration kill period must be >= druid.coordinator.period.metadataStoreManagementPeriod");
     log.debug(
@@ -113,9 +114,7 @@ public class KillCompactionConfig implements CoordinatorDuty {
                           dataSourceCompactionConfig ->
                               activeDatasources.contains(
                                   dataSourceCompactionConfig.getDataSource()))
-                      .collect(
-                          Collectors.toMap(
-                              DataSourceCompactionConfig::getDataSource, Function.identity()));
+                      .collect(toMap(DataSourceCompactionConfig::getDataSource, identity()));
 
               // Calculate number of compaction configs to remove for logging
               int compactionConfigRemoved = current.getCompactionConfigs().size() - updated.size();
@@ -146,7 +145,7 @@ public class KillCompactionConfig implements CoordinatorDuty {
               }
               return result;
             },
-            e -> e instanceof RetryableException,
+            RetryableException.class::isInstance,
             UPDATE_NUM_RETRY);
       } catch (Exception e) {
         log.error(e, "Failed to kill compaction configurations");
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillDatasourceMetadata.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillDatasourceMetadata.java
index 73fa126ec0..d55d33d607 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillDatasourceMetadata.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillDatasourceMetadata.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.server.coordinator.duty;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.stream.Collectors.toSet;
+
 import com.google.common.base.Strings;
 import com.google.inject.Inject;
 import java.util.Collection;
 import java.util.Map;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.indexing.overlord.IndexerMetadataStorageCoordinator;
 import org.apache.druid.indexing.overlord.supervisor.SupervisorSpec;
 import org.apache.druid.java.util.common.logger.Logger;
@@ -59,14 +60,14 @@ public class KillDatasourceMetadata implements CoordinatorDuty {
     this.indexerMetadataStorageCoordinator = indexerMetadataStorageCoordinator;
     this.metadataSupervisorManager = metadataSupervisorManager;
     this.period = config.getCoordinatorDatasourceKillPeriod().getMillis();
-    Preconditions.checkArgument(
+    checkArgument(
         this.period >= config.getCoordinatorMetadataStoreManagementPeriod().getMillis(),
         "Coordinator datasource metadata kill period must be >= druid.coordinator.period.metadataStoreManagementPeriod");
     this.retainDuration = config.getCoordinatorDatasourceKillDurationToRetain().getMillis();
-    Preconditions.checkArgument(
+    checkArgument(
         this.retainDuration >= 0,
         "Coordinator datasource metadata kill retainDuration must be >= 0");
-    Preconditions.checkArgument(
+    checkArgument(
         this.retainDuration < System.currentTimeMillis(),
         "Coordinator datasource metadata kill retainDuration cannot be greater than current time in ms");
     log.debug(
@@ -89,10 +90,10 @@ public class KillDatasourceMetadata implements CoordinatorDuty {
             metadataSupervisorManager.getLatestActiveOnly();
         Set<String> allDatasourceWithActiveSupervisor =
             allActiveSupervisor.values().stream()
-                .map(supervisorSpec -> supervisorSpec.getDataSources())
+                .map(SupervisorSpec::getDataSources)
                 .flatMap(Collection::stream)
                 .filter(datasource -> !Strings.isNullOrEmpty(datasource))
-                .collect(Collectors.toSet());
+                .collect(toSet());
         // We exclude removing datasource metadata with active supervisor
         int datasourceMetadataRemovedCount =
             indexerMetadataStorageCoordinator.removeDataSourceMetadataOlderThan(
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillRules.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillRules.java
index d7fe70a4ab..a4e8ea4501 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillRules.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillRules.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.coordinator.duty;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.google.inject.Inject;
 import org.apache.druid.java.util.common.logger.Logger;
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
@@ -37,13 +38,12 @@ public class KillRules implements CoordinatorDuty {
   @Inject
   public KillRules(DruidCoordinatorConfig config) {
     this.period = config.getCoordinatorRuleKillPeriod().getMillis();
-    Preconditions.checkArgument(
+    checkArgument(
         this.period >= config.getCoordinatorMetadataStoreManagementPeriod().getMillis(),
         "coordinator rule kill period must be >= druid.coordinator.period.metadataStoreManagementPeriod");
     this.retainDuration = config.getCoordinatorRuleKillDurationToRetain().getMillis();
-    Preconditions.checkArgument(
-        this.retainDuration >= 0, "coordinator rule kill retainDuration must be >= 0");
-    Preconditions.checkArgument(
+    checkArgument(this.retainDuration >= 0, "coordinator rule kill retainDuration must be >= 0");
+    checkArgument(
         this.retainDuration < System.currentTimeMillis(),
         "Coordinator rule kill retainDuration cannot be greater than current time in ms");
     log.debug(
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillSupervisors.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillSupervisors.java
index 794392e995..99ad9ca577 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillSupervisors.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillSupervisors.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.coordinator.duty;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.google.inject.Inject;
 import org.apache.druid.java.util.common.logger.Logger;
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
@@ -46,13 +47,13 @@ public class KillSupervisors implements CoordinatorDuty {
       DruidCoordinatorConfig config, MetadataSupervisorManager metadataSupervisorManager) {
     this.metadataSupervisorManager = metadataSupervisorManager;
     this.period = config.getCoordinatorSupervisorKillPeriod().getMillis();
-    Preconditions.checkArgument(
+    checkArgument(
         this.period >= config.getCoordinatorMetadataStoreManagementPeriod().getMillis(),
         "Coordinator supervisor kill period must be >= druid.coordinator.period.metadataStoreManagementPeriod");
     this.retainDuration = config.getCoordinatorSupervisorKillDurationToRetain().getMillis();
-    Preconditions.checkArgument(
+    checkArgument(
         this.retainDuration >= 0, "Coordinator supervisor kill retainDuration must be >= 0");
-    Preconditions.checkArgument(
+    checkArgument(
         this.retainDuration < System.currentTimeMillis(),
         "Coordinator supervisor kill retainDuration cannot be greater than current time in ms");
     log.debug(
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillSupervisorsCustomDuty.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillSupervisorsCustomDuty.java
index 84b9dca606..88a26c6f7c 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillSupervisorsCustomDuty.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillSupervisorsCustomDuty.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JacksonInject;
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import org.apache.druid.java.util.common.logger.Logger;
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
 import org.apache.druid.java.util.emitter.service.ServiceMetricEvent;
@@ -53,7 +54,7 @@ public class KillSupervisorsCustomDuty implements CoordinatorCustomDuty {
       @JacksonInject MetadataSupervisorManager metadataSupervisorManager) {
     this.metadataSupervisorManager = metadataSupervisorManager;
     this.retainDuration = retainDuration;
-    Preconditions.checkArgument(
+    checkArgument(
         this.retainDuration != null && this.retainDuration.getMillis() >= 0,
         "(Custom Duty) Coordinator supervisor kill retainDuration must be >= 0");
     log.info(
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillUnusedSegments.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillUnusedSegments.java
index 0ed8d77874..4c470b28c2 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/KillUnusedSegments.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/KillUnusedSegments.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.coordinator.duty;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.google.inject.Inject;
 import java.util.Collection;
 import java.util.List;
@@ -61,7 +62,7 @@ public class KillUnusedSegments implements CoordinatorDuty {
       IndexingServiceClient indexingServiceClient,
       DruidCoordinatorConfig config) {
     this.period = config.getCoordinatorKillPeriod().getMillis();
-    Preconditions.checkArgument(
+    checkArgument(
         this.period > config.getCoordinatorIndexingPeriod().getMillis(),
         "coordinator kill period must be greater than druid.coordinator.period.indexingPeriod");
 
@@ -75,8 +76,7 @@ public class KillUnusedSegments implements CoordinatorDuty {
     }
 
     this.maxSegmentsToKill = config.getCoordinatorKillMaxSegments();
-    Preconditions.checkArgument(
-        this.maxSegmentsToKill > 0, "coordinator kill maxSegments must be > 0");
+    checkArgument(this.maxSegmentsToKill > 0, "coordinator kill maxSegments must be > 0");
 
     log.info(
         "Kill Task scheduling enabled with period [%s], retainDuration [%s], maxSegmentsToKill [%s]",
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java
index 92c1e3ee2c..69b15aa1d0 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java
@@ -19,15 +19,18 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Collections.emptyList;
+import static java.util.Comparator.naturalOrder;
+import static java.util.Objects.requireNonNull;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
-import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
@@ -37,7 +40,6 @@ import java.util.NoSuchElementException;
 import java.util.Objects;
 import java.util.PriorityQueue;
 import java.util.Set;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.commons.lang.ArrayUtils;
 import org.apache.druid.client.indexing.ClientCompactionTaskGranularitySpec;
@@ -240,7 +242,7 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
 
     final List<DataSegment> resultSegments = entry.segments;
 
-    Preconditions.checkState(!resultSegments.isEmpty(), "Queue entry must not be empty");
+    checkState(!resultSegments.isEmpty(), "Queue entry must not be empty");
 
     final String dataSource = resultSegments.get(0).getDataSource();
 
@@ -292,7 +294,7 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
                   interval ->
                       timeline.lookup(interval).stream()
                           .filter(holder -> isCompactibleHolder(interval, holder)))
-              .collect(Collectors.toList());
+              .collect(toList());
       this.originalTimeline = originalTimeline;
     }
 
@@ -328,12 +330,12 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
       List<DataSegment> candidates =
           Streams.sequentialStreamFrom(timelineObjectHolder.getObject())
               .map(PartitionChunk::getObject)
-              .collect(Collectors.toList());
+              .collect(toList());
       if (originalTimeline != null) {
         Interval umbrellaInterval =
             JodaUtils.umbrellaInterval(
-                candidates.stream().map(DataSegment::getInterval).collect(Collectors.toList()));
-        return Lists.newArrayList(
+                candidates.stream().map(DataSegment::getInterval).collect(toList()));
+        return new ArrayList<>(
             originalTimeline.findNonOvershadowedObjectsInInterval(
                 umbrellaInterval, Partitions.ONLY_COMPLETE));
       }
@@ -360,7 +362,7 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
   }
 
   private boolean needsCompaction(DataSourceCompactionConfig config, SegmentsToCompact candidates) {
-    Preconditions.checkState(!candidates.isEmpty(), "Empty candidates");
+    checkState(!candidates.isEmpty(), "Empty candidates");
     final ClientCompactionTaskQueryTuningConfig tuningConfig =
         ClientCompactionTaskQueryTuningConfig.from(
             config.getTuningConfig(), config.getMaxRowsPerSegment(), null);
@@ -622,14 +624,12 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
       Period skipOffset,
       Granularity configuredSegmentGranularity,
       @Nullable List<Interval> skipIntervals) {
-    Preconditions.checkArgument(
-        timeline != null && !timeline.isEmpty(), "timeline should not be null or empty");
-    Preconditions.checkNotNull(skipOffset, "skipOffset");
+    checkArgument(timeline != null && !timeline.isEmpty(), "timeline should not be null or empty");
+    requireNonNull(skipOffset, "skipOffset");
 
     final TimelineObjectHolder<String, DataSegment> first =
-        Preconditions.checkNotNull(timeline.first(), "first");
-    final TimelineObjectHolder<String, DataSegment> last =
-        Preconditions.checkNotNull(timeline.last(), "last");
+        requireNonNull(timeline.first(), "first");
+    final TimelineObjectHolder<String, DataSegment> last = requireNonNull(timeline.last(), "last");
     final List<Interval> fullSkipIntervals =
         sortAndAddSkipIntervalFromLatest(
             last.getInterval().getEnd(), skipOffset, configuredSegmentGranularity, skipIntervals);
@@ -651,7 +651,7 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
     for (Interval lookupInterval : filteredInterval) {
       if (Intervals.ETERNITY.equals(lookupInterval)) {
         log.warn("Cannot compact datasource[%s] since interval is ETERNITY.", dataSourceName);
-        return Collections.emptyList();
+        return emptyList();
       }
       final List<DataSegment> segments =
           timeline
@@ -661,7 +661,7 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
               // lookupInterval, while
               // we are interested only in segments fully lying within lookupInterval here.
               .filter(segment -> lookupInterval.contains(segment.getInterval()))
-              .collect(Collectors.toList());
+              .collect(toList());
 
       if (segments.isEmpty()) {
         continue;
@@ -670,12 +670,12 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
       DateTime searchStart =
           segments.stream()
               .map(segment -> segment.getId().getIntervalStart())
-              .min(Comparator.naturalOrder())
+              .min(naturalOrder())
               .orElseThrow(AssertionError::new);
       DateTime searchEnd =
           segments.stream()
               .map(segment -> segment.getId().getIntervalEnd())
-              .max(Comparator.naturalOrder())
+              .max(naturalOrder())
               .orElseThrow(AssertionError::new);
       searchIntervals.add(new Interval(searchStart, searchEnd));
     }
@@ -773,7 +773,7 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
     private final List<DataSegment> segments;
 
     private QueueEntry(List<DataSegment> segments) {
-      Preconditions.checkArgument(segments != null && !segments.isEmpty());
+      checkArgument(segments != null && !segments.isEmpty());
       DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;
       for (DataSegment segment : segments) {
         if (segment.getInterval().getStart().compareTo(minStart) < 0) {
@@ -797,7 +797,7 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
     private final long totalSize;
 
     private SegmentsToCompact() {
-      this(Collections.emptyList());
+      this(emptyList());
     }
 
     private SegmentsToCompact(List<DataSegment> segments) {
@@ -819,7 +819,7 @@ public class NewestSegmentFirstIterator implements CompactionSegmentIterator {
 
     private Interval getUmbrellaInterval() {
       return JodaUtils.umbrellaInterval(
-          segments.stream().map(DataSegment::getInterval).collect(Collectors.toList()));
+          segments.stream().map(DataSegment::getInterval).collect(toList()));
     }
 
     private long getNumberOfIntervals() {
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/duty/SegmentCompactionUtil.java b/server/src/main/java/org/apache/druid/server/coordinator/duty/SegmentCompactionUtil.java
index dbdd04e7be..7846f50e7f 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/duty/SegmentCompactionUtil.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/duty/SegmentCompactionUtil.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.coordinator.duty;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+
 import org.joda.time.Interval;
 
 /** Util class used by {@link CompactSegments} and {@link CompactionSegmentSearchPolicy}. */
@@ -31,7 +32,7 @@ class SegmentCompactionUtil {
    * @return an interval of {@code largeInterval} - {@code smallInterval}.
    */
   static Interval removeIntervalFromEnd(Interval largeInterval, Interval smallInterval) {
-    Preconditions.checkArgument(
+    checkArgument(
         largeInterval.getEnd().equals(smallInterval.getEnd()),
         "end should be same. largeInterval[%s] smallInterval[%s]",
         largeInterval,
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRule.java
index d529672a32..8e719f97c8 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRule.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.server.coordinator.rules;
 
+import static java.util.stream.Collectors.toSet;
+
 import it.unimi.dsi.fastutil.objects.Object2LongMap;
 import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.java.util.emitter.EmittingLogger;
 import org.apache.druid.server.coordination.ServerType;
 import org.apache.druid.server.coordinator.CoordinatorStats;
@@ -64,7 +65,7 @@ public abstract class BroadcastDistributionRule implements Rule {
 
                   return !isServingSegment && !serverHolder.isLoadingSegment(segment);
                 })
-            .collect(Collectors.toSet());
+            .collect(toSet());
 
     final CoordinatorStats stats = new CoordinatorStats();
     return stats
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverBroadcastDistributionRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverBroadcastDistributionRule.java
index d91f4888ce..8bde9b0be0 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverBroadcastDistributionRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverBroadcastDistributionRule.java
@@ -32,8 +32,8 @@ public class ForeverBroadcastDistributionRule extends BroadcastDistributionRule
   @JsonCreator
   public ForeverBroadcastDistributionRule() {}
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return TYPE;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverDropRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverDropRule.java
index 0ffdbb6ade..a8cf3e71c9 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverDropRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverDropRule.java
@@ -26,8 +26,8 @@ import org.joda.time.Interval;
 
 /** */
 public class ForeverDropRule extends DropRule {
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return "dropForever";
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverLoadRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverLoadRule.java
index 7b96df3139..c62bfb5d9b 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverLoadRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/ForeverLoadRule.java
@@ -42,14 +42,14 @@ public class ForeverLoadRule extends LoadRule {
     validateTieredReplicants(this.tieredReplicants);
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return "loadForever";
   }
 
-  @Override
   @JsonProperty
+  @Override
   public Map<String, Integer> getTieredReplicants() {
     return tieredReplicants;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalBroadcastDistributionRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalBroadcastDistributionRule.java
index aa02d4098a..30c8066906 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalBroadcastDistributionRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalBroadcastDistributionRule.java
@@ -35,8 +35,8 @@ public class IntervalBroadcastDistributionRule extends BroadcastDistributionRule
     this.interval = interval;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return TYPE;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalDropRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalDropRule.java
index 0bf5812173..54a7c4c08b 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalDropRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalDropRule.java
@@ -35,8 +35,8 @@ public class IntervalDropRule extends DropRule {
     this.interval = interval;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return "dropByInterval";
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalLoadRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalLoadRule.java
index cb8c822bff..f6a744ddaf 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalLoadRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/IntervalLoadRule.java
@@ -48,14 +48,14 @@ public class IntervalLoadRule extends LoadRule {
     this.interval = interval;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return "loadByInterval";
   }
 
-  @Override
   @JsonProperty
+  @Override
   public Map<String, Integer> getTieredReplicants() {
     return tieredReplicants;
   }
@@ -107,7 +107,6 @@ public class IntervalLoadRule extends LoadRule {
   @Override
   public int hashCode() {
     int result = interval != null ? interval.hashCode() : 0;
-    result = 31 * result + (tieredReplicants != null ? tieredReplicants.hashCode() : 0);
-    return result;
+    return 31 * result + (tieredReplicants != null ? tieredReplicants.hashCode() : 0);
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/LoadRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/LoadRule.java
index 12c9643afd..8b534e2e78 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/LoadRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/LoadRule.java
@@ -19,11 +19,16 @@
 
 package org.apache.druid.server.coordinator.rules;
 
+import static java.util.Collections.emptyList;
+import static java.util.stream.Collectors.partitioningBy;
+import static java.util.stream.Collectors.toCollection;
+import static java.util.stream.Collectors.toList;
+
+import com.google.common.collect.Iterators;
 import it.unimi.dsi.fastutil.objects.Object2IntMap;
 import it.unimi.dsi.fastutil.objects.Object2IntOpenHashMap;
 import it.unimi.dsi.fastutil.objects.Object2LongMap;
 import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -31,7 +36,6 @@ import java.util.Map;
 import java.util.NavigableSet;
 import java.util.TreeSet;
 import java.util.function.Predicate;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import org.apache.druid.java.util.common.IAE;
 import org.apache.druid.java.util.common.StringUtils;
@@ -163,7 +167,7 @@ public abstract class LoadRule implements Rule {
               tier,
               targetReplicants.getOrDefault(tier, 0),
               numAssigned, // note that the currentReplicantsInTier is the just-assigned primary
-                           // replica.
+              // replica.
               params,
               createLoadQueueSizeLimitingPredicate(segment)
                   .and(holder -> !holder.equals(primaryHolderToLoad)),
@@ -189,10 +193,10 @@ public abstract class LoadRule implements Rule {
     final NavigableSet<ServerHolder> queue = druidCluster.getHistoricalsByTier(tier);
     if (queue == null) {
       log.makeAlert("Tier[%s] has no servers! Check your cluster configuration!", tier).emit();
-      return Collections.emptyList();
+      return emptyList();
     }
     Predicate<ServerHolder> isActive = s -> !s.isDecommissioning();
-    return queue.stream().filter(isActive.and(predicate)).collect(Collectors.toList());
+    return queue.stream().filter(isActive.and(predicate)).collect(toList());
   }
 
   private Iterator<ServerHolder> getRoundRobinIterator(
@@ -240,7 +244,7 @@ public abstract class LoadRule implements Rule {
       final ServerHolder candidate;
       if (useRoundRobinAssignment) {
         Iterator<ServerHolder> roundRobinIterator = getRoundRobinIterator(params, tier, segment);
-        candidate = roundRobinIterator.hasNext() ? roundRobinIterator.next() : null;
+        candidate = Iterators.getNext(roundRobinIterator, null);
       } else {
         candidate = params.getBalancerStrategy().findNewSegmentHomeReplicator(segment, holders);
         if (candidate != null) {
@@ -350,7 +354,7 @@ public abstract class LoadRule implements Rule {
         // Call balancer strategy
         holder = params.getBalancerStrategy().findNewSegmentHomeReplicator(segment, holders);
       } else {
-        holder = roundRobinServerIterator.hasNext() ? roundRobinServerIterator.next() : null;
+        holder = Iterators.getNext(roundRobinServerIterator, null);
       }
 
       if (holder == null) {
@@ -451,9 +455,7 @@ public abstract class LoadRule implements Rule {
     Map<Boolean, TreeSet<ServerHolder>> holders =
         holdersInTier.stream()
             .filter(s -> s.isServingSegment(segment))
-            .collect(
-                Collectors.partitioningBy(
-                    ServerHolder::isDecommissioning, Collectors.toCollection(TreeSet::new)));
+            .collect(partitioningBy(ServerHolder::isDecommissioning, toCollection(TreeSet::new)));
     TreeSet<ServerHolder> decommissioningServers = holders.get(true);
     TreeSet<ServerHolder> activeServers = holders.get(false);
     int left =
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodBroadcastDistributionRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodBroadcastDistributionRule.java
index 2403e14429..694d258c6d 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodBroadcastDistributionRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodBroadcastDistributionRule.java
@@ -41,8 +41,8 @@ public class PeriodBroadcastDistributionRule extends BroadcastDistributionRule {
     this.includeFuture = includeFuture == null ? DEFAULT_INCLUDE_FUTURE : includeFuture;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return TYPE;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodDropBeforeRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodDropBeforeRule.java
index 655f49a99e..2b54cef113 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodDropBeforeRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodDropBeforeRule.java
@@ -34,8 +34,8 @@ public class PeriodDropBeforeRule extends DropRule {
     this.period = period;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return "dropBeforeByPeriod";
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodDropRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodDropRule.java
index 5a10abfd4a..222c273d09 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodDropRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodDropRule.java
@@ -40,8 +40,8 @@ public class PeriodDropRule extends DropRule {
     this.includeFuture = includeFuture == null ? DEFAULT_INCLUDE_FUTURE : includeFuture;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return "dropByPeriod";
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodLoadRule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodLoadRule.java
index 508056c2b3..bd0e885e6f 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodLoadRule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/PeriodLoadRule.java
@@ -53,8 +53,8 @@ public class PeriodLoadRule extends LoadRule {
     this.includeFuture = includeFuture == null ? DEFAULT_INCLUDE_FUTURE : includeFuture;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getType() {
     return "loadByPeriod";
   }
@@ -69,8 +69,8 @@ public class PeriodLoadRule extends LoadRule {
     return includeFuture;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public Map<String, Integer> getTieredReplicants() {
     return tieredReplicants;
   }
diff --git a/server/src/main/java/org/apache/druid/server/coordinator/rules/Rule.java b/server/src/main/java/org/apache/druid/server/coordinator/rules/Rule.java
index 367b0abb89..c77d54dcb0 100644
--- a/server/src/main/java/org/apache/druid/server/coordinator/rules/Rule.java
+++ b/server/src/main/java/org/apache/druid/server/coordinator/rules/Rule.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.coordinator.rules;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JsonSubTypes;
 import com.fasterxml.jackson.annotation.JsonTypeInfo;
-import com.google.common.base.Preconditions;
 import it.unimi.dsi.fastutil.objects.Object2LongMap;
 import java.util.Map;
 import org.apache.druid.server.coordinator.CoordinatorStats;
@@ -35,25 +36,24 @@ import org.joda.time.Interval;
 
 /** */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "loadByPeriod", value = PeriodLoadRule.class),
-      @JsonSubTypes.Type(name = "loadByInterval", value = IntervalLoadRule.class),
-      @JsonSubTypes.Type(name = "loadForever", value = ForeverLoadRule.class),
-      @JsonSubTypes.Type(name = "dropByPeriod", value = PeriodDropRule.class),
-      @JsonSubTypes.Type(name = "dropBeforeByPeriod", value = PeriodDropBeforeRule.class),
-      @JsonSubTypes.Type(name = "dropByInterval", value = IntervalDropRule.class),
-      @JsonSubTypes.Type(name = "dropForever", value = ForeverDropRule.class),
-      @JsonSubTypes.Type(
-          name = ForeverBroadcastDistributionRule.TYPE,
-          value = ForeverBroadcastDistributionRule.class),
-      @JsonSubTypes.Type(
-          name = IntervalBroadcastDistributionRule.TYPE,
-          value = IntervalBroadcastDistributionRule.class),
-      @JsonSubTypes.Type(
-          name = PeriodBroadcastDistributionRule.TYPE,
-          value = PeriodBroadcastDistributionRule.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = "dropBeforeByPeriod", value = PeriodDropBeforeRule.class),
+  @JsonSubTypes.Type(name = "dropByInterval", value = IntervalDropRule.class),
+  @JsonSubTypes.Type(name = "dropByPeriod", value = PeriodDropRule.class),
+  @JsonSubTypes.Type(name = "dropForever", value = ForeverDropRule.class),
+  @JsonSubTypes.Type(
+      name = ForeverBroadcastDistributionRule.TYPE,
+      value = ForeverBroadcastDistributionRule.class),
+  @JsonSubTypes.Type(
+      name = IntervalBroadcastDistributionRule.TYPE,
+      value = IntervalBroadcastDistributionRule.class),
+  @JsonSubTypes.Type(name = "loadByInterval", value = IntervalLoadRule.class),
+  @JsonSubTypes.Type(name = "loadByPeriod", value = PeriodLoadRule.class),
+  @JsonSubTypes.Type(name = "loadForever", value = ForeverLoadRule.class),
+  @JsonSubTypes.Type(
+      name = PeriodBroadcastDistributionRule.TYPE,
+      value = PeriodBroadcastDistributionRule.class)
+})
 public interface Rule {
   String getType();
 
@@ -78,7 +78,7 @@ public interface Rule {
       Map<String, Object2LongMap<String>> underReplicatedPerTier,
       SegmentReplicantLookup segmentReplicantLookup,
       DataSegment segment) {
-    Preconditions.checkArgument(!canLoadSegments());
+    checkArgument(!canLoadSegments());
   }
 
   /**
@@ -93,7 +93,7 @@ public interface Rule {
       SegmentReplicantLookup segmentReplicantLookup,
       DruidCluster cluster,
       DataSegment segment) {
-    Preconditions.checkArgument(!canLoadSegments());
+    checkArgument(!canLoadSegments());
   }
 
   /**
diff --git a/server/src/main/java/org/apache/druid/server/emitter/ComposingEmitterModule.java b/server/src/main/java/org/apache/druid/server/emitter/ComposingEmitterModule.java
index ea0a2da092..1630331bf8 100644
--- a/server/src/main/java/org/apache/druid/server/emitter/ComposingEmitterModule.java
+++ b/server/src/main/java/org/apache/druid/server/emitter/ComposingEmitterModule.java
@@ -44,9 +44,9 @@ public class ComposingEmitterModule implements DruidModule {
     JsonConfigProvider.bind(binder, "druid.emitter.composing", ComposingEmitterConfig.class);
   }
 
-  @Provides
   @ManageLifecycle
   @Named("composing")
+  @Provides
   public Emitter getEmitter(ComposingEmitterConfig config, final Injector injector) {
     log.info("Creating Composing Emitter with %s", config.getEmitters());
 
diff --git a/server/src/main/java/org/apache/druid/server/emitter/EmitterModule.java b/server/src/main/java/org/apache/druid/server/emitter/EmitterModule.java
index 21fc4bd054..b7bb441981 100644
--- a/server/src/main/java/org/apache/druid/server/emitter/EmitterModule.java
+++ b/server/src/main/java/org/apache/druid/server/emitter/EmitterModule.java
@@ -84,8 +84,8 @@ public class EmitterModule implements Module {
                 version)); // Version is null during `mvn test`.
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public ServiceEmitter getServiceEmitter(
       @Self Supplier<DruidNode> configSupplier,
       Emitter emitter,
diff --git a/server/src/main/java/org/apache/druid/server/emitter/HttpEmitterModule.java b/server/src/main/java/org/apache/druid/server/emitter/HttpEmitterModule.java
index 5093aef507..885010ac0a 100644
--- a/server/src/main/java/org/apache/druid/server/emitter/HttpEmitterModule.java
+++ b/server/src/main/java/org/apache/druid/server/emitter/HttpEmitterModule.java
@@ -81,9 +81,9 @@ public class HttpEmitterModule implements Module {
     return new DefaultAsyncHttpClient(builder.build());
   }
 
-  @Provides
   @ManageLifecycle
   @Named("http")
+  @Provides
   public Emitter getEmitter(
       Supplier<HttpEmitterConfig> config,
       Supplier<HttpEmitterSSLClientConfig> sslConfig,
diff --git a/server/src/main/java/org/apache/druid/server/emitter/LogEmitterModule.java b/server/src/main/java/org/apache/druid/server/emitter/LogEmitterModule.java
index ba5e6122e6..6975f9f898 100644
--- a/server/src/main/java/org/apache/druid/server/emitter/LogEmitterModule.java
+++ b/server/src/main/java/org/apache/druid/server/emitter/LogEmitterModule.java
@@ -40,9 +40,9 @@ public class LogEmitterModule implements Module {
     JsonConfigProvider.bind(binder, "druid.emitter.logging", LoggingEmitterConfig.class);
   }
 
-  @Provides
   @ManageLifecycle
   @Named(EMITTER_TYPE)
+  @Provides
   public Emitter makeEmitter(Supplier<LoggingEmitterConfig> config, ObjectMapper jsonMapper) {
     return new LoggingEmitter(config.get(), jsonMapper);
   }
diff --git a/server/src/main/java/org/apache/druid/server/emitter/NoopEmitterModule.java b/server/src/main/java/org/apache/druid/server/emitter/NoopEmitterModule.java
index 63df9c6d23..e51b1d97d5 100644
--- a/server/src/main/java/org/apache/druid/server/emitter/NoopEmitterModule.java
+++ b/server/src/main/java/org/apache/druid/server/emitter/NoopEmitterModule.java
@@ -34,9 +34,9 @@ public class NoopEmitterModule implements Module {
   @Override
   public void configure(Binder binder) {}
 
-  @Provides
   @ManageLifecycle
   @Named(EMITTER_TYPE)
+  @Provides
   public Emitter makeEmitter() {
     return new NoopEmitter();
   }
diff --git a/server/src/main/java/org/apache/druid/server/emitter/ParametrizedUriEmitterModule.java b/server/src/main/java/org/apache/druid/server/emitter/ParametrizedUriEmitterModule.java
index 3a39cf466c..a34b0929fd 100644
--- a/server/src/main/java/org/apache/druid/server/emitter/ParametrizedUriEmitterModule.java
+++ b/server/src/main/java/org/apache/druid/server/emitter/ParametrizedUriEmitterModule.java
@@ -48,9 +48,9 @@ public class ParametrizedUriEmitterModule implements Module {
         ParametrizedUriEmitterSSLClientConfig.class);
   }
 
-  @Provides
   @ManageLifecycle
   @Named("parametrized")
+  @Provides
   public Emitter getEmitter(
       Supplier<ParametrizedUriEmitterConfig> config,
       Supplier<ParametrizedUriEmitterSSLClientConfig> parametrizedSSLClientConfig,
diff --git a/server/src/main/java/org/apache/druid/server/emitter/SwitchingEmitterModule.java b/server/src/main/java/org/apache/druid/server/emitter/SwitchingEmitterModule.java
index 9c79f18596..dfd11be0b7 100644
--- a/server/src/main/java/org/apache/druid/server/emitter/SwitchingEmitterModule.java
+++ b/server/src/main/java/org/apache/druid/server/emitter/SwitchingEmitterModule.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server.emitter;
 
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toMap;
+
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Binder;
 import com.google.inject.Injector;
@@ -29,7 +32,6 @@ import com.google.inject.name.Named;
 import com.google.inject.name.Names;
 import java.util.List;
 import java.util.Map;
-import java.util.stream.Collectors;
 import org.apache.druid.guice.JsonConfigProvider;
 import org.apache.druid.guice.ManageLifecycle;
 import org.apache.druid.java.util.common.logger.Logger;
@@ -47,9 +49,9 @@ public class SwitchingEmitterModule implements Module {
     JsonConfigProvider.bind(binder, "druid.emitter.switching", SwitchingEmitterConfig.class);
   }
 
-  @Provides
   @ManageLifecycle
   @Named(EMITTER_TYPE)
+  @Provides
   public Emitter makeEmitter(SwitchingEmitterConfig config, final Injector injector) {
     log.info(
         "Createing Switching emitter with %s, and default emitter %s",
@@ -57,7 +59,7 @@ public class SwitchingEmitterModule implements Module {
     Map<String, List<Emitter>> switchingEmitters =
         config.getEmitters().entrySet().stream()
             .collect(
-                Collectors.toMap(
+                toMap(
                     Map.Entry::getKey,
                     entry ->
                         entry.getValue().stream()
@@ -65,7 +67,7 @@ public class SwitchingEmitterModule implements Module {
                                 emitterName ->
                                     injector.getInstance(
                                         Key.get(Emitter.class, Names.named(emitterName))))
-                            .collect(Collectors.toList())));
+                            .collect(toList())));
 
     ImmutableList.Builder<Emitter> defaultEmittersBuilder = new ImmutableList.Builder<>();
     for (String emitterName : config.getDefaultEmitter()) {
diff --git a/server/src/main/java/org/apache/druid/server/http/BrokerResource.java b/server/src/main/java/org/apache/druid/server/http/BrokerResource.java
index eb966736cb..ffe97f1ec7 100644
--- a/server/src/main/java/org/apache/druid/server/http/BrokerResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/BrokerResource.java
@@ -41,8 +41,8 @@ public class BrokerResource {
 
   @GET
   @Path("/loadstatus")
-  @ResourceFilters(StateResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(StateResourceFilter.class)
   public Response getLoadStatus() {
     return Response.ok(ImmutableMap.of("inventoryInitialized", brokerServerView.isInitialized()))
         .build();
diff --git a/server/src/main/java/org/apache/druid/server/http/ClusterResource.java b/server/src/main/java/org/apache/druid/server/http/ClusterResource.java
index c0b50dcc72..bf54e036d3 100644
--- a/server/src/main/java/org/apache/druid/server/http/ClusterResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/ClusterResource.java
@@ -83,8 +83,8 @@ public class ClusterResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON})
   @Path("/{nodeRole}")
+  @Produces(MediaType.APPLICATION_JSON)
   public Response getClusterServers(
       @PathParam("nodeRole") NodeRole nodeRole, @QueryParam("full") boolean full) {
     if (nodeRole == null) {
diff --git a/server/src/main/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResource.java b/server/src/main/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResource.java
index fe3505a432..4986850204 100644
--- a/server/src/main/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResource.java
@@ -19,20 +19,21 @@
 
 package org.apache.druid.server.http;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toMap;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.inject.Inject;
 import com.sun.jersey.spi.container.ResourceFilters;
-import java.nio.charset.StandardCharsets;
 import java.util.List;
 import java.util.Map;
 import java.util.NoSuchElementException;
 import java.util.concurrent.Callable;
 import java.util.concurrent.ThreadLocalRandom;
-import java.util.function.Function;
-import java.util.stream.Collectors;
 import javax.servlet.http.HttpServletRequest;
 import javax.ws.rs.Consumes;
 import javax.ws.rs.DELETE;
@@ -97,9 +98,9 @@ public class CoordinatorCompactionConfigsResource {
     return Response.ok(CoordinatorCompactionConfig.current(manager)).build();
   }
 
+  @Consumes(MediaType.APPLICATION_JSON)
   @POST
   @Path("/taskslots")
-  @Consumes(MediaType.APPLICATION_JSON)
   public Response setCompactionTaskLimit(
       @QueryParam("ratio") Double compactionTaskSlotRatio,
       @QueryParam("max") Integer maxCompactionTaskSlots,
@@ -125,8 +126,8 @@ public class CoordinatorCompactionConfigsResource {
     return updateConfigHelper(callable);
   }
 
-  @POST
   @Consumes(MediaType.APPLICATION_JSON)
+  @POST
   public Response addOrUpdateCompactionConfig(
       final DataSourceCompactionConfig newConfig,
       @HeaderParam(AuditManager.X_DRUID_AUTHOR) @DefaultValue("") final String author,
@@ -140,9 +141,7 @@ public class CoordinatorCompactionConfigsResource {
           final CoordinatorCompactionConfig newCompactionConfig;
           final Map<String, DataSourceCompactionConfig> newConfigs =
               current.getCompactionConfigs().stream()
-                  .collect(
-                      Collectors.toMap(
-                          DataSourceCompactionConfig::getDataSource, Function.identity()));
+                  .collect(toMap(DataSourceCompactionConfig::getDataSource, identity()));
           newConfigs.put(newConfig.getDataSource(), newConfig);
           newCompactionConfig =
               CoordinatorCompactionConfig.from(current, ImmutableList.copyOf(newConfigs.values()));
@@ -163,8 +162,7 @@ public class CoordinatorCompactionConfigsResource {
     final CoordinatorCompactionConfig current = CoordinatorCompactionConfig.current(manager);
     final Map<String, DataSourceCompactionConfig> configs =
         current.getCompactionConfigs().stream()
-            .collect(
-                Collectors.toMap(DataSourceCompactionConfig::getDataSource, Function.identity()));
+            .collect(toMap(DataSourceCompactionConfig::getDataSource, identity()));
 
     final DataSourceCompactionConfig config = configs.get(dataSource);
     if (config == null) {
@@ -201,7 +199,7 @@ public class CoordinatorCompactionConfigsResource {
       for (AuditEntry audit : auditEntries) {
         CoordinatorCompactionConfig coordinatorCompactionConfig =
             CoordinatorCompactionConfig.convertByteToConfig(
-                manager, audit.getPayload().getBytes(StandardCharsets.UTF_8));
+                manager, audit.getPayload().getBytes(UTF_8));
         history.add(coordinatorCompactionConfig, audit.getAuditInfo(), audit.getAuditTime());
       }
       return Response.ok(history.getHistory()).build();
@@ -227,9 +225,7 @@ public class CoordinatorCompactionConfigsResource {
               CoordinatorCompactionConfig.convertByteToConfig(manager, currentBytes);
           final Map<String, DataSourceCompactionConfig> configs =
               current.getCompactionConfigs().stream()
-                  .collect(
-                      Collectors.toMap(
-                          DataSourceCompactionConfig::getDataSource, Function.identity()));
+                  .collect(toMap(DataSourceCompactionConfig::getDataSource, identity()));
 
           final DataSourceCompactionConfig config = configs.remove(dataSource);
           if (config == null) {
diff --git a/server/src/main/java/org/apache/druid/server/http/CoordinatorDynamicConfigsResource.java b/server/src/main/java/org/apache/druid/server/http/CoordinatorDynamicConfigsResource.java
index 77bd1e351b..5da5af88ec 100644
--- a/server/src/main/java/org/apache/druid/server/http/CoordinatorDynamicConfigsResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/CoordinatorDynamicConfigsResource.java
@@ -64,8 +64,8 @@ public class CoordinatorDynamicConfigsResource {
   }
 
   // default value is used for backwards compatibility
-  @POST
   @Consumes(MediaType.APPLICATION_JSON)
+  @POST
   public Response setDynamicConfigs(
       final CoordinatorDynamicConfig.Builder dynamicConfigBuilder,
       @HeaderParam(AuditManager.X_DRUID_AUTHOR) @DefaultValue("") final String author,
diff --git a/server/src/main/java/org/apache/druid/server/http/CoordinatorResource.java b/server/src/main/java/org/apache/druid/server/http/CoordinatorResource.java
index 0fe57505b6..1c7c8271ec 100644
--- a/server/src/main/java/org/apache/druid/server/http/CoordinatorResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/CoordinatorResource.java
@@ -50,8 +50,8 @@ public class CoordinatorResource {
 
   @GET
   @Path("/leader")
-  @ResourceFilters(StateResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(StateResourceFilter.class)
   public Response getLeader() {
     return Response.ok(coordinator.getCurrentLeader()).build();
   }
@@ -75,8 +75,8 @@ public class CoordinatorResource {
 
   @GET
   @Path("/loadstatus")
-  @ResourceFilters(StateResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(StateResourceFilter.class)
   public Response getLoadStatus(
       @QueryParam("simple") String simple,
       @QueryParam("full") String full,
@@ -97,8 +97,8 @@ public class CoordinatorResource {
 
   @GET
   @Path("/loadqueue")
-  @ResourceFilters(StateResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(StateResourceFilter.class)
   public Response getLoadQueue(
       @QueryParam("simple") String simple, @QueryParam("full") String full) {
     if (simple != null) {
diff --git a/server/src/main/java/org/apache/druid/server/http/DataSourcesResource.java b/server/src/main/java/org/apache/druid/server/http/DataSourcesResource.java
index 869c1dcaa0..55f6b564ff 100644
--- a/server/src/main/java/org/apache/druid/server/http/DataSourcesResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/DataSourcesResource.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server.http;
 
+import static java.util.Collections.emptyMap;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toSet;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.annotations.VisibleForTesting;
@@ -30,7 +34,6 @@ import com.google.inject.Inject;
 import com.sun.jersey.spi.container.ResourceFilters;
 import it.unimi.dsi.fastutil.objects.Object2LongMap;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Comparator;
 import java.util.EnumMap;
 import java.util.HashMap;
@@ -43,7 +46,6 @@ import java.util.SortedMap;
 import java.util.TreeMap;
 import java.util.TreeSet;
 import java.util.function.Predicate;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import javax.servlet.http.HttpServletRequest;
 import javax.ws.rs.Consumes;
@@ -136,10 +138,9 @@ public class DataSourcesResource {
     if (full != null) {
       entity = datasources;
     } else if (simple != null) {
-      entity = datasources.stream().map(this::makeSimpleDatasource).collect(Collectors.toList());
+      entity = datasources.stream().map(this::makeSimpleDatasource).collect(toList());
     } else {
-      entity =
-          datasources.stream().map(ImmutableDruidDataSource::getName).collect(Collectors.toList());
+      entity = datasources.stream().map(ImmutableDruidDataSource::getName).collect(toList());
     }
 
     return builder.entity(entity).build();
@@ -169,9 +170,9 @@ public class DataSourcesResource {
     int markSegments() throws UnknownSegmentIdsException;
   }
 
+  @Consumes(MediaType.APPLICATION_JSON)
   @POST
   @Path("/{dataSourceName}")
-  @Consumes(MediaType.APPLICATION_JSON)
   @ResourceFilters(DatasourceResourceFilter.class)
   public Response markAsUsedAllNonOvershadowedSegments(
       @PathParam("dataSourceName") final String dataSourceName) {
@@ -204,11 +205,11 @@ public class DataSourcesResource {
         "markAsUsedNonOvershadowedSegments", dataSourceName, payload, markSegments);
   }
 
+  @Consumes(MediaType.APPLICATION_JSON)
   @POST
   @Path("/{dataSourceName}/markUnused")
-  @ResourceFilters(DatasourceResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
-  @Consumes(MediaType.APPLICATION_JSON)
+  @ResourceFilters(DatasourceResourceFilter.class)
   public Response markSegmentsAsUnused(
       @PathParam("dataSourceName") final String dataSourceName,
       final MarkDataSourceSegmentsPayload payload) {
@@ -222,13 +223,13 @@ public class DataSourcesResource {
                 payload.getSegmentIds().stream()
                     .map(idStr -> SegmentId.tryParse(dataSourceName, idStr))
                     .filter(Objects::nonNull)
-                    .collect(Collectors.toSet());
+                    .collect(toSet());
 
             // Note: segments for the "wrong" datasource are ignored.
             return segmentsMetadataManager.markSegmentsAsUnused(
                 segmentIds.stream()
                     .filter(segmentId -> segmentId.getDataSource().equals(dataSourceName))
-                    .collect(Collectors.toSet()));
+                    .collect(toSet()));
           }
         };
     return doMarkSegmentsWithPayload("markSegmentsAsUnused", dataSourceName, payload, markSegments);
@@ -289,8 +290,8 @@ public class DataSourcesResource {
   @DELETE
   @Deprecated
   @Path("/{dataSourceName}")
-  @ResourceFilters(DatasourceResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(DatasourceResourceFilter.class)
   public Response markAsUnusedAllSegmentsOrKillUnusedSegmentsInInterval(
       @PathParam("dataSourceName") final String dataSourceName,
       @QueryParam("kill") final String kill,
@@ -311,8 +312,8 @@ public class DataSourcesResource {
 
   @DELETE
   @Path("/{dataSourceName}/intervals/{interval}")
-  @ResourceFilters(DatasourceResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(DatasourceResourceFilter.class)
   public Response killUnusedSegmentsInInterval(
       @PathParam("dataSourceName") final String dataSourceName,
       @PathParam("interval") final String interval) {
@@ -612,9 +613,9 @@ public class DataSourcesResource {
     return Response.ok(ImmutableMap.of("segmentStateChanged", segmentStateChanged)).build();
   }
 
+  @Consumes(MediaType.APPLICATION_JSON)
   @POST
   @Path("/{dataSourceName}/segments/{segmentId}")
-  @Consumes(MediaType.APPLICATION_JSON)
   @ResourceFilters(DatasourceResourceFilter.class)
   public Response markSegmentAsUsed(
       @PathParam("dataSourceName") String dataSourceName,
@@ -647,7 +648,7 @@ public class DataSourcesResource {
         serverInventoryView.getInventory().stream()
             .map(server -> server.getDataSource(dataSourceName))
             .filter(Objects::nonNull)
-            .collect(Collectors.toList());
+            .collect(toList());
 
     if (dataSources.isEmpty()) {
       return null;
@@ -666,7 +667,7 @@ public class DataSourcesResource {
       }
     }
 
-    return new ImmutableDruidDataSource(dataSourceName, Collections.emptyMap(), segmentMap);
+    return new ImmutableDruidDataSource(dataSourceName, emptyMap(), segmentMap);
   }
 
   @Nullable
diff --git a/server/src/main/java/org/apache/druid/server/http/HistoricalResource.java b/server/src/main/java/org/apache/druid/server/http/HistoricalResource.java
index f16bcbd3d1..eb0c4e832b 100644
--- a/server/src/main/java/org/apache/druid/server/http/HistoricalResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/HistoricalResource.java
@@ -41,8 +41,8 @@ public class HistoricalResource {
 
   @GET
   @Path("/loadstatus")
-  @ResourceFilters(StateResourceFilter.class)
   @Produces(MediaType.APPLICATION_JSON)
+  @ResourceFilters(StateResourceFilter.class)
   public Response getLoadStatus() {
     return Response.ok(ImmutableMap.of("cacheInitialized", segmentLoadDropHandler.isStarted()))
         .build();
diff --git a/server/src/main/java/org/apache/druid/server/http/HostAndPortWithScheme.java b/server/src/main/java/org/apache/druid/server/http/HostAndPortWithScheme.java
index 82c23e8eca..71d936cc36 100644
--- a/server/src/main/java/org/apache/druid/server/http/HostAndPortWithScheme.java
+++ b/server/src/main/java/org/apache/druid/server/http/HostAndPortWithScheme.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.http;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkState;
+
 import com.google.common.net.HostAndPort;
 import org.apache.druid.java.util.common.IAE;
 import org.apache.druid.java.util.common.StringUtils;
@@ -57,7 +58,7 @@ public class HostAndPortWithScheme {
 
   private static String checkAndGetScheme(String scheme) {
     String schemeLowerCase = StringUtils.toLowerCase(scheme);
-    Preconditions.checkState("http".equals(schemeLowerCase) || "https".equals(schemeLowerCase));
+    checkState("http".equals(schemeLowerCase) || "https".equals(schemeLowerCase));
     return schemeLowerCase;
   }
 
@@ -106,7 +107,6 @@ public class HostAndPortWithScheme {
   @Override
   public int hashCode() {
     int result = scheme.hashCode();
-    result = 31 * result + hostAndPort.hashCode();
-    return result;
+    return 31 * result + hostAndPort.hashCode();
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/http/InventoryViewUtils.java b/server/src/main/java/org/apache/druid/server/http/InventoryViewUtils.java
index d707b5710c..59715e763b 100644
--- a/server/src/main/java/org/apache/druid/server/http/InventoryViewUtils.java
+++ b/server/src/main/java/org/apache/druid/server/http/InventoryViewUtils.java
@@ -19,11 +19,14 @@
 
 package org.apache.druid.server.http;
 
-import java.util.Collections;
+import static java.util.Collections.singletonList;
+import static java.util.Collections.unmodifiableSortedSet;
+import static java.util.Comparator.comparing;
+import static java.util.stream.Collectors.toCollection;
+
 import java.util.Comparator;
 import java.util.SortedSet;
 import java.util.TreeSet;
-import java.util.stream.Collectors;
 import javax.servlet.http.HttpServletRequest;
 import org.apache.druid.client.DruidDataSource;
 import org.apache.druid.client.ImmutableDruidDataSource;
@@ -34,14 +37,14 @@ import org.apache.druid.server.security.AuthorizerMapper;
 
 public interface InventoryViewUtils {
   static Comparator<ImmutableDruidDataSource> comparingByName() {
-    return Comparator.comparing(ImmutableDruidDataSource::getName);
+    return comparing(ImmutableDruidDataSource::getName);
   }
 
   static SortedSet<ImmutableDruidDataSource> getDataSources(InventoryView serverInventoryView) {
     return serverInventoryView.getInventory().stream()
         .flatMap(server -> server.getDataSources().stream())
         .map(DruidDataSource::toImmutableDruidDataSource)
-        .collect(Collectors.toCollection(() -> new TreeSet<>(comparingByName())));
+        .collect(toCollection(() -> new TreeSet<>(comparingByName())));
   }
 
   static SortedSet<ImmutableDruidDataSource> getSecuredDataSources(
@@ -57,11 +60,11 @@ public interface InventoryViewUtils {
             request,
             getDataSources(inventoryView),
             datasource ->
-                Collections.singletonList(
+                singletonList(
                     AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(datasource.getName())),
             authorizerMapper);
     SortedSet<ImmutableDruidDataSource> set = new TreeSet<>(comparingByName());
     filteredResources.forEach(set::add);
-    return Collections.unmodifiableSortedSet(set);
+    return unmodifiableSortedSet(set);
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/http/LookupCoordinatorResource.java b/server/src/main/java/org/apache/druid/server/http/LookupCoordinatorResource.java
index 1c64efac90..3c958e24c4 100644
--- a/server/src/main/java/org/apache/druid/server/http/LookupCoordinatorResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/LookupCoordinatorResource.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.http;
 
+import static java.util.stream.Collectors.toMap;
+
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.core.type.TypeReference;
@@ -39,7 +41,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
-import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import javax.servlet.http.HttpServletRequest;
 import javax.ws.rs.Consumes;
@@ -114,8 +115,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON})
   @Path("/config/all")
+  @Produces(MediaType.APPLICATION_JSON)
   public Response getAllLookupSpecs() {
     try {
       final Map<String, Map<String, LookupExtractorFactoryMapContainer>> knownLookups =
@@ -131,10 +132,10 @@ public class LookupCoordinatorResource {
     }
   }
 
+  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @POST
   @Path("/config")
   @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
-  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response updateAllLookups(
       InputStream in,
       @HeaderParam(AuditManager.X_DRUID_AUTHOR) @DefaultValue("") final String author,
@@ -169,8 +170,8 @@ public class LookupCoordinatorResource {
   }
 
   @DELETE
-  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @Path("/config/{tier}")
+  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response deleteTier(
       @PathParam("tier") String tier,
       @HeaderParam(AuditManager.X_DRUID_AUTHOR) @DefaultValue("") final String author,
@@ -197,8 +198,8 @@ public class LookupCoordinatorResource {
   }
 
   @DELETE
-  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @Path("/config/{tier}/{lookup}")
+  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response deleteLookup(
       @PathParam("tier") String tier,
       @PathParam("lookup") String lookup,
@@ -232,8 +233,8 @@ public class LookupCoordinatorResource {
   }
 
   @POST
-  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @Path("/config/{tier}/{lookup}")
+  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response createOrUpdateLookup(
       @PathParam("tier") String tier,
       @PathParam("lookup") String lookup,
@@ -278,8 +279,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @Path("/config/{tier}/{lookup}")
+  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response getSpecificLookup(
       @PathParam("tier") String tier, @PathParam("lookup") String lookup) {
     try {
@@ -311,8 +312,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @Path("/config/{tier}")
+  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response getSpecificTier(
       @PathParam("tier") String tier,
       @DefaultValue("false") @QueryParam("detailed") boolean detailed) {
@@ -349,8 +350,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON})
   @Path("/status")
+  @Produces(MediaType.APPLICATION_JSON)
   public Response getAllLookupsStatus(@QueryParam("detailed") boolean detailed) {
     try {
       Map<String, Map<String, LookupExtractorFactoryMapContainer>> configuredLookups =
@@ -394,8 +395,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON})
   @Path("/status/{tier}")
+  @Produces(MediaType.APPLICATION_JSON)
   public Response getLookupStatusForTier(
       @PathParam("tier") String tier, @QueryParam("detailed") boolean detailed) {
     try {
@@ -440,8 +441,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON})
   @Path("/status/{tier}/{lookup}")
+  @Produces(MediaType.APPLICATION_JSON)
   public Response getSpecificLookupStatus(
       @PathParam("tier") String tier,
       @PathParam("lookup") String lookup,
@@ -512,8 +513,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON})
   @Path("/nodeStatus")
+  @Produces(MediaType.APPLICATION_JSON)
   public Response getAllNodesStatus(
       @QueryParam("discover") boolean discover,
       @QueryParam("detailed") @Nullable Boolean detailed) {
@@ -552,8 +553,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON})
   @Path("/nodeStatus/{tier}")
+  @Produces(MediaType.APPLICATION_JSON)
   public Response getNodesStatusInTier(@PathParam("tier") String tier) {
     try {
       Map<HostAndPort, LookupsState<LookupExtractorFactoryMapContainer>> lookupsStateOnHosts =
@@ -582,8 +583,8 @@ public class LookupCoordinatorResource {
   }
 
   @GET
-  @Produces({MediaType.APPLICATION_JSON})
   @Path("/nodeStatus/{tier}/{hostAndPort}")
+  @Produces(MediaType.APPLICATION_JSON)
   public Response getSpecificNodeStatus(
       @PathParam("tier") String tier, @PathParam("hostAndPort") HostAndPort hostAndPort) {
     try {
@@ -629,10 +630,10 @@ public class LookupCoordinatorResource {
         } else {
           Map<String, String> current =
               lookupsState.getCurrent().entrySet().stream()
-                  .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().getVersion()));
+                  .collect(toMap(Map.Entry::getKey, e -> e.getValue().getVersion()));
           Map<String, String> toLoad =
               lookupsState.getToLoad().entrySet().stream()
-                  .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().getVersion()));
+                  .collect(toMap(Map.Entry::getKey, e -> e.getValue().getVersion()));
           tierNodesStatus.put(node, new LookupsState<>(current, toLoad, lookupsState.getToDrop()));
         }
       }
diff --git a/server/src/main/java/org/apache/druid/server/http/MetadataResource.java b/server/src/main/java/org/apache/druid/server/http/MetadataResource.java
index d62d769e14..b3c6022ec4 100644
--- a/server/src/main/java/org/apache/druid/server/http/MetadataResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/MetadataResource.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server.http;
 
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toCollection;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Function;
 import com.google.common.collect.Collections2;
@@ -26,11 +30,9 @@ import com.google.common.collect.Iterables;
 import com.google.inject.Inject;
 import com.sun.jersey.spi.container.ResourceFilters;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Set;
 import java.util.TreeSet;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 import javax.annotation.Nullable;
 import javax.servlet.http.HttpServletRequest;
@@ -96,14 +98,13 @@ public class MetadataResource {
       dataSourceNamesPreAuth =
           druidDataSources.stream()
               .map(ImmutableDruidDataSource::getName)
-              .collect(Collectors.toCollection(TreeSet::new));
+              .collect(toCollection(TreeSet::new));
     }
 
     final TreeSet<String> dataSourceNamesPostAuth = new TreeSet<>();
     Function<String, Iterable<ResourceAction>> raGenerator =
         datasourceName ->
-            Collections.singletonList(
-                AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(datasourceName));
+            singletonList(AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(datasourceName));
 
     Iterables.addAll(
         dataSourceNamesPostAuth,
@@ -143,14 +144,14 @@ public class MetadataResource {
               .filter(
                   dataSourceWithUsedSegments ->
                       dataSources.contains(dataSourceWithUsedSegments.getName()))
-              .collect(Collectors.toList());
+              .collect(toList());
     }
     final Stream<DataSegment> usedSegments =
         dataSourcesWithUsedSegments.stream().flatMap(t -> t.getSegments().stream());
 
     final Function<DataSegment, Iterable<ResourceAction>> raGenerator =
         segment ->
-            Collections.singletonList(
+            singletonList(
                 AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(segment.getDataSource()));
 
     final Iterable<DataSegment> authorizedSegments =
@@ -173,7 +174,7 @@ public class MetadataResource {
               .filter(
                   dataSourceWithUsedSegments ->
                       dataSources.contains(dataSourceWithUsedSegments.getName()))
-              .collect(Collectors.toList());
+              .collect(toList());
     }
     final Stream<DataSegment> usedSegments =
         dataSourcesWithUsedSegments.stream().flatMap(t -> t.getSegments().stream());
@@ -186,7 +187,7 @@ public class MetadataResource {
 
     final Function<SegmentWithOvershadowedStatus, Iterable<ResourceAction>> raGenerator =
         segment ->
-            Collections.singletonList(
+            singletonList(
                 AuthorizationUtils.DATASOURCE_READ_RA_GENERATOR.apply(
                     segment.getDataSegment().getDataSource()));
 
diff --git a/server/src/main/java/org/apache/druid/server/http/RulesResource.java b/server/src/main/java/org/apache/druid/server/http/RulesResource.java
index 03f888c069..9bcc9cf504 100644
--- a/server/src/main/java/org/apache/druid/server/http/RulesResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/RulesResource.java
@@ -81,9 +81,9 @@ public class RulesResource {
   }
 
   // default value is used for backwards compatibility
+  @Consumes(MediaType.APPLICATION_JSON)
   @POST
   @Path("/{dataSourceName}")
-  @Consumes(MediaType.APPLICATION_JSON)
   @ResourceFilters(RulesResourceFilter.class)
   public Response setDatasourceRules(
       @PathParam("dataSourceName") final String dataSourceName,
diff --git a/server/src/main/java/org/apache/druid/server/http/SegmentListerResource.java b/server/src/main/java/org/apache/druid/server/http/SegmentListerResource.java
index bec75c1274..9559b165b1 100644
--- a/server/src/main/java/org/apache/druid/server/http/SegmentListerResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/SegmentListerResource.java
@@ -111,9 +111,9 @@ public class SegmentListerResource {
    * @return null to avoid "MUST return a non-void type" warning.
    * @throws IOException
    */
+  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @GET
   @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
-  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Void getSegments(
       @QueryParam("counter") long counter,
       @QueryParam("hash") long hash,
@@ -205,10 +205,10 @@ public class SegmentListerResource {
    * <p>It returns a map of "load/drop request -> SUCCESS/FAILED/PENDING status" for each request in
    * the batch.
    */
+  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @POST
   @Path("/changeRequests")
   @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
-  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public void applyDataSegmentChangeRequests(
       @QueryParam("timeout") long timeout,
       List<DataSegmentChangeRequest> changeRequestList,
diff --git a/server/src/main/java/org/apache/druid/server/http/SelfDiscoveryResource.java b/server/src/main/java/org/apache/druid/server/http/SelfDiscoveryResource.java
index 4a7bbe9e06..d2bfbcff1d 100644
--- a/server/src/main/java/org/apache/druid/server/http/SelfDiscoveryResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/SelfDiscoveryResource.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server.http;
 
+import static java.util.Collections.singletonMap;
+
 import com.google.common.collect.Lists;
 import com.google.inject.Inject;
 import com.google.inject.Singleton;
 import com.sun.jersey.spi.container.ResourceFilters;
-import java.util.Collections;
 import java.util.List;
 import java.util.Set;
 import java.util.function.BooleanSupplier;
@@ -86,7 +87,7 @@ public class SelfDiscoveryResource {
   @Produces(MediaType.APPLICATION_JSON)
   @ResourceFilters(StateResourceFilter.class)
   public Response getSelfDiscoveredStatus() {
-    return Response.ok(Collections.singletonMap("selfDiscovered", isDiscoveredAllRoles())).build();
+    return Response.ok(singletonMap("selfDiscovered", isDiscoveredAllRoles())).build();
   }
 
   /** See the description of this endpoint in api-reference.md. */
diff --git a/server/src/main/java/org/apache/druid/server/http/TiersResource.java b/server/src/main/java/org/apache/druid/server/http/TiersResource.java
index d03f5d2df5..df15e640d4 100644
--- a/server/src/main/java/org/apache/druid/server/http/TiersResource.java
+++ b/server/src/main/java/org/apache/druid/server/http/TiersResource.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.server.http;
 
+import static java.util.stream.Collectors.toSet;
+
 import com.google.inject.Inject;
 import com.sun.jersey.spi.container.ResourceFilters;
 import java.util.EnumMap;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
-import java.util.stream.Collectors;
 import javax.ws.rs.GET;
 import javax.ws.rs.Path;
 import javax.ws.rs.PathParam;
@@ -75,9 +76,7 @@ public class TiersResource {
     }
 
     Set<String> tiers =
-        serverInventoryView.getInventory().stream()
-            .map(DruidServer::getTier)
-            .collect(Collectors.toSet());
+        serverInventoryView.getInventory().stream().map(DruidServer::getTier).collect(toSet());
 
     return builder.entity(tiers).build();
   }
@@ -118,9 +117,9 @@ public class TiersResource {
     Set<String> retVal =
         serverInventoryView.getInventory().stream()
             .filter(druidServer -> druidServer.getTier().equalsIgnoreCase(tierName))
-            .flatMap(
-                druidServer -> druidServer.getDataSources().stream().map(DruidDataSource::getName))
-            .collect(Collectors.toSet());
+            .flatMap(druidServer -> druidServer.getDataSources().stream())
+            .map(DruidDataSource::getName)
+            .collect(toSet());
 
     return Response.ok(retVal).build();
   }
diff --git a/server/src/main/java/org/apache/druid/server/http/security/DatasourceResourceFilter.java b/server/src/main/java/org/apache/druid/server/http/security/DatasourceResourceFilter.java
index 88ea04f490..4743c30f93 100644
--- a/server/src/main/java/org/apache/druid/server/http/security/DatasourceResourceFilter.java
+++ b/server/src/main/java/org/apache/druid/server/http/security/DatasourceResourceFilter.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.http.security;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.base.Predicate;
 import com.google.common.collect.Iterables;
 import com.google.inject.Inject;
@@ -78,7 +79,7 @@ public class DatasourceResourceFilter extends AbstractResourceFilter {
                         })
                     + 1)
             .getPath();
-    Preconditions.checkNotNull(dataSourceName);
+    requireNonNull(dataSourceName);
     return dataSourceName;
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/http/security/RulesResourceFilter.java b/server/src/main/java/org/apache/druid/server/http/security/RulesResourceFilter.java
index cf0e566506..b777e88e8f 100644
--- a/server/src/main/java/org/apache/druid/server/http/security/RulesResourceFilter.java
+++ b/server/src/main/java/org/apache/druid/server/http/security/RulesResourceFilter.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.http.security;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import com.google.common.base.Predicate;
 import com.google.common.collect.Iterables;
 import com.google.inject.Inject;
@@ -60,7 +61,7 @@ public class RulesResourceFilter extends AbstractResourceFilter {
                         })
                     + 1)
             .getPath();
-    Preconditions.checkNotNull(dataSourceName);
+    requireNonNull(dataSourceName);
 
     final ResourceAction resourceAction =
         new ResourceAction(
diff --git a/server/src/main/java/org/apache/druid/server/initialization/ZkPathsConfig.java b/server/src/main/java/org/apache/druid/server/initialization/ZkPathsConfig.java
index 7eb5c643a1..4ffc277dd9 100644
--- a/server/src/main/java/org/apache/druid/server/initialization/ZkPathsConfig.java
+++ b/server/src/main/java/org/apache/druid/server/initialization/ZkPathsConfig.java
@@ -37,24 +37,24 @@ public class ZkPathsConfig {
   }
 
   public String getPropertiesPath() {
-    return (null == propertiesPath) ? defaultPath("properties") : propertiesPath;
+    return propertiesPath == null ? defaultPath("properties") : propertiesPath;
   }
 
   public String getAnnouncementsPath() {
-    return (null == announcementsPath) ? defaultPath("announcements") : announcementsPath;
+    return announcementsPath == null ? defaultPath("announcements") : announcementsPath;
   }
 
   @Deprecated
   public String getServedSegmentsPath() {
-    return (null == servedSegmentsPath) ? defaultPath("servedSegments") : servedSegmentsPath;
+    return servedSegmentsPath == null ? defaultPath("servedSegments") : servedSegmentsPath;
   }
 
   public String getLiveSegmentsPath() {
-    return (null == liveSegmentsPath) ? defaultPath("segments") : liveSegmentsPath;
+    return liveSegmentsPath == null ? defaultPath("segments") : liveSegmentsPath;
   }
 
   public String getCoordinatorPath() {
-    return (null == coordinatorPath) ? defaultPath("coordinator") : coordinatorPath;
+    return coordinatorPath == null ? defaultPath("coordinator") : coordinatorPath;
   }
 
   public String getOverlordPath() {
@@ -62,11 +62,11 @@ public class ZkPathsConfig {
   }
 
   public String getLoadQueuePath() {
-    return (null == loadQueuePath) ? defaultPath("loadQueue") : loadQueuePath;
+    return loadQueuePath == null ? defaultPath("loadQueue") : loadQueuePath;
   }
 
   public String getConnectorPath() {
-    return (null == connectorPath) ? defaultPath("connector") : connectorPath;
+    return connectorPath == null ? defaultPath("connector") : connectorPath;
   }
 
   public String getInternalDiscoveryPath() {
@@ -79,7 +79,7 @@ public class ZkPathsConfig {
 
   @Override
   public boolean equals(Object other) {
-    if (null == other) {
+    if (other == null) {
       return false;
     }
     if (this == other) {
@@ -111,7 +111,6 @@ public class ZkPathsConfig {
     result = 31 * result + (liveSegmentsPath != null ? liveSegmentsPath.hashCode() : 0);
     result = 31 * result + (coordinatorPath != null ? coordinatorPath.hashCode() : 0);
     result = 31 * result + (loadQueuePath != null ? loadQueuePath.hashCode() : 0);
-    result = 31 * result + (connectorPath != null ? connectorPath.hashCode() : 0);
-    return result;
+    return 31 * result + (connectorPath != null ? connectorPath.hashCode() : 0);
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/ChatHandlerServerModule.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/ChatHandlerServerModule.java
index 4ba61fce20..2106b3f3a3 100644
--- a/server/src/main/java/org/apache/druid/server/initialization/jetty/ChatHandlerServerModule.java
+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/ChatHandlerServerModule.java
@@ -82,16 +82,16 @@ public class ChatHandlerServerModule implements Module {
         .to(Key.get(TLSServerConfig.class));
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public TaskIdResponseHeaderFilterHolder taskIdResponseHeaderFilterHolderBuilder(
       final DataSourceTaskIdHolder taskIdHolder) {
     return new TaskIdResponseHeaderFilterHolder(
         "/druid/worker/v1/chat/*", taskIdHolder.getTaskId());
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   @RemoteChatHandler
   public Server getServer(
       Injector injector,
diff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/CliIndexerServerModule.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/CliIndexerServerModule.java
index 6864093394..0994eccb8b 100644
--- a/server/src/main/java/org/apache/druid/server/initialization/jetty/CliIndexerServerModule.java
+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/CliIndexerServerModule.java
@@ -106,16 +106,16 @@ public class CliIndexerServerModule implements Module {
         .to(Key.get(TLSServerConfig.class));
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public TaskIdResponseHeaderFilterHolder taskIdResponseHeaderFilterHolderBuilder(
       final DataSourceTaskIdHolder taskIdHolder) {
     return new TaskIdResponseHeaderFilterHolder(
         "/druid/worker/v1/chat/*", taskIdHolder.getTaskId());
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   @RemoteChatHandler
   public Server getServer(
       Injector injector,
diff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java
index 196d683043..e45c3e2987 100644
--- a/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java
+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java
@@ -19,12 +19,12 @@
 
 package org.apache.druid.server.initialization.jetty;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.jaxrs.json.JacksonJsonProvider;
 import com.fasterxml.jackson.jaxrs.smile.JacksonSmileProvider;
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
-import com.google.common.primitives.Ints;
 import com.google.inject.Binder;
 import com.google.inject.Binding;
 import com.google.inject.Injector;
@@ -138,8 +138,8 @@ public class JettyServerModule extends JerseyServletModule {
     MetricsModule.register(binder, JettyMonitor.class);
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public Server getServer(
       final Injector injector,
       final Lifecycle lifecycle,
@@ -156,16 +156,16 @@ public class JettyServerModule extends JerseyServletModule {
         injector.getInstance(TLSCertificateChecker.class));
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public JacksonJsonProvider getJacksonJsonProvider(@Json ObjectMapper objectMapper) {
     final JacksonJsonProvider provider = new JacksonJsonProvider();
     provider.setMapper(objectMapper);
     return provider;
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public JacksonSmileProvider getJacksonSmileProvider(@Smile ObjectMapper objectMapper) {
     final JacksonSmileProvider provider = new JacksonSmileProvider();
     provider.setMapper(objectMapper);
@@ -334,7 +334,7 @@ public class JettyServerModule extends JerseyServletModule {
     for (ServerConnector connector : serverConnectors) {
       connectors[index++] = connector;
       connector.setIdleTimeout(
-          Ints.checkedCast(config.getMaxIdleTime().toStandardDuration().getMillis()));
+          Math.toIntExact(config.getMaxIdleTime().toStandardDuration().getMillis()));
       // workaround suggested in -
       // https://bugs.eclipse.org/bugs/show_bug.cgi?id=435322#c66 for jetty half open connection
       // issues during failovers
@@ -403,7 +403,7 @@ public class JettyServerModule extends JerseyServletModule {
             server.start();
             if (node.isEnableTlsPort()) {
               // Perform validation
-              Preconditions.checkNotNull(sslContextFactory);
+              requireNonNull(sslContextFactory);
               final SSLEngine sslEngine = sslContextFactory.newSSLEngine();
               if (sslEngine.getEnabledCipherSuites() == null
                   || sslEngine.getEnabledCipherSuites().length == 0) {
@@ -480,8 +480,8 @@ public class JettyServerModule extends JerseyServletModule {
     return numServerConnector * 8;
   }
 
-  @Provides
   @LazySingleton
+  @Provides
   public JettyMonitor getJettyMonitor(DataSourceTaskIdHolder dataSourceTaskIdHolder) {
     return new JettyMonitor(
         dataSourceTaskIdHolder.getDataSource(), dataSourceTaskIdHolder.getTaskId());
diff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/LimitRequestsFilter.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/LimitRequestsFilter.java
index 3aa30b6bf6..3b448676e6 100644
--- a/server/src/main/java/org/apache/druid/server/initialization/jetty/LimitRequestsFilter.java
+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/LimitRequestsFilter.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.initialization.jetty;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+
 import java.io.IOException;
 import java.util.concurrent.atomic.AtomicInteger;
 import javax.servlet.Filter;
@@ -37,7 +38,7 @@ public class LimitRequestsFilter implements Filter {
   private final AtomicInteger activeRequestsCount = new AtomicInteger();
 
   public LimitRequestsFilter(int maxActiveRequests) {
-    Preconditions.checkArgument(
+    checkArgument(
         maxActiveRequests > 0 && maxActiveRequests < Integer.MAX_VALUE,
         "maxActiveRequests must be > 0 and < Integer.MAX_VALUE.");
     this.maxActiveRequests = maxActiveRequests;
diff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/ResponseHeaderFilterHolder.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/ResponseHeaderFilterHolder.java
index 531303764e..23218805c3 100644
--- a/server/src/main/java/org/apache/druid/server/initialization/jetty/ResponseHeaderFilterHolder.java
+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/ResponseHeaderFilterHolder.java
@@ -19,7 +19,6 @@
 
 package org.apache.druid.server.initialization.jetty;
 
-import com.google.common.collect.ImmutableMap;
 import java.io.IOException;
 import java.util.EnumSet;
 import java.util.Enumeration;
@@ -54,7 +53,7 @@ public class ResponseHeaderFilterHolder implements ServletFilterHolder {
 
   @Override
   public Map<String, String> getInitParameters() {
-    return ImmutableMap.copyOf(headers);
+    return headers;
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/StandardResponseHeaderFilterHolder.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/StandardResponseHeaderFilterHolder.java
index 586768336d..6aa195d3e7 100644
--- a/server/src/main/java/org/apache/druid/server/initialization/jetty/StandardResponseHeaderFilterHolder.java
+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/StandardResponseHeaderFilterHolder.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.initialization.jetty;
 
+import static java.util.Collections.emptyMap;
+
 import com.google.common.collect.ImmutableSet;
 import com.google.inject.Inject;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.EnumSet;
 import java.util.Map;
 import java.util.Set;
@@ -101,7 +102,7 @@ public class StandardResponseHeaderFilterHolder implements ServletFilterHolder {
 
   @Override
   public Map<String, String> getInitParameters() {
-    return Collections.emptyMap();
+    return emptyMap();
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/server/listener/resource/AbstractListenerHandler.java b/server/src/main/java/org/apache/druid/server/listener/resource/AbstractListenerHandler.java
index 573a49cfcc..a48add8b1d 100644
--- a/server/src/main/java/org/apache/druid/server/listener/resource/AbstractListenerHandler.java
+++ b/server/src/main/java/org/apache/druid/server/listener/resource/AbstractListenerHandler.java
@@ -86,15 +86,14 @@ public abstract class AbstractListenerHandler<ObjType> implements ListenerHandle
           mapper.readValue(inputStream, JacksonUtils.TYPE_REFERENCE_MAP_STRING_OBJECT);
       // Now do the ACTUAL conversion
       inObjMap =
-          ImmutableMap.copyOf(
-              Maps.transformValues(
-                  tempMap,
-                  new Function<Object, ObjType>() {
-                    @Override
-                    public ObjType apply(Object input) {
-                      return mapper.convertValue(input, inObjTypeRef);
-                    }
-                  }));
+          Maps.transformValues(
+              tempMap,
+              new Function<Object, ObjType>() {
+                @Override
+                public ObjType apply(Object input) {
+                  return mapper.convertValue(input, inObjTypeRef);
+                }
+              });
     } catch (final IOException ex) {
       LOG.debug(ex, "Bad request");
       return Response.status(Response.Status.BAD_REQUEST)
diff --git a/server/src/main/java/org/apache/druid/server/listener/resource/ListenerResource.java b/server/src/main/java/org/apache/druid/server/listener/resource/ListenerResource.java
index d2bbdbde01..f67e233f38 100644
--- a/server/src/main/java/org/apache/druid/server/listener/resource/ListenerResource.java
+++ b/server/src/main/java/org/apache/druid/server/listener/resource/ListenerResource.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.listener.resource;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.jaxrs.smile.SmileMediaTypes;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Strings;
 import java.io.InputStream;
 import javax.servlet.http.HttpServletRequest;
@@ -67,14 +68,14 @@ public abstract class ListenerResource {
       final @Json ObjectMapper jsonMapper,
       final @Smile ObjectMapper smileMapper,
       final ListenerHandler handler) {
-    this.jsonMapper = Preconditions.checkNotNull(jsonMapper, "jsonMapper");
-    this.smileMapper = Preconditions.checkNotNull(smileMapper, "smileMapper");
-    this.handler = Preconditions.checkNotNull(handler, "listener handler");
+    this.jsonMapper = requireNonNull(jsonMapper, "jsonMapper");
+    this.smileMapper = requireNonNull(smileMapper, "smileMapper");
+    this.handler = requireNonNull(handler, "listener handler");
   }
 
+  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @POST
   @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
-  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response serviceAnnouncementPOSTAll(
       final InputStream inputStream,
       final @Context HttpServletRequest req // used only to get request content-type
@@ -90,10 +91,10 @@ public abstract class ListenerResource {
     }
   }
 
+  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   @POST
   @Path("/updates")
   @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
-  @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response serviceAnnouncementHandleUpdates(
       final InputStream inputStream,
       final @Context HttpServletRequest req // used only to get request content-type
@@ -120,8 +121,8 @@ public abstract class ListenerResource {
     }
   }
 
-  @Path("/{id}")
   @GET
+  @Path("/{id}")
   @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response serviceAnnouncementGET(final @PathParam("id") String id) {
     if (Strings.isNullOrEmpty(id)) {
@@ -135,8 +136,8 @@ public abstract class ListenerResource {
     }
   }
 
-  @Path("/{id}")
   @DELETE
+  @Path("/{id}")
   @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response serviceAnnouncementDELETE(final @PathParam("id") String id) {
     if (Strings.isNullOrEmpty(id)) {
@@ -150,8 +151,8 @@ public abstract class ListenerResource {
     }
   }
 
-  @Path("/{id}")
   @POST
+  @Path("/{id}")
   @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})
   public Response serviceAnnouncementPOST(
       final @PathParam("id") String id,
diff --git a/server/src/main/java/org/apache/druid/server/log/DefaultRequestLogEvent.java b/server/src/main/java/org/apache/druid/server/log/DefaultRequestLogEvent.java
index 02499760c7..eeac669d8d 100644
--- a/server/src/main/java/org/apache/druid/server/log/DefaultRequestLogEvent.java
+++ b/server/src/main/java/org/apache/druid/server/log/DefaultRequestLogEvent.java
@@ -52,7 +52,7 @@ public final class DefaultRequestLogEvent implements RequestLogEvent {
    * Override {@link JsonValue} serialization, instead use annotations to include type information
    * for polymorphic {@link Query} objects.
    */
-  @JsonValue(value = false)
+  @JsonValue(false)
   @Override
   public EventMap toMap() {
     final EventMap.Builder builder =
@@ -72,8 +72,8 @@ public final class DefaultRequestLogEvent implements RequestLogEvent {
     return builder.build();
   }
 
-  @Override
   @JsonProperty("feed")
+  @Override
   public String getFeed() {
     return feed;
   }
diff --git a/server/src/main/java/org/apache/druid/server/log/FileRequestLogger.java b/server/src/main/java/org/apache/druid/server/log/FileRequestLogger.java
index 35c0c3f849..d9a1ab3b24 100644
--- a/server/src/main/java/org/apache/druid/server/log/FileRequestLogger.java
+++ b/server/src/main/java/org/apache/druid/server/log/FileRequestLogger.java
@@ -19,14 +19,15 @@
 
 package org.apache.druid.server.log;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import java.io.File;
 import java.io.FileNotFoundException;
 import java.io.FileOutputStream;
 import java.io.IOException;
 import java.io.OutputStreamWriter;
-import java.nio.charset.StandardCharsets;
 import java.util.concurrent.Callable;
 import java.util.concurrent.ScheduledExecutorService;
 import org.apache.druid.java.util.common.DateTimes;
@@ -69,7 +70,7 @@ public class FileRequestLogger implements RequestLogger {
     this.baseDir = baseDir;
     this.filePattern = DateTimeFormat.forPattern(filePattern);
     this.durationToRetain = durationToRetain;
-    Preconditions.checkArgument(
+    checkArgument(
         this.durationToRetain == null
             || this.durationToRetain.compareTo(Duration.standardDays(1)) >= 0,
         "request logs retention period must be atleast P1D");
@@ -157,8 +158,7 @@ public class FileRequestLogger implements RequestLogger {
 
   private OutputStreamWriter getFileWriter() throws FileNotFoundException {
     return new OutputStreamWriter(
-        new FileOutputStream(new File(baseDir, filePattern.print(currentDay)), true),
-        StandardCharsets.UTF_8);
+        new FileOutputStream(new File(baseDir, filePattern.print(currentDay)), true), UTF_8);
   }
 
   @LifecycleStop
diff --git a/server/src/main/java/org/apache/druid/server/lookup/cache/LookupCoordinatorManager.java b/server/src/main/java/org/apache/druid/server/lookup/cache/LookupCoordinatorManager.java
index 641421422d..4b93bd31f7 100644
--- a/server/src/main/java/org/apache/druid/server/lookup/cache/LookupCoordinatorManager.java
+++ b/server/src/main/java/org/apache/druid/server/lookup/cache/LookupCoordinatorManager.java
@@ -19,12 +19,14 @@
 
 package org.apache.druid.server.lookup.cache;
 
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.jaxrs.smile.SmileMediaTypes;
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.Collections2;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Sets;
@@ -174,7 +176,7 @@ public class LookupCoordinatorManager {
   public boolean updateLookups(
       final Map<String, Map<String, LookupExtractorFactoryMapContainer>> updateSpec,
       AuditInfo auditInfo) {
-    Preconditions.checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
+    checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
 
     if (updateSpec.isEmpty() && lookupMapConfigRef.get() != null) {
       return true;
@@ -187,7 +189,7 @@ public class LookupCoordinatorManager {
         updateSpec.entrySet()) {
       for (Map.Entry<String, LookupExtractorFactoryMapContainer> e :
           tierEntry.getValue().entrySet()) {
-        Preconditions.checkNotNull(
+        checkNotNull(
             e.getValue().getVersion(),
             "lookup [%s]:[%s] does not have version.",
             tierEntry.getKey(),
@@ -246,12 +248,12 @@ public class LookupCoordinatorManager {
   }
 
   public Map<String, Map<String, LookupExtractorFactoryMapContainer>> getKnownLookups() {
-    Preconditions.checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
+    checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
     return lookupMapConfigRef.get();
   }
 
   public boolean deleteTier(final String tier, AuditInfo auditInfo) {
-    Preconditions.checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
+    checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
 
     synchronized (this) {
       final Map<String, Map<String, LookupExtractorFactoryMapContainer>> priorSpec =
@@ -273,7 +275,7 @@ public class LookupCoordinatorManager {
   }
 
   public boolean deleteLookup(final String tier, final String lookup, AuditInfo auditInfo) {
-    Preconditions.checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
+    checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
 
     synchronized (this) {
       final Map<String, Map<String, LookupExtractorFactoryMapContainer>> priorSpec =
@@ -309,12 +311,12 @@ public class LookupCoordinatorManager {
   }
 
   public Set<String> discoverTiers() {
-    Preconditions.checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
+    checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
     return lookupNodeDiscovery.getAllTiers();
   }
 
   public Collection<HostAndPort> discoverNodesInTier(String tier) {
-    Preconditions.checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
+    checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
     return Collections2.transform(
         lookupNodeDiscovery.getNodesInTier(tier),
         new Function<HostAndPortWithScheme, HostAndPort>() {
@@ -327,7 +329,7 @@ public class LookupCoordinatorManager {
 
   public Map<HostAndPort, LookupsState<LookupExtractorFactoryMapContainer>>
       getLastKnownLookupsStateOnNodes() {
-    Preconditions.checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
+    checkState(lifecycleLock.awaitStarted(5, TimeUnit.SECONDS), "not started");
     return knownOldState.get();
   }
 
@@ -512,8 +514,8 @@ public class LookupCoordinatorManager {
     return convertedTierLookups;
   }
 
-  @VisibleForTesting
   @SuppressWarnings("MapEntry")
+  @VisibleForTesting
   void lookupManagementLoop() {
     // Sanity check for if we are shutting down
     if (Thread.currentThread().isInterrupted()
@@ -695,8 +697,7 @@ public class LookupCoordinatorManager {
     toDrop.addAll(currLookupsStateOnNode.getCurrent().keySet());
     toDrop.addAll(currLookupsStateOnNode.getToLoad().keySet());
     toDrop = Sets.difference(toDrop, currLookupsStateOnNode.getToDrop());
-    toDrop = Sets.difference(toDrop, nodeTierLookupsToBe.keySet());
-    return toDrop;
+    return Sets.difference(toDrop, nodeTierLookupsToBe.keySet());
   }
 
   static URL getLookupsURL(HostAndPortWithScheme druidNode) throws MalformedURLException {
diff --git a/server/src/main/java/org/apache/druid/server/lookup/cache/LookupExtractorFactoryMapContainer.java b/server/src/main/java/org/apache/druid/server/lookup/cache/LookupExtractorFactoryMapContainer.java
index 6e29684888..b702f72638 100644
--- a/server/src/main/java/org/apache/druid/server/lookup/cache/LookupExtractorFactoryMapContainer.java
+++ b/server/src/main/java/org/apache/druid/server/lookup/cache/LookupExtractorFactoryMapContainer.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.lookup.cache;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.Map;
 import java.util.Objects;
 import org.apache.druid.java.util.common.guava.Comparators;
@@ -40,8 +41,7 @@ public class LookupExtractorFactoryMapContainer {
       @JsonProperty("version") String version,
       @JsonProperty("lookupExtractorFactory") Map<String, Object> lookupExtractorFactory) {
     this.version = version;
-    this.lookupExtractorFactory =
-        Preconditions.checkNotNull(lookupExtractorFactory, "null factory");
+    this.lookupExtractorFactory = requireNonNull(lookupExtractorFactory, "null factory");
   }
 
   @JsonProperty
diff --git a/server/src/main/java/org/apache/druid/server/metrics/MetricsModule.java b/server/src/main/java/org/apache/druid/server/metrics/MetricsModule.java
index 3685e4ea12..8c838a1af7 100644
--- a/server/src/main/java/org/apache/druid/server/metrics/MetricsModule.java
+++ b/server/src/main/java/org/apache/druid/server/metrics/MetricsModule.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.metrics;
 
+import static java.util.stream.Collectors.joining;
+
 import com.google.common.base.Supplier;
 import com.google.common.collect.Iterables;
 import com.google.inject.Binder;
@@ -34,7 +36,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.discovery.NodeRole;
 import org.apache.druid.guice.DruidBinders;
 import org.apache.druid.guice.JsonConfigProvider;
@@ -94,8 +95,8 @@ public class MetricsModule implements Module {
         .asEagerSingleton();
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public MonitorScheduler getMonitorScheduler(
       Supplier<DruidMonitorSchedulerConfig> config,
       MonitorsConfig monitorsConfig,
@@ -113,9 +114,7 @@ public class MetricsModule implements Module {
       log.info(
           "Loaded %d monitors: %s",
           monitors.size(),
-          monitors.stream()
-              .map(monitor -> monitor.getClass().getName())
-              .collect(Collectors.joining(", ")));
+          monitors.stream().map(monitor -> monitor.getClass().getName()).collect(joining(", ")));
     }
 
     if (ClockDriftSafeMonitorScheduler.class
@@ -137,8 +136,8 @@ public class MetricsModule implements Module {
     }
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public JvmMonitor getJvmMonitor(DataSourceTaskIdHolder dataSourceTaskIdHolder) {
     Map<String, String[]> dimensions =
         MonitorsConfig.mapOfDatasourceAndTaskID(
@@ -146,8 +145,8 @@ public class MetricsModule implements Module {
     return new JvmMonitor(dimensions);
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public JvmCpuMonitor getJvmCpuMonitor(DataSourceTaskIdHolder dataSourceTaskIdHolder) {
     Map<String, String[]> dimensions =
         MonitorsConfig.mapOfDatasourceAndTaskID(
@@ -155,8 +154,8 @@ public class MetricsModule implements Module {
     return new JvmCpuMonitor(dimensions);
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public JvmThreadsMonitor getJvmThreadsMonitor(DataSourceTaskIdHolder dataSourceTaskIdHolder) {
     Map<String, String[]> dimensions =
         MonitorsConfig.mapOfDatasourceAndTaskID(
@@ -164,8 +163,8 @@ public class MetricsModule implements Module {
     return new JvmThreadsMonitor(dimensions);
   }
 
-  @Provides
   @ManageLifecycle
+  @Provides
   public SysMonitor getSysMonitor(
       DataSourceTaskIdHolder dataSourceTaskIdHolder, @Self Set<NodeRole> nodeRoles) {
     if (nodeRoles.contains(NodeRole.PEON)) {
diff --git a/server/src/main/java/org/apache/druid/server/router/AvaticaConnectionBalancer.java b/server/src/main/java/org/apache/druid/server/router/AvaticaConnectionBalancer.java
index cf06ad27dc..c1c9bfab80 100644
--- a/server/src/main/java/org/apache/druid/server/router/AvaticaConnectionBalancer.java
+++ b/server/src/main/java/org/apache/druid/server/router/AvaticaConnectionBalancer.java
@@ -29,15 +29,12 @@ import org.apache.druid.client.selector.Server;
     use = JsonTypeInfo.Id.NAME,
     property = "type",
     defaultImpl = RendezvousHashAvaticaConnectionBalancer.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(
-          name = "rendezvousHash",
-          value = RendezvousHashAvaticaConnectionBalancer.class),
-      @JsonSubTypes.Type(
-          name = "consistentHash",
-          value = ConsistentHashAvaticaConnectionBalancer.class)
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(
+      name = "consistentHash",
+      value = ConsistentHashAvaticaConnectionBalancer.class),
+  @JsonSubTypes.Type(name = "rendezvousHash", value = RendezvousHashAvaticaConnectionBalancer.class)
+})
 public interface AvaticaConnectionBalancer {
   /**
    * @param servers Servers to balance across
diff --git a/server/src/main/java/org/apache/druid/server/router/ConsistentHasher.java b/server/src/main/java/org/apache/druid/server/router/ConsistentHasher.java
index f89418d2ad..de40b87ebc 100644
--- a/server/src/main/java/org/apache/druid/server/router/ConsistentHasher.java
+++ b/server/src/main/java/org/apache/druid/server/router/ConsistentHasher.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.server.router;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.common.hash.HashFunction;
 import com.google.common.hash.Hashing;
 import it.unimi.dsi.fastutil.longs.Long2ObjectMap;
 import it.unimi.dsi.fastutil.longs.Long2ObjectRBTreeMap;
 import it.unimi.dsi.fastutil.longs.Long2ObjectSortedMap;
-import java.nio.charset.StandardCharsets;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
@@ -110,7 +111,7 @@ public class ConsistentHasher {
     long[] hashes = new long[REPLICATION_FACTOR];
     for (int i = 0; i < REPLICATION_FACTOR; i++) {
       String vnode = key + "-" + i;
-      hashes[i] = hashFn.hashString(vnode, StandardCharsets.UTF_8).asLong();
+      hashes[i] = hashFn.hashString(vnode, UTF_8).asLong();
     }
 
     nodeKeyHashes.put(key, hashes);
diff --git a/server/src/main/java/org/apache/druid/server/router/RendezvousHasher.java b/server/src/main/java/org/apache/druid/server/router/RendezvousHasher.java
index 8c16355483..c35d0f5d95 100644
--- a/server/src/main/java/org/apache/druid/server/router/RendezvousHasher.java
+++ b/server/src/main/java/org/apache/druid/server/router/RendezvousHasher.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.router;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.common.hash.HashFunction;
 import com.google.common.hash.Hashing;
-import java.nio.charset.StandardCharsets;
 import java.util.Set;
 
 /**
@@ -41,12 +42,7 @@ public class RendezvousHasher {
 
     for (String nodeId : nodeIds) {
       long combinedHash =
-          HASH_FN
-              .newHasher()
-              .putString(nodeId, StandardCharsets.UTF_8)
-              .putBytes(key)
-              .hash()
-              .asLong();
+          HASH_FN.newHasher().putString(nodeId, UTF_8).putBytes(key).hash().asLong();
       if (maxNode == null) {
         maxHash = combinedHash;
         maxNode = nodeId;
diff --git a/server/src/main/java/org/apache/druid/server/scheduling/HiLoQueryLaningStrategy.java b/server/src/main/java/org/apache/druid/server/scheduling/HiLoQueryLaningStrategy.java
index 252046fba5..cef8e16c66 100644
--- a/server/src/main/java/org/apache/druid/server/scheduling/HiLoQueryLaningStrategy.java
+++ b/server/src/main/java/org/apache/druid/server/scheduling/HiLoQueryLaningStrategy.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.server.scheduling;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;
 import it.unimi.dsi.fastutil.objects.Object2IntMap;
 import java.util.Optional;
@@ -44,8 +46,8 @@ public class HiLoQueryLaningStrategy implements QueryLaningStrategy {
 
   @JsonCreator
   public HiLoQueryLaningStrategy(@JsonProperty("maxLowPercent") Integer maxLowPercent) {
-    this.maxLowPercent = Preconditions.checkNotNull(maxLowPercent, "maxLowPercent must be set");
-    Preconditions.checkArgument(
+    this.maxLowPercent = requireNonNull(maxLowPercent, "maxLowPercent must be set");
+    checkArgument(
         0 < maxLowPercent && maxLowPercent <= 100, "maxLowPercent must be in the range 1 to 100");
   }
 
@@ -64,7 +66,7 @@ public class HiLoQueryLaningStrategy implements QueryLaningStrategy {
     // we make sure that priority has been set.
     Integer priority = null;
     final QueryContext queryContext = theQuery.context();
-    if (null != queryContext.get(QueryContexts.PRIORITY_KEY)) {
+    if (queryContext.get(QueryContexts.PRIORITY_KEY) != null) {
       priority = queryContext.getPriority();
     }
     final String lane = queryContext.getLane();
diff --git a/server/src/main/java/org/apache/druid/server/scheduling/ManualQueryLaningStrategy.java b/server/src/main/java/org/apache/druid/server/scheduling/ManualQueryLaningStrategy.java
index 7aa461f650..5a71b115b6 100644
--- a/server/src/main/java/org/apache/druid/server/scheduling/ManualQueryLaningStrategy.java
+++ b/server/src/main/java/org/apache/druid/server/scheduling/ManualQueryLaningStrategy.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.server.scheduling;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;
 import it.unimi.dsi.fastutil.objects.Object2IntMap;
 import java.util.Map;
@@ -42,22 +44,22 @@ public class ManualQueryLaningStrategy implements QueryLaningStrategy {
   public ManualQueryLaningStrategy(
       @JsonProperty("lanes") Map<String, Integer> lanes,
       @JsonProperty("isLimitPercent") @Nullable Boolean isLimitPercent) {
-    this.lanes = Preconditions.checkNotNull(lanes, "lanes must be set");
+    this.lanes = requireNonNull(lanes, "lanes must be set");
     this.isLimitPercent = isLimitPercent != null ? isLimitPercent : false;
-    Preconditions.checkArgument(lanes.size() > 0, "lanes must define at least one lane");
-    Preconditions.checkArgument(
+    checkArgument(lanes.size() > 0, "lanes must define at least one lane");
+    checkArgument(
         lanes.values().stream().allMatch(x -> this.isLimitPercent ? 0 < x && x <= 100 : x > 0),
         this.isLimitPercent
             ? "All lane limits must be in the range 1 to 100"
             : "All lane limits must be greater than 0");
-    Preconditions.checkArgument(
+    checkArgument(
         lanes.keySet().stream().noneMatch(QueryScheduler.TOTAL::equals),
         "Lane cannot be named 'total'");
 
     // 'default' has special meaning for resilience4j bulkhead used by query scheduler, this
     // restriction
     // can potentially be relaxed if we ever change enforcement mechanism
-    Preconditions.checkArgument(
+    checkArgument(
         lanes.keySet().stream().noneMatch("default"::equals), "Lane cannot be named 'default'");
   }
 
diff --git a/server/src/main/java/org/apache/druid/server/scheduling/ThresholdBasedQueryPrioritizationStrategy.java b/server/src/main/java/org/apache/druid/server/scheduling/ThresholdBasedQueryPrioritizationStrategy.java
index a23534ff03..4a270f1739 100644
--- a/server/src/main/java/org/apache/druid/server/scheduling/ThresholdBasedQueryPrioritizationStrategy.java
+++ b/server/src/main/java/org/apache/druid/server/scheduling/ThresholdBasedQueryPrioritizationStrategy.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.scheduling;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import com.google.common.base.Preconditions;
 import java.util.Optional;
 import java.util.Set;
 import javax.annotation.Nullable;
@@ -62,7 +63,7 @@ public class ThresholdBasedQueryPrioritizationStrategy implements QueryPrioritiz
         durationThresholdString == null
             ? Optional.empty()
             : Optional.of(new Period(durationThresholdString).toStandardDuration());
-    Preconditions.checkArgument(
+    checkArgument(
         segmentCountThreshold != null
             || periodThreshold.isPresent()
             || durationThreshold.isPresent(),
@@ -75,17 +76,17 @@ public class ThresholdBasedQueryPrioritizationStrategy implements QueryPrioritiz
     Query<T> theQuery = query.getQuery();
     final boolean violatesPeriodThreshold =
         periodThreshold
-            .map(
+            .filter(
                 duration -> {
                   final DateTime periodThresholdStartDate = DateTimes.nowUtc().minus(duration);
                   return theQuery.getIntervals().stream()
                       .anyMatch(interval -> interval.getStart().isBefore(periodThresholdStartDate));
                 })
-            .orElse(false);
+            .isPresent();
     final boolean violatesDurationThreshold =
         durationThreshold
-            .map(duration -> theQuery.getDuration().isLongerThan(duration))
-            .orElse(false);
+            .filter(duration -> theQuery.getDuration().isLongerThan(duration))
+            .isPresent();
     boolean violatesSegmentThreshold = segments.size() > segmentCountThreshold;
 
     if (violatesPeriodThreshold || violatesDurationThreshold || violatesSegmentThreshold) {
diff --git a/server/src/main/java/org/apache/druid/server/security/Action.java b/server/src/main/java/org/apache/druid/server/security/Action.java
index 725ea83fef..df7640e42e 100644
--- a/server/src/main/java/org/apache/druid/server/security/Action.java
+++ b/server/src/main/java/org/apache/druid/server/security/Action.java
@@ -26,7 +26,7 @@ public enum Action {
   READ,
   WRITE;
 
-  @JsonCreator
+  @JsonCreator(mode = JsonCreator.Mode.DELEGATING)
   public static Action fromString(String name) {
     if (name == null) {
       return null;
diff --git a/server/src/main/java/org/apache/druid/server/security/AuthConfig.java b/server/src/main/java/org/apache/druid/server/security/AuthConfig.java
index 8b6c881d37..bb09a8607d 100644
--- a/server/src/main/java/org/apache/druid/server/security/AuthConfig.java
+++ b/server/src/main/java/org/apache/druid/server/security/AuthConfig.java
@@ -19,10 +19,12 @@
 
 package org.apache.druid.server.security;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptySet;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.collect.ImmutableSet;
-import java.util.Collections;
 import java.util.List;
 import java.util.Objects;
 import java.util.Set;
@@ -102,11 +104,10 @@ public class AuthConfig {
       @JsonProperty("enableInputSourceSecurity") boolean enableInputSourceSecurity) {
     this.authenticatorChain = authenticatorChain;
     this.authorizers = authorizers;
-    this.unsecuredPaths = unsecuredPaths == null ? Collections.emptyList() : unsecuredPaths;
+    this.unsecuredPaths = unsecuredPaths == null ? emptyList() : unsecuredPaths;
     this.allowUnauthenticatedHttpOptions = allowUnauthenticatedHttpOptions;
     this.authorizeQueryContextParams = authorizeQueryContextParams;
-    this.unsecuredContextKeys =
-        unsecuredContextKeys == null ? Collections.emptySet() : unsecuredContextKeys;
+    this.unsecuredContextKeys = unsecuredContextKeys == null ? emptySet() : unsecuredContextKeys;
     this.securedContextKeys = securedContextKeys;
     this.enableInputSourceSecurity = enableInputSourceSecurity;
   }
diff --git a/server/src/main/java/org/apache/druid/server/security/Authenticator.java b/server/src/main/java/org/apache/druid/server/security/Authenticator.java
index f085a2745e..7f95381e58 100644
--- a/server/src/main/java/org/apache/druid/server/security/Authenticator.java
+++ b/server/src/main/java/org/apache/druid/server/security/Authenticator.java
@@ -30,14 +30,13 @@ import org.apache.druid.server.initialization.jetty.ServletFilterHolder;
 import org.eclipse.jetty.client.api.Request;
 
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = AuthConfig.ALLOW_ALL_NAME, value = AllowAllAuthenticator.class),
-      @JsonSubTypes.Type(name = AuthConfig.ANONYMOUS_NAME, value = AnonymousAuthenticator.class),
-      @JsonSubTypes.Type(
-          name = AuthConfig.TRUSTED_DOMAIN_NAME,
-          value = TrustedDomainAuthenticator.class),
-    })
+@JsonSubTypes({
+  @JsonSubTypes.Type(name = AuthConfig.ALLOW_ALL_NAME, value = AllowAllAuthenticator.class),
+  @JsonSubTypes.Type(name = AuthConfig.ANONYMOUS_NAME, value = AnonymousAuthenticator.class),
+  @JsonSubTypes.Type(
+      name = AuthConfig.TRUSTED_DOMAIN_NAME,
+      value = TrustedDomainAuthenticator.class)
+})
 /**
  * This interface is essentially a ServletFilterHolder with additional requirements on the
  * getFilter() method contract, plus:
diff --git a/server/src/main/java/org/apache/druid/server/security/AuthenticatorMapper.java b/server/src/main/java/org/apache/druid/server/security/AuthenticatorMapper.java
index cbd4e40a58..b9dc2c6661 100644
--- a/server/src/main/java/org/apache/druid/server/security/AuthenticatorMapper.java
+++ b/server/src/main/java/org/apache/druid/server/security/AuthenticatorMapper.java
@@ -19,7 +19,7 @@
 
 package org.apache.druid.server.security;
 
-import com.google.common.collect.Lists;
+import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
@@ -35,6 +35,6 @@ public class AuthenticatorMapper {
   }
 
   public List<Authenticator> getAuthenticatorChain() {
-    return Lists.newArrayList(authenticatorMap.values());
+    return new ArrayList<>(authenticatorMap.values());
   }
 }
diff --git a/server/src/main/java/org/apache/druid/server/security/AuthorizationUtils.java b/server/src/main/java/org/apache/druid/server/security/AuthorizationUtils.java
index d8d394f924..0c1c5f6c1e 100644
--- a/server/src/main/java/org/apache/druid/server/security/AuthorizationUtils.java
+++ b/server/src/main/java/org/apache/druid/server/security/AuthorizationUtils.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server.security;
 
+import static java.util.Collections.singletonList;
+
 import com.google.common.base.Function;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -53,8 +54,7 @@ public class AuthorizationUtils {
       final HttpServletRequest request,
       final ResourceAction resourceAction,
       final AuthorizerMapper authorizerMapper) {
-    return authorizeAllResourceActions(
-        request, Collections.singletonList(resourceAction), authorizerMapper);
+    return authorizeAllResourceActions(request, singletonList(resourceAction), authorizerMapper);
   }
 
   /**
@@ -244,33 +244,29 @@ public class AuthorizationUtils {
     }
 
     final Map<ResourceAction, Access> resultCache = new HashMap<>();
-    final Iterable<ResType> filteredResources =
-        Iterables.filter(
-            resources,
-            resource -> {
-              final Iterable<ResourceAction> resourceActions =
-                  resourceActionGenerator.apply(resource);
-              if (resourceActions == null) {
-                return false;
-              }
-              if (authorizer instanceof AllowAllAuthorizer) {
-                return true;
-              }
-              for (ResourceAction resourceAction : resourceActions) {
-                Access access =
-                    resultCache.computeIfAbsent(
-                        resourceAction,
-                        ra ->
-                            authorizer.authorize(
-                                authenticationResult, ra.getResource(), ra.getAction()));
-                if (!access.isAllowed()) {
-                  return false;
-                }
-              }
-              return true;
-            });
-
-    return filteredResources;
+    return Iterables.filter(
+        resources,
+        resource -> {
+          final Iterable<ResourceAction> resourceActions = resourceActionGenerator.apply(resource);
+          if (resourceActions == null) {
+            return false;
+          }
+          if (authorizer instanceof AllowAllAuthorizer) {
+            return true;
+          }
+          for (ResourceAction resourceAction : resourceActions) {
+            Access access =
+                resultCache.computeIfAbsent(
+                    resourceAction,
+                    ra ->
+                        authorizer.authorize(
+                            authenticationResult, ra.getResource(), ra.getAction()));
+            if (!access.isAllowed()) {
+              return false;
+            }
+          }
+          return true;
+        });
   }
 
   /**
@@ -324,7 +320,7 @@ public class AuthorizationUtils {
                   resourceActionGenerator,
                   authorizerMapper));
 
-      if (filteredList.size() > 0) {
+      if (!filteredList.isEmpty()) {
         filteredResources.put(entry.getKey(), filteredList);
       }
     }
diff --git a/server/src/main/java/org/apache/druid/server/security/Authorizer.java b/server/src/main/java/org/apache/druid/server/security/Authorizer.java
index 9deaf743f8..f8939778bd 100644
--- a/server/src/main/java/org/apache/druid/server/security/Authorizer.java
+++ b/server/src/main/java/org/apache/druid/server/security/Authorizer.java
@@ -24,9 +24,7 @@ import com.fasterxml.jackson.annotation.JsonTypeInfo;
 
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
 @JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = AuthConfig.ALLOW_ALL_NAME, value = AllowAllAuthorizer.class)
-    })
+    @JsonSubTypes.Type(name = AuthConfig.ALLOW_ALL_NAME, value = AllowAllAuthorizer.class))
 /**
  * An Authorizer is responsible for performing authorization checks for resource accesses.
  *
diff --git a/server/src/main/java/org/apache/druid/server/security/Escalator.java b/server/src/main/java/org/apache/druid/server/security/Escalator.java
index 3fff7956d0..a82a930410 100644
--- a/server/src/main/java/org/apache/druid/server/security/Escalator.java
+++ b/server/src/main/java/org/apache/druid/server/security/Escalator.java
@@ -29,10 +29,7 @@ import org.apache.druid.java.util.http.client.HttpClient;
  * like Authenticators.
  */
 @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type", defaultImpl = NoopEscalator.class)
-@JsonSubTypes(
-    value = {
-      @JsonSubTypes.Type(name = "noop", value = NoopEscalator.class),
-    })
+@JsonSubTypes(@JsonSubTypes.Type(name = "noop", value = NoopEscalator.class))
 public interface Escalator {
   /**
    * Return a client that sends requests with the format/information necessary to authenticate
diff --git a/server/src/main/java/org/apache/druid/server/security/PreResponseAuthorizationCheckFilter.java b/server/src/main/java/org/apache/druid/server/security/PreResponseAuthorizationCheckFilter.java
index 116b1b738d..340091287c 100644
--- a/server/src/main/java/org/apache/druid/server/security/PreResponseAuthorizationCheckFilter.java
+++ b/server/src/main/java/org/apache/druid/server/security/PreResponseAuthorizationCheckFilter.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.security;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.io.IOException;
 import java.io.OutputStream;
-import java.nio.charset.StandardCharsets;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -181,7 +182,7 @@ public class PreResponseAuthorizationCheckFilter implements Filter {
     resp.setContentType("application/json");
     resp.setCharacterEncoding("UTF-8");
     try {
-      outputStream.write(errorJson.getBytes(StandardCharsets.UTF_8));
+      outputStream.write(errorJson.getBytes(UTF_8));
     } catch (IOException ioe) {
       log.error("Can't get writer from HTTP response.");
     }
diff --git a/server/src/main/java/org/apache/druid/server/security/ResourceAction.java b/server/src/main/java/org/apache/druid/server/security/ResourceAction.java
index cfb79039cb..ee2ae15dc4 100644
--- a/server/src/main/java/org/apache/druid/server/security/ResourceAction.java
+++ b/server/src/main/java/org/apache/druid/server/security/ResourceAction.java
@@ -64,8 +64,7 @@ public class ResourceAction {
   @Override
   public int hashCode() {
     int result = getResource().hashCode();
-    result = 31 * result + getAction().hashCode();
-    return result;
+    return 31 * result + getAction().hashCode();
   }
 
   @Override
diff --git a/server/src/main/java/org/apache/druid/server/security/SecuritySanityCheckFilter.java b/server/src/main/java/org/apache/druid/server/security/SecuritySanityCheckFilter.java
index 3f7e4855b0..bb9bb77f70 100644
--- a/server/src/main/java/org/apache/druid/server/security/SecuritySanityCheckFilter.java
+++ b/server/src/main/java/org/apache/druid/server/security/SecuritySanityCheckFilter.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.security;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.io.IOException;
 import java.io.OutputStream;
-import java.nio.charset.StandardCharsets;
 import javax.servlet.Filter;
 import javax.servlet.FilterChain;
 import javax.servlet.FilterConfig;
@@ -81,7 +82,7 @@ public class SecuritySanityCheckFilter implements Filter {
     resp.setContentType("application/json");
     resp.setCharacterEncoding("UTF-8");
     try {
-      outputStream.write(errorJson.getBytes(StandardCharsets.UTF_8));
+      outputStream.write(errorJson.getBytes(UTF_8));
     } catch (IOException ioe) {
       log.error("Can't get writer from HTTP response.");
     }
diff --git a/server/src/main/java/org/apache/druid/server/security/TLSUtils.java b/server/src/main/java/org/apache/druid/server/security/TLSUtils.java
index eb7dad5e65..ec03e208dc 100644
--- a/server/src/main/java/org/apache/druid/server/security/TLSUtils.java
+++ b/server/src/main/java/org/apache/druid/server/security/TLSUtils.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.server.security;
 
-import com.google.common.base.Preconditions;
+import static java.util.Objects.requireNonNull;
+
 import java.io.IOException;
 import java.io.InputStream;
 import java.nio.file.Files;
@@ -129,7 +130,7 @@ public class TLSUtils {
     }
 
     public SSLContext build() {
-      Preconditions.checkNotNull(trustStorePath, "must specify a trustStorePath");
+      requireNonNull(trustStorePath, "must specify a trustStorePath");
 
       return createSSLContext(
           protocol,
diff --git a/server/src/main/java/org/apache/druid/server/security/TrustedDomainAuthenticator.java b/server/src/main/java/org/apache/druid/server/security/TrustedDomainAuthenticator.java
index 85c6ac248e..afc8b4b9c1 100644
--- a/server/src/main/java/org/apache/druid/server/security/TrustedDomainAuthenticator.java
+++ b/server/src/main/java/org/apache/druid/server/security/TrustedDomainAuthenticator.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.security;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.annotation.JsonTypeName;
-import com.google.common.base.Preconditions;
 import com.google.common.base.Strings;
 import java.io.IOException;
 import java.util.EnumSet;
@@ -57,7 +58,7 @@ public class TrustedDomainAuthenticator implements Authenticator {
       @JsonProperty("useForwardedHeaders") Boolean useForwardedHeaders,
       @JsonProperty("authorizerName") String authorizerName,
       @JsonProperty("identity") String identity) {
-    Preconditions.checkArgument(!Strings.isNullOrEmpty(domain), "Invalid domain name %s", domain);
+    checkArgument(!Strings.isNullOrEmpty(domain), "Invalid domain name %s", domain);
     this.domain = domain;
     this.useForwardedHeaders =
         useForwardedHeaders == null ? DEFAULT_USE_FORWARDED_HEADERS : useForwardedHeaders;
@@ -127,14 +128,14 @@ public class TrustedDomainAuthenticator implements Authenticator {
     return null;
   }
 
-  @Override
   @Nullable
+  @Override
   public String getAuthChallengeHeader() {
     return null;
   }
 
-  @Override
   @Nullable
+  @Override
   public AuthenticationResult authenticateJDBCContext(Map<String, Object> context) {
     return null;
   }
diff --git a/server/src/test/java/org/apache/druid/catalog/model/TableMetadataTest.java b/server/src/test/java/org/apache/druid/catalog/model/TableMetadataTest.java
index 21c23ea18a..3f659eed82 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/TableMetadataTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/TableMetadataTest.java
@@ -74,13 +74,13 @@ public class TableMetadataTest {
     {
       // Missing schema
       TableMetadata table = TableMetadata.newTable(TableId.of(null, "foo"), spec);
-      assertThrows(IAE.class, () -> table.validate());
+      assertThrows(IAE.class, table::validate);
     }
 
     {
       // Missing table name
       TableMetadata table = TableMetadata.newTable(TableId.of(TableId.DRUID_SCHEMA, null), spec);
-      assertThrows(IAE.class, () -> table.validate());
+      assertThrows(IAE.class, table::validate);
     }
   }
 
diff --git a/server/src/test/java/org/apache/druid/catalog/model/table/CsvInputFormatTest.java b/server/src/test/java/org/apache/druid/catalog/model/table/CsvInputFormatTest.java
index 0f83d8a64f..6d5dd63c07 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/table/CsvInputFormatTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/table/CsvInputFormatTest.java
@@ -19,12 +19,12 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.singletonList;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNull;
 
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -57,13 +57,12 @@ public class CsvInputFormatTest extends BaseExternTableTest {
     assertEquals(0, csvFormat.getSkipHeaderRows());
     assertFalse(csvFormat.isFindColumnsFromHeader());
     assertNull(csvFormat.getListDelimiter());
-    assertEquals(Collections.singletonList("a"), csvFormat.getColumns());
+    assertEquals(singletonList("a"), csvFormat.getColumns());
   }
 
   @Test
   public void testConversion() {
-    CsvInputFormat format =
-        new CsvInputFormat(Collections.singletonList("a"), ";", false, false, 1);
+    CsvInputFormat format = new CsvInputFormat(singletonList("a"), ";", false, false, 1);
     TableMetadata table =
         TableBuilder.external("foo")
             .inputSource(toMap(new InlineInputSource("a\n")))
@@ -96,12 +95,12 @@ public class CsvInputFormatTest extends BaseExternTableTest {
     args.put(FlatTextFormatDefn.LIST_DELIMITER_PARAMETER, ";");
     args.put(FlatTextFormatDefn.SKIP_ROWS_PARAMETER, 1);
     InputFormatDefn defn = registry.inputFormatDefnFor(CsvInputFormat.TYPE_KEY);
-    List<ColumnSpec> columns = Collections.singletonList(new ColumnSpec("a", null, null));
+    List<ColumnSpec> columns = singletonList(new ColumnSpec("a", null, null));
     InputFormat inputFormat = defn.convertFromArgs(args, columns, mapper);
     CsvInputFormat csvFormat = (CsvInputFormat) inputFormat;
     assertEquals(1, csvFormat.getSkipHeaderRows());
     assertFalse(csvFormat.isFindColumnsFromHeader());
     assertEquals(";", csvFormat.getListDelimiter());
-    assertEquals(Collections.singletonList("a"), csvFormat.getColumns());
+    assertEquals(singletonList("a"), csvFormat.getColumns());
   }
 }
diff --git a/server/src/test/java/org/apache/druid/catalog/model/table/DatasourceTableTest.java b/server/src/test/java/org/apache/druid/catalog/model/table/DatasourceTableTest.java
index 215b1a7229..7ebdf0d01c 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/table/DatasourceTableTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/table/DatasourceTableTest.java
@@ -19,6 +19,7 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.singletonList;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotEquals;
@@ -30,7 +31,6 @@ import static org.junit.Assert.assertTrue;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableMap;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -77,7 +77,7 @@ public class DatasourceTableTest {
   }
 
   private void expectValidationFails(final ResolvedTable table) {
-    assertThrows(IAE.class, () -> table.validate());
+    assertThrows(IAE.class, table::validate);
   }
 
   private void expectValidationFails(final TableSpec spec) {
@@ -194,7 +194,7 @@ public class DatasourceTableTest {
     // Name is required
     {
       ColumnSpec spec = new ColumnSpec(null, null, null);
-      assertThrows(IAE.class, () -> spec.validate());
+      assertThrows(IAE.class, spec::validate);
     }
     {
       ColumnSpec spec = new ColumnSpec("foo", null, null);
@@ -425,9 +425,7 @@ public class DatasourceTableTest {
     assertMergeFails(spec, update);
 
     // Merge
-    updatedProps =
-        ImmutableMap.of(
-            DatasourceDefn.HIDDEN_COLUMNS_PROPERTY, Collections.singletonList("mumble"));
+    updatedProps = ImmutableMap.of(DatasourceDefn.HIDDEN_COLUMNS_PROPERTY, singletonList("mumble"));
     update = new TableSpec(null, updatedProps, null);
     merged = mergeTables(spec, update);
     expectValidationSucceeds(merged);
@@ -442,8 +440,7 @@ public class DatasourceTableTest {
     Map<String, Object> props = ImmutableMap.of(DatasourceDefn.SEGMENT_GRANULARITY_PROPERTY, "P1D");
     TableSpec spec = new TableSpec(DatasourceDefn.TABLE_TYPE, props, null);
 
-    List<ColumnSpec> colUpdates =
-        Collections.singletonList(new ColumnSpec("a", Columns.BIGINT, null));
+    List<ColumnSpec> colUpdates = singletonList(new ColumnSpec("a", Columns.BIGINT, null));
     TableSpec update = new TableSpec(null, null, colUpdates);
     TableSpec merged = mergeTables(spec, update);
     List<ColumnSpec> columns = merged.columns();
@@ -490,8 +487,8 @@ public class DatasourceTableTest {
    * it, then copy the JSON from the console. The examples pull out bits and pieces in multiple
    * places.
    */
-  @Test
   @Ignore
+  @Test
   public void docExample() {
     TableSpec spec =
         TableBuilder.datasource("foo", "PT1H")
diff --git a/server/src/test/java/org/apache/druid/catalog/model/table/DelimitedInputFormatTest.java b/server/src/test/java/org/apache/druid/catalog/model/table/DelimitedInputFormatTest.java
index 44ca46b677..53b8d36cbd 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/table/DelimitedInputFormatTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/table/DelimitedInputFormatTest.java
@@ -19,6 +19,7 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.singletonList;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNull;
@@ -26,7 +27,6 @@ import static org.junit.Assert.assertTrue;
 
 import com.google.common.collect.ImmutableMap;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -66,13 +66,13 @@ public class DelimitedInputFormatTest extends BaseExternTableTest {
     assertFalse(delmited.isFindColumnsFromHeader());
     assertNull(delmited.getListDelimiter());
     assertEquals("|", delmited.getDelimiter());
-    assertEquals(Collections.singletonList("a"), delmited.getColumns());
+    assertEquals(singletonList("a"), delmited.getColumns());
   }
 
   @Test
   public void testConversion() {
     DelimitedInputFormat format =
-        new DelimitedInputFormat(Collections.singletonList("a"), ";", "|", false, false, 1);
+        new DelimitedInputFormat(singletonList("a"), ";", "|", false, false, 1);
     TableMetadata table =
         TableBuilder.external("foo")
             .inputSource(toMap(new InlineInputSource("a\n")))
@@ -108,13 +108,13 @@ public class DelimitedInputFormatTest extends BaseExternTableTest {
     args.put(FlatTextFormatDefn.LIST_DELIMITER_PARAMETER, ";");
     args.put(FlatTextFormatDefn.SKIP_ROWS_PARAMETER, 1);
     InputFormatDefn defn = registry.inputFormatDefnFor(DelimitedInputFormat.TYPE_KEY);
-    List<ColumnSpec> columns = Collections.singletonList(new ColumnSpec("a", null, null));
+    List<ColumnSpec> columns = singletonList(new ColumnSpec("a", null, null));
     InputFormat inputFormat = defn.convertFromArgs(args, columns, mapper);
     DelimitedInputFormat delmited = (DelimitedInputFormat) inputFormat;
     assertEquals(1, delmited.getSkipHeaderRows());
     assertFalse(delmited.isFindColumnsFromHeader());
     assertEquals(";", delmited.getListDelimiter());
     assertEquals("|", delmited.getDelimiter());
-    assertEquals(Collections.singletonList("a"), delmited.getColumns());
+    assertEquals(singletonList("a"), delmited.getColumns());
   }
 }
diff --git a/server/src/test/java/org/apache/druid/catalog/model/table/ExternalTableTest.java b/server/src/test/java/org/apache/druid/catalog/model/table/ExternalTableTest.java
index eeeec95a02..c914fe0b19 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/table/ExternalTableTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/table/ExternalTableTest.java
@@ -19,13 +19,13 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.singletonList;
 import static org.junit.Assert.assertThrows;
 
 import com.google.common.collect.ImmutableMap;
 import java.io.File;
 import java.net.URI;
 import java.net.URISyntaxException;
-import java.util.Collections;
 import java.util.Map;
 import org.apache.druid.catalog.model.Columns;
 import org.apache.druid.catalog.model.ResolvedTable;
@@ -53,7 +53,7 @@ public class ExternalTableTest extends BaseExternTableTest {
     // Empty table: not valid
     TableMetadata table = TableBuilder.external("foo").build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -61,7 +61,7 @@ public class ExternalTableTest extends BaseExternTableTest {
     // Empty table: not valid
     TableMetadata table = TableBuilder.external("foo").inputSource(ImmutableMap.of()).build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -70,7 +70,7 @@ public class ExternalTableTest extends BaseExternTableTest {
     TableMetadata table =
         TableBuilder.external("foo").inputSource(ImmutableMap.of("type", "unknown")).build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -91,7 +91,7 @@ public class ExternalTableTest extends BaseExternTableTest {
             .inputFormat(ImmutableMap.of())
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -103,14 +103,13 @@ public class ExternalTableTest extends BaseExternTableTest {
             .inputFormat(ImmutableMap.of("type", "unknown"))
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
   public void testValidateSourceAndFormat() {
     // Format is given without columns: it is validated
-    CsvInputFormat format =
-        new CsvInputFormat(Collections.singletonList("a"), ";", false, false, 0);
+    CsvInputFormat format = new CsvInputFormat(singletonList("a"), ";", false, false, 0);
     TableMetadata table =
         TableBuilder.external("foo")
             .inputSource(toMap(new InlineInputSource("a\n")))
@@ -125,8 +124,8 @@ public class ExternalTableTest extends BaseExternTableTest {
    * Test case for multiple of the {@code ext.md} examples. To use this, enable the test, run it,
    * then copy the JSON from the console. The examples pull out bits and pieces in multiple places.
    */
-  @Test
   @Ignore
+  @Test
   public void wikipediaDocExample() {
     JsonInputFormat format = new JsonInputFormat(null, null, true, true, false);
     LocalInputSource inputSource =
@@ -151,12 +150,12 @@ public class ExternalTableTest extends BaseExternTableTest {
     LOG.info(table.spec().toString());
   }
 
-  @Test
   @Ignore
+  @Test
   public void httpDocExample() throws URISyntaxException {
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("https://example.com/my.csv")), // removed
+            singletonList(new URI("https://example.com/my.csv")), // removed
             "bob",
             new DefaultPasswordProvider("secret"),
             new HttpInputSourceConfig(null));
@@ -175,12 +174,12 @@ public class ExternalTableTest extends BaseExternTableTest {
     LOG.info(table.spec().toString());
   }
 
-  @Test
   @Ignore
+  @Test
   public void httpConnDocExample() throws URISyntaxException {
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("https://example.com/")),
+            singletonList(new URI("https://example.com/")),
             "bob",
             new DefaultPasswordProvider("secret"),
             new HttpInputSourceConfig(null));
@@ -192,8 +191,8 @@ public class ExternalTableTest extends BaseExternTableTest {
     LOG.info(table.spec().toString());
   }
 
-  @Test
   @Ignore
+  @Test
   public void localDocExample() {
     Map<String, Object> sourceMap =
         ImmutableMap.of("type", LocalInputSource.TYPE_KEY, "baseDir", "/var/data");
diff --git a/server/src/test/java/org/apache/druid/catalog/model/table/HttpInputSourceDefnTest.java b/server/src/test/java/org/apache/druid/catalog/model/table/HttpInputSourceDefnTest.java
index 63795f2b52..5c1196b294 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/table/HttpInputSourceDefnTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/table/HttpInputSourceDefnTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singleton;
+import static java.util.Collections.singletonList;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
@@ -30,7 +34,6 @@ import com.google.common.collect.ImmutableMap;
 import java.net.URI;
 import java.net.URISyntaxException;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 import org.apache.druid.catalog.model.CatalogUtils;
@@ -70,7 +73,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
             .column("y", Columns.BIGINT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -85,7 +88,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
             .column("y", Columns.BIGINT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -93,7 +96,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
     // No format: not valid if URI is provided
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("http://example.com/file.csv")),
+            singletonList(new URI("http://example.com/file.csv")),
             null,
             null,
             new HttpInputSourceConfig(null));
@@ -104,7 +107,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
             .column("y", Columns.BIGINT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -112,7 +115,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
     // No format: not valid if URI is provided
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("http://example.com/file.csv")),
+            singletonList(new URI("http://example.com/file.csv")),
             null,
             null,
             new HttpInputSourceConfig(null));
@@ -122,7 +125,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
             .inputFormat(CSV_FORMAT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -142,7 +145,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
     // No format: not valid if URI is provided
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("http://example.com/file.csv")),
+            singletonList(new URI("http://example.com/file.csv")),
             null,
             null,
             new HttpInputSourceConfig(null));
@@ -153,7 +156,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
             .property(HttpInputSourceDefn.URI_TEMPLATE_PROPERTY, "http://example.com/{}")
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -189,8 +192,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
 
     // Convert to an external table. Must provide the URIs plus format and columns.
     Map<String, Object> args = new HashMap<>();
-    args.put(
-        HttpInputSourceDefn.URIS_PARAMETER, Collections.singletonList("http://foo.com/my.csv"));
+    args.put(HttpInputSourceDefn.URIS_PARAMETER, singletonList("http://foo.com/my.csv"));
     args.put(HttpInputSourceDefn.USER_PARAMETER, "bob");
     args.put(HttpInputSourceDefn.PASSWORD_PARAMETER, "secret");
     args.put(FormattedInputSourceDefn.FORMAT_PARAMETER, CsvFormatDefn.TYPE_KEY);
@@ -198,14 +200,14 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
     validateHappyPath(externSpec, true);
 
     // But, it fails if there are no columns.
-    assertThrows(IAE.class, () -> fn.apply("x", args, Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", args, emptyList(), mapper));
   }
 
   @Test
   public void testFullTableSpecHappyPath() throws URISyntaxException {
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("http://foo.com/my.csv")),
+            singletonList(new URI("http://foo.com/my.csv")),
             "bob",
             new DefaultPasswordProvider("secret"),
             new HttpInputSourceConfig(null));
@@ -234,11 +236,11 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
     assertTrue(fn.parameters().isEmpty());
 
     // Convert to an external table.
-    externSpec = fn.apply("x", Collections.emptyMap(), Collections.emptyList(), mapper);
+    externSpec = fn.apply("x", emptyMap(), emptyList(), mapper);
     validateHappyPath(externSpec, true);
 
     // But, it fails columns are provided since the table already has them.
-    assertThrows(IAE.class, () -> fn.apply("x", Collections.emptyMap(), COLUMNS, mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", emptyMap(), COLUMNS, mapper));
   }
 
   @Test
@@ -274,9 +276,8 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
     ExternalTableSpec externSpec =
         fn.apply(
             "x",
-            ImmutableMap.of(
-                HttpInputSourceDefn.URIS_PARAMETER, Collections.singletonList("my.csv")),
-            Collections.emptyList(),
+            ImmutableMap.of(HttpInputSourceDefn.URIS_PARAMETER, singletonList("my.csv")),
+            emptyList(),
             mapper);
     validateHappyPath(externSpec, false);
   }
@@ -319,9 +320,8 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
     ExternalTableSpec externSpec =
         fn.apply(
             "x",
-            ImmutableMap.of(
-                HttpInputSourceDefn.URIS_PARAMETER, Collections.singletonList("my.csv")),
-            Collections.emptyList(),
+            ImmutableMap.of(HttpInputSourceDefn.URIS_PARAMETER, singletonList("my.csv")),
+            emptyList(),
             mapper);
     validateHappyPath(externSpec, true);
   }
@@ -330,7 +330,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
   public void testTemplateSpecWithoutFormatHappyPath() throws URISyntaxException {
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("http://foo.com/my.csv")), // removed
+            singletonList(new URI("http://foo.com/my.csv")), // removed
             "bob",
             new DefaultPasswordProvider("secret"),
             new HttpInputSourceConfig(null));
@@ -357,7 +357,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
 
     // Convert to an external table. Must provide the URIs plus format and columns.
     Map<String, Object> args = new HashMap<>();
-    args.put(HttpInputSourceDefn.URIS_PARAMETER, Collections.singletonList("my.csv"));
+    args.put(HttpInputSourceDefn.URIS_PARAMETER, singletonList("my.csv"));
     args.put(FormattedInputSourceDefn.FORMAT_PARAMETER, CsvFormatDefn.TYPE_KEY);
     ExternalTableSpec externSpec = fn.apply("x", args, COLUMNS, mapper);
     validateHappyPath(externSpec, true);
@@ -393,15 +393,14 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
             Arrays.asList("http://foo.com/foo.csv", "http://foo.com/bar.csv")),
         sourceSpec.getUris());
     assertEquals(
-        Collections.singleton(HttpInputSourceDefn.TYPE_KEY),
-        externSpec.inputSourceTypesSupplier.get());
+        singleton(HttpInputSourceDefn.TYPE_KEY), externSpec.inputSourceTypesSupplier.get());
   }
 
   @Test
   public void testMultipleURIsWithTemplate() throws URISyntaxException {
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("http://foo.com/my.csv")), // removed
+            singletonList(new URI("http://foo.com/my.csv")), // removed
             "bob",
             new DefaultPasswordProvider("secret"),
             new HttpInputSourceConfig(null));
@@ -429,7 +428,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
         fn.apply(
             "x",
             ImmutableMap.of(HttpInputSourceDefn.URIS_PARAMETER, Arrays.asList("my.csv", "bar.csv")),
-            Collections.emptyList(),
+            emptyList(),
             mapper);
 
     HttpInputSource sourceSpec = (HttpInputSource) externSpec.inputSource;
@@ -438,8 +437,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
             Arrays.asList("http://foo.com/my.csv", "http://foo.com/bar.csv")),
         sourceSpec.getUris());
     assertEquals(
-        Collections.singleton(HttpInputSourceDefn.TYPE_KEY),
-        externSpec.inputSourceTypesSupplier.get());
+        singleton(HttpInputSourceDefn.TYPE_KEY), externSpec.inputSourceTypesSupplier.get());
   }
 
   @Test
@@ -465,15 +463,14 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
             Arrays.asList("http://foo.com/foo.csv", "http://foo.com/bar.csv")),
         sourceSpec.getUris());
     assertEquals(
-        Collections.singleton(HttpInputSourceDefn.TYPE_KEY),
-        externSpec.inputSourceTypesSupplier.get());
+        singleton(HttpInputSourceDefn.TYPE_KEY), externSpec.inputSourceTypesSupplier.get());
   }
 
   @Test
   public void testEnvPassword() throws URISyntaxException {
     HttpInputSource inputSource =
         new HttpInputSource(
-            Collections.singletonList(new URI("http://foo.com/my.csv")),
+            singletonList(new URI("http://foo.com/my.csv")),
             "bob",
             new EnvironmentVariablePasswordProvider("SECRET"),
             new HttpInputSourceConfig(null));
@@ -500,8 +497,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
         ((EnvironmentVariablePasswordProvider) sourceSpec.getHttpAuthenticationPasswordProvider())
             .getVariable());
     assertEquals(
-        Collections.singleton(HttpInputSourceDefn.TYPE_KEY),
-        externSpec.inputSourceTypesSupplier.get());
+        singleton(HttpInputSourceDefn.TYPE_KEY), externSpec.inputSourceTypesSupplier.get());
   }
 
   private void validateHappyPath(ExternalTableSpec externSpec, boolean withUser) {
@@ -524,8 +520,7 @@ public class HttpInputSourceDefnTest extends BaseExternTableTest {
     assertEquals(ColumnType.STRING, sig.getColumnType(0).get());
     assertEquals(ColumnType.LONG, sig.getColumnType(1).get());
     assertEquals(
-        Collections.singleton(HttpInputSourceDefn.TYPE_KEY),
-        externSpec.inputSourceTypesSupplier.get());
+        singleton(HttpInputSourceDefn.TYPE_KEY), externSpec.inputSourceTypesSupplier.get());
   }
 
   private Map<String, Object> httpToMap(HttpInputSource source) {
diff --git a/server/src/test/java/org/apache/druid/catalog/model/table/InlineInputSourceDefnTest.java b/server/src/test/java/org/apache/druid/catalog/model/table/InlineInputSourceDefnTest.java
index d3bf047cfe..e13580d5f7 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/table/InlineInputSourceDefnTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/table/InlineInputSourceDefnTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singleton;
+import static java.util.Collections.singletonList;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertThrows;
@@ -26,7 +29,6 @@ import static org.junit.Assert.assertTrue;
 
 import com.google.common.collect.ImmutableMap;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -52,7 +54,7 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
             .column("x", Columns.VARCHAR)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -64,7 +66,7 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
             .column("x", Columns.VARCHAR)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -75,7 +77,7 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
             .inputFormat(CSV_FORMAT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -104,7 +106,7 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
   public void testMissingArgs() {
     InputSourceDefn defn = registry.inputSourceDefnFor(InlineInputSourceDefn.TYPE_KEY);
     TableFunction fn = defn.adHocTableFn();
-    assertThrows(IAE.class, () -> fn.apply("x", new HashMap<>(), Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", new HashMap<>(), emptyList(), mapper));
   }
 
   @Test
@@ -113,7 +115,7 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
     TableFunction fn = defn.adHocTableFn();
     Map<String, Object> args = new HashMap<>();
     args.put(InlineInputSourceDefn.DATA_PROPERTY, "a");
-    assertThrows(IAE.class, () -> fn.apply("x", args, Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", args, emptyList(), mapper));
   }
 
   @Test
@@ -137,12 +139,10 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
     CsvInputFormat format = (CsvInputFormat) extern.inputFormat;
     assertEquals(Arrays.asList("a", "b"), format.getColumns());
     assertEquals(2, extern.signature.size());
-    assertEquals(
-        Collections.singleton(InlineInputSourceDefn.TYPE_KEY),
-        extern.inputSourceTypesSupplier.get());
+    assertEquals(singleton(InlineInputSourceDefn.TYPE_KEY), extern.inputSourceTypesSupplier.get());
 
     // Fails if no columns are provided.
-    assertThrows(IAE.class, () -> fn.apply("x", new HashMap<>(), Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", new HashMap<>(), emptyList(), mapper));
   }
 
   @Test
@@ -165,7 +165,7 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
     assertTrue(fn.parameters().isEmpty());
 
     // Verify the conversion
-    ExternalTableSpec extern = fn.apply("x", new HashMap<>(), Collections.emptyList(), mapper);
+    ExternalTableSpec extern = fn.apply("x", new HashMap<>(), emptyList(), mapper);
 
     assertTrue(extern.inputSource instanceof InlineInputSource);
     InlineInputSource inputSource = (InlineInputSource) extern.inputSource;
@@ -174,9 +174,7 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
     CsvInputFormat actualFormat = (CsvInputFormat) extern.inputFormat;
     assertEquals(Arrays.asList("a", "b"), actualFormat.getColumns());
     assertEquals(2, extern.signature.size());
-    assertEquals(
-        Collections.singleton(InlineInputSourceDefn.TYPE_KEY),
-        extern.inputSourceTypesSupplier.get());
+    assertEquals(singleton(InlineInputSourceDefn.TYPE_KEY), extern.inputSourceTypesSupplier.get());
 
     // Cannot supply columns with the function
     List<ColumnSpec> columns =
@@ -188,8 +186,7 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
   @Test
   public void testDefinedTable() {
     // Define an inline table
-    CsvInputFormat format =
-        new CsvInputFormat(Collections.singletonList("a"), ";", false, false, 0);
+    CsvInputFormat format = new CsvInputFormat(singletonList("a"), ";", false, false, 0);
     TableMetadata table =
         TableBuilder.external("foo")
             .inputSource(toMap(new InlineInputSource("a,b\nc,d")))
@@ -211,8 +208,6 @@ public class InlineInputSourceDefnTest extends BaseExternTableTest {
     CsvInputFormat actualFormat = (CsvInputFormat) extern.inputFormat;
     assertEquals(Arrays.asList("a", "b"), actualFormat.getColumns());
     assertEquals(2, extern.signature.size());
-    assertEquals(
-        Collections.singleton(InlineInputSourceDefn.TYPE_KEY),
-        extern.inputSourceTypesSupplier.get());
+    assertEquals(singleton(InlineInputSourceDefn.TYPE_KEY), extern.inputSourceTypesSupplier.get());
   }
 }
diff --git a/server/src/test/java/org/apache/druid/catalog/model/table/JsonInputFormatTest.java b/server/src/test/java/org/apache/druid/catalog/model/table/JsonInputFormatTest.java
index 9fc52d21d7..3c9a59b812 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/table/JsonInputFormatTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/table/JsonInputFormatTest.java
@@ -19,13 +19,13 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.singletonList;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
 
 import com.google.common.collect.ImmutableMap;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -95,7 +95,7 @@ public class JsonInputFormatTest extends BaseExternTableTest {
   public void testCreateFromArgs() {
     Map<String, Object> args = new HashMap<>();
     InputFormatDefn defn = registry.inputFormatDefnFor(JsonInputFormat.TYPE_KEY);
-    List<ColumnSpec> columns = Collections.singletonList(new ColumnSpec("a", null, null));
+    List<ColumnSpec> columns = singletonList(new ColumnSpec("a", null, null));
     InputFormat inputFormat = defn.convertFromArgs(args, columns, mapper);
     JsonInputFormat jsonFormat = (JsonInputFormat) inputFormat;
     assertNull(jsonFormat.getFlattenSpec());
diff --git a/server/src/test/java/org/apache/druid/catalog/model/table/LocalInputSourceDefnTest.java b/server/src/test/java/org/apache/druid/catalog/model/table/LocalInputSourceDefnTest.java
index 1ce754e934..1c7d2bfd38 100644
--- a/server/src/test/java/org/apache/druid/catalog/model/table/LocalInputSourceDefnTest.java
+++ b/server/src/test/java/org/apache/druid/catalog/model/table/LocalInputSourceDefnTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.catalog.model.table;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singleton;
+import static java.util.Collections.singletonList;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
@@ -29,7 +33,6 @@ import static org.junit.Assert.assertTrue;
 import com.google.common.collect.ImmutableMap;
 import java.io.File;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -64,7 +67,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
             .column("y", Columns.BIGINT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -78,7 +81,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
             .column("y", Columns.BIGINT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -91,7 +94,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
             .inputFormat(CSV_FORMAT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -134,7 +137,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     // Valid if neither columns nor format are provided. This is a "connection"
     // to some local directory.
     LocalInputSource inputSource =
-        new LocalInputSource(null, null, Collections.singletonList(new File("/tmp/myFile.csv")));
+        new LocalInputSource(null, null, singletonList(new File("/tmp/myFile.csv")));
     TableMetadata table =
         TableBuilder.external("foo")
             .inputSource(toMap(inputSource))
@@ -157,7 +160,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
             LocalInputSourceDefn.FILTER_FIELD,
             "*.csv",
             LocalInputSourceDefn.FILES_FIELD,
-            Collections.singletonList("foo.csv"));
+            singletonList("foo.csv"));
     TableMetadata table =
         TableBuilder.external("foo")
             .inputSource(source)
@@ -166,7 +169,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
             .column("y", Columns.BIGINT)
             .build();
     ResolvedTable resolved = registry.resolve(table.spec());
-    assertThrows(IAE.class, () -> resolved.validate());
+    assertThrows(IAE.class, resolved::validate);
   }
 
   @Test
@@ -195,7 +198,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     validateFormat(externSpec);
 
     // But, it fails if there are no columns.
-    assertThrows(IAE.class, () -> fn.apply("x", args, Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", args, emptyList(), mapper));
   }
 
   @Test
@@ -214,7 +217,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     validateFormat(externSpec);
 
     // But, it fails if there are no columns.
-    assertThrows(IAE.class, () -> fn.apply("x", args, Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", args, emptyList(), mapper));
   }
 
   @Test
@@ -234,7 +237,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     validateFormat(externSpec);
 
     // But, it fails if there are no columns.
-    assertThrows(IAE.class, () -> fn.apply("x", args, Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", args, emptyList(), mapper));
   }
 
   @Test
@@ -255,7 +258,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     validateFormat(externSpec);
 
     // But, it fails if there are no columns.
-    assertThrows(IAE.class, () -> fn.apply("x", args, Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", args, emptyList(), mapper));
   }
 
   @Test
@@ -327,7 +330,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     assertEquals(0, fn.parameters().size());
 
     // Apply the function with no arguments and no columns (since columns are already defined.)
-    externSpec = fn.apply("x", Collections.emptyMap(), Collections.emptyList(), mapper);
+    externSpec = fn.apply("x", emptyMap(), emptyList(), mapper);
     sourceSpec = (LocalInputSource) externSpec.inputSource;
     assertEquals("/tmp", sourceSpec.getBaseDir().toString());
     assertEquals("*.csv", sourceSpec.getFilter());
@@ -335,12 +338,12 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     validateFormat(externSpec);
 
     // Fails if columns are provided.
-    assertThrows(IAE.class, () -> fn.apply("x", Collections.emptyMap(), COLUMNS, mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", emptyMap(), COLUMNS, mapper));
   }
 
   @Test
   public void testFullyDefinedFiles() {
-    List<File> files = Collections.singletonList(new File("/tmp/my.csv"));
+    List<File> files = singletonList(new File("/tmp/my.csv"));
     LocalInputSource inputSource = new LocalInputSource(null, null, files);
     TableMetadata table =
         TableBuilder.external("foo")
@@ -374,7 +377,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     assertEquals(0, fn.parameters().size());
 
     // Apply the function with no arguments and no columns (since columns are already defined.)
-    externSpec = fn.apply("x", Collections.emptyMap(), Collections.emptyList(), mapper);
+    externSpec = fn.apply("x", emptyMap(), emptyList(), mapper);
     sourceSpec = (LocalInputSource) externSpec.inputSource;
     assertNull(sourceSpec.getBaseDir());
     assertNull(sourceSpec.getFilter());
@@ -382,7 +385,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     validateFormat(externSpec);
 
     // Fails if columns are provided.
-    assertThrows(IAE.class, () -> fn.apply("x", Collections.emptyMap(), COLUMNS, mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", emptyMap(), COLUMNS, mapper));
   }
 
   @Test
@@ -411,15 +414,14 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     assertFalse(hasParam(fn, FormattedInputSourceDefn.FORMAT_PARAMETER));
 
     // Must provide an additional parameter.
-    assertThrows(
-        IAE.class, () -> fn.apply("x", Collections.emptyMap(), Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", emptyMap(), emptyList(), mapper));
 
     {
       // Create a table with a file pattern.
       Map<String, Object> args = new HashMap<>();
       args.put(LocalInputSourceDefn.FILTER_PARAMETER, "*.csv");
       // Apply the function with no arguments and no columns (since columns are already defined.)
-      ExternalTableSpec externSpec = fn.apply("x", args, Collections.emptyList(), mapper);
+      ExternalTableSpec externSpec = fn.apply("x", args, emptyList(), mapper);
       LocalInputSource sourceSpec = (LocalInputSource) externSpec.inputSource;
       assertEquals("/tmp", sourceSpec.getBaseDir().toString());
       assertEquals("*.csv", sourceSpec.getFilter());
@@ -430,7 +432,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
       // Create a table with a file list.
       Map<String, Object> args = new HashMap<>();
       args.put(LocalInputSourceDefn.FILES_PARAMETER, Arrays.asList("foo.csv", "bar.csv"));
-      ExternalTableSpec externSpec = fn.apply("x", args, Collections.emptyList(), mapper);
+      ExternalTableSpec externSpec = fn.apply("x", args, emptyList(), mapper);
       LocalInputSource sourceSpec = (LocalInputSource) externSpec.inputSource;
       assertNull(sourceSpec.getBaseDir());
       assertNull(sourceSpec.getFilter());
@@ -463,8 +465,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     assertTrue(hasParam(fn, FormattedInputSourceDefn.FORMAT_PARAMETER));
 
     // Must provide an additional parameter.
-    assertThrows(
-        IAE.class, () -> fn.apply("x", Collections.emptyMap(), Collections.emptyList(), mapper));
+    assertThrows(IAE.class, () -> fn.apply("x", emptyMap(), emptyList(), mapper));
 
     {
       // Create a table with a file pattern and format.
@@ -473,7 +474,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
       args.put(FormattedInputSourceDefn.FORMAT_PARAMETER, CsvFormatDefn.TYPE_KEY);
 
       // Function fails without columns, since the table has none.
-      assertThrows(IAE.class, () -> fn.apply("x", args, Collections.emptyList(), mapper));
+      assertThrows(IAE.class, () -> fn.apply("x", args, emptyList(), mapper));
 
       // Apply the function with no arguments and columns
       ExternalTableSpec externSpec = fn.apply("x", args, COLUMNS, mapper);
@@ -490,7 +491,7 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
       args.put(FormattedInputSourceDefn.FORMAT_PARAMETER, CsvFormatDefn.TYPE_KEY);
 
       // Function fails without columns, since the table has none.
-      assertThrows(IAE.class, () -> fn.apply("x", args, Collections.emptyList(), mapper));
+      assertThrows(IAE.class, () -> fn.apply("x", args, emptyList(), mapper));
 
       // Provide format and columns.
       ExternalTableSpec externSpec = fn.apply("x", args, COLUMNS, mapper);
@@ -513,7 +514,6 @@ public class LocalInputSourceDefnTest extends BaseExternTableTest {
     assertEquals(ColumnType.STRING, sig.getColumnType(0).get());
     assertEquals(ColumnType.LONG, sig.getColumnType(1).get());
     assertEquals(
-        Collections.singleton(LocalInputSourceDefn.TYPE_KEY),
-        externSpec.inputSourceTypesSupplier.get());
+        singleton(LocalInputSourceDefn.TYPE_KEY), externSpec.inputSourceTypesSupplier.get());
   }
 }
diff --git a/server/src/test/java/org/apache/druid/client/BrokerInternalQueryConfigTest.java b/server/src/test/java/org/apache/druid/client/BrokerInternalQueryConfigTest.java
index 9443e1a15a..798eb049a2 100644
--- a/server/src/test/java/org/apache/druid/client/BrokerInternalQueryConfigTest.java
+++ b/server/src/test/java/org/apache/druid/client/BrokerInternalQueryConfigTest.java
@@ -96,8 +96,8 @@ public class BrokerInternalQueryConfigTest {
                     binder, "druid.broker.internal.query.config", BrokerInternalQueryConfig.class);
               }
 
-              @Provides
               @LazySingleton
+              @Provides
               public ObjectMapper jsonMapper() {
                 return new DefaultObjectMapper();
               }
diff --git a/server/src/test/java/org/apache/druid/client/BrokerServerViewTest.java b/server/src/test/java/org/apache/druid/client/BrokerServerViewTest.java
index c92de2a31e..d2e9220ca0 100644
--- a/server/src/test/java/org/apache/druid/client/BrokerServerViewTest.java
+++ b/server/src/test/java/org/apache/druid/client/BrokerServerViewTest.java
@@ -19,17 +19,19 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Predicates.alwaysTrue;
+import static java.util.Collections.emptySet;
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.dataformat.smile.SmileFactory;
 import com.fasterxml.jackson.dataformat.smile.SmileGenerator;
-import com.google.common.base.Predicates;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.Set;
 import java.util.concurrent.CountDownLatch;
@@ -348,8 +350,7 @@ public class BrokerServerViewTest extends CuratorTestBase {
     for (int i = 0; i < 5; ++i) {
       Assert.assertEquals(server21, selector.pick(null).getServer());
     }
-    Assert.assertEquals(
-        Collections.singletonList(server21.getMetadata()), selector.getCandidates(2));
+    Assert.assertEquals(singletonList(server21.getMetadata()), selector.getCandidates(2));
   }
 
   @Test
@@ -410,8 +411,7 @@ public class BrokerServerViewTest extends CuratorTestBase {
     for (int i = 0; i < 5; ++i) {
       Assert.assertEquals(historicalServer, selector.pick(null).getServer());
     }
-    Assert.assertEquals(
-        Collections.singletonList(historicalServer.getMetadata()), selector.getCandidates(2));
+    Assert.assertEquals(singletonList(historicalServer.getMetadata()), selector.getCandidates(2));
   }
 
   @Test
@@ -473,8 +473,7 @@ public class BrokerServerViewTest extends CuratorTestBase {
     for (int i = 0; i < 5; ++i) {
       Assert.assertEquals(server21, selector.pick(null).getServer());
     }
-    Assert.assertEquals(
-        Collections.singletonList(server21.getMetadata()), selector.getCandidates(2));
+    Assert.assertEquals(singletonList(server21.getMetadata()), selector.getCandidates(2));
   }
 
   @Test(expected = ISE.class)
@@ -487,12 +486,12 @@ public class BrokerServerViewTest extends CuratorTestBase {
 
   @Test(expected = ISE.class)
   public void testEmptyWatchedTiersConfig() throws Exception {
-    setupViews(Collections.emptySet(), null, true);
+    setupViews(emptySet(), null, true);
   }
 
   @Test(expected = ISE.class)
   public void testEmptyIgnoredTiersConfig() throws Exception {
-    setupViews(null, Collections.emptySet(), true);
+    setupViews(null, emptySet(), true);
   }
 
   /** Creates a DruidServer of type HISTORICAL and sets up a ZNode for it. */
@@ -548,8 +547,7 @@ public class BrokerServerViewTest extends CuratorTestBase {
       Set<String> watchedTiers, Set<String> ignoredTiers, boolean watchRealtimeTasks)
       throws Exception {
     baseView =
-        new BatchServerInventoryView(
-            zkPathsConfig, curator, jsonMapper, Predicates.alwaysTrue(), "test") {
+        new BatchServerInventoryView(zkPathsConfig, curator, jsonMapper, alwaysTrue(), "test") {
           @Override
           public void registerSegmentCallback(Executor exec, final SegmentCallback callback) {
             super.registerSegmentCallback(
diff --git a/server/src/test/java/org/apache/druid/client/CachingClusteredClientFunctionalityTest.java b/server/src/test/java/org/apache/druid/client/CachingClusteredClientFunctionalityTest.java
index 449f86ad67..b3b546a951 100644
--- a/server/src/test/java/org/apache/druid/client/CachingClusteredClientFunctionalityTest.java
+++ b/server/src/test/java/org/apache/druid/client/CachingClusteredClientFunctionalityTest.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.client;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Ordering;
 import it.unimi.dsi.fastutil.ints.Int2ObjectRBTreeMap;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.Comparator;
 import java.util.List;
 import java.util.Optional;
@@ -110,7 +111,7 @@ public class CachingClusteredClientFunctionalityTest {
             .dataSource("test")
             .intervals("2015-01-02/2015-01-03")
             .granularity("day")
-            .aggregators(Collections.singletonList(new CountAggregatorFactory("rows")))
+            .aggregators(singletonList(new CountAggregatorFactory("rows")))
             .context(ImmutableMap.of("uncoveredIntervalsLimit", 3));
 
     ResponseContext responseContext = ResponseContext.createEmpty();
@@ -214,7 +215,7 @@ public class CachingClusteredClientFunctionalityTest {
                       Int2ObjectRBTreeMap<Set<QueryableDruidServer>> prioritizedServers,
                       DataSegment segment,
                       int numServersToPick) {
-                    return Collections.singletonList(
+                    return singletonList(
                         new QueryableDruidServer(
                             new DruidServer(
                                 "localhost",
diff --git a/server/src/test/java/org/apache/druid/client/CachingClusteredClientPerfTest.java b/server/src/test/java/org/apache/druid/client/CachingClusteredClientPerfTest.java
index d7142a9961..e96d9d93f4 100644
--- a/server/src/test/java/org/apache/druid/client/CachingClusteredClientPerfTest.java
+++ b/server/src/test/java/org/apache/druid/client/CachingClusteredClientPerfTest.java
@@ -19,13 +19,16 @@
 
 package org.apache.druid.client;
 
+import static java.util.Collections.singletonList;
 import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
 
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterators;
 import com.google.common.collect.Ordering;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
@@ -68,7 +71,6 @@ import org.apache.druid.timeline.partition.LinearShardSpec;
 import org.joda.time.Interval;
 import org.junit.Assert;
 import org.junit.Test;
-import org.mockito.Mockito;
 
 /**
  * Performance tests for {@link CachingClusteredClient}, that do not require a real cluster, can be
@@ -116,25 +118,25 @@ public class CachingClusteredClientPerfTest {
                   segment.getShardSpec().createChunk(ss));
             }));
 
-    TimelineServerView serverView = Mockito.mock(TimelineServerView.class);
-    QueryScheduler queryScheduler = Mockito.mock(QueryScheduler.class);
+    TimelineServerView serverView = mock(TimelineServerView.class);
+    QueryScheduler queryScheduler = mock(QueryScheduler.class);
     // mock scheduler to return same sequence as argument
-    Mockito.when(queryScheduler.run(any(), any())).thenAnswer(i -> i.getArgument(1));
-    Mockito.when(queryScheduler.prioritizeAndLaneQuery(any(), any()))
+    when(queryScheduler.run(any(), any())).thenAnswer(i -> i.getArgument(1));
+    when(queryScheduler.prioritizeAndLaneQuery(any(), any()))
         .thenAnswer(i -> ((QueryPlus) i.getArgument(0)).getQuery());
 
-    Mockito.doReturn(Optional.of(timeline)).when(serverView).getTimeline(any());
-    Mockito.doReturn(new MockQueryRunner()).when(serverView).getQueryRunner(any());
+    doReturn(Optional.of(timeline)).when(serverView).getTimeline(any());
+    doReturn(new MockQueryRunner()).when(serverView).getQueryRunner(any());
     CachingClusteredClient cachingClusteredClient =
         new CachingClusteredClient(
             new MockQueryToolChestWareHouse(),
             serverView,
             MapCache.create(1024),
             TestHelper.makeJsonMapper(),
-            Mockito.mock(CachePopulator.class),
+            mock(CachePopulator.class),
             new CacheConfig(),
-            Mockito.mock(DruidHttpClientConfig.class),
-            Mockito.mock(DruidProcessingConfig.class),
+            mock(DruidHttpClientConfig.class),
+            mock(DruidProcessingConfig.class),
             ForkJoinPool.commonPool(),
             queryScheduler,
             JoinableFactoryWrapperTest.NOOP_JOINABLE_FACTORY_WRAPPER,
@@ -150,7 +152,7 @@ public class CachingClusteredClientPerfTest {
   private Query<SegmentDescriptor> makeFakeQuery(Interval interval) {
     return new TestQuery(
         new TableDataSource("test"),
-        new MultipleIntervalSegmentSpec(Collections.singletonList(interval)),
+        new MultipleIntervalSegmentSpec(singletonList(interval)),
         false,
         ImmutableMap.of(BaseQuery.QUERY_ID, "testQuery"));
   }
diff --git a/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java b/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java
index a94ffebbb1..a87518dcfb 100644
--- a/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java
+++ b/server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java
@@ -19,11 +19,15 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.shuffle;
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.annotation.JsonSerialize;
 import com.google.common.base.Function;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
@@ -39,10 +43,8 @@ import com.google.common.util.concurrent.ListeningExecutorService;
 import com.google.common.util.concurrent.MoreExecutors;
 import com.google.common.util.concurrent.SettableFuture;
 import java.io.IOException;
-import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
@@ -338,7 +340,7 @@ public class CachingClusteredClientTest {
               return future;
             } else {
               List<Pair<SettableFuture, Object>> tasks = Lists.newArrayList(taskQueue.iterator());
-              Collections.shuffle(tasks, new Random(0));
+              shuffle(tasks, new Random(0));
 
               for (final Pair<SettableFuture, Object> pair : tasks) {
                 ListenableFuture future =
@@ -366,8 +368,8 @@ public class CachingClusteredClientTest {
                 : (ListenableFuture<T>) delegate.submit((Runnable) task);
           }
 
-          @SuppressWarnings("ParameterPackage")
           @Override
+          @SuppressWarnings("ParameterPackage")
           public <T> ListenableFuture<T> submit(Callable<T> task) {
             return maybeSubmitTask(task, true);
           }
@@ -457,8 +459,8 @@ public class CachingClusteredClientTest {
             21894));
   }
 
-  @Test
   @SuppressWarnings("unchecked")
+  @Test
   public void testTimeseriesCaching() {
     final Druids.TimeseriesQueryBuilder builder =
         Druids.newTimeseriesQueryBuilder()
@@ -569,8 +571,8 @@ public class CachingClusteredClientTest {
         runner.run(QueryPlus.wrap(query)));
   }
 
-  @Test
   @SuppressWarnings("unchecked")
+  @Test
   public void testCachingOverBulkLimitEnforcesLimit() {
     final int limit = 10;
     final Interval interval = Intervals.of("2011-01-01/2011-01-02");
@@ -728,8 +730,8 @@ public class CachingClusteredClientTest {
         runner.run(QueryPlus.wrap(query)));
   }
 
-  @Test
   @SuppressWarnings("unchecked")
+  @Test
   public void testTimeseriesCachingTimeZone() {
     final Druids.TimeseriesQueryBuilder builder =
         Druids.newTimeseriesQueryBuilder()
@@ -859,8 +861,8 @@ public class CachingClusteredClientTest {
     Assert.assertEquals(1, cache.getStats().getNumMisses());
   }
 
-  @Test
   @SuppressWarnings("unchecked")
+  @Test
   public void testTopNCaching() {
     final TopNQueryBuilder builder =
         new TopNQueryBuilder()
@@ -1125,8 +1127,8 @@ public class CachingClusteredClientTest {
         runner.run(QueryPlus.wrap(query)));
   }
 
-  @Test
   @SuppressWarnings("unchecked")
+  @Test
   public void testTopNCachingTimeZone() {
     final TopNQueryBuilder builder =
         new TopNQueryBuilder()
@@ -1411,8 +1413,8 @@ public class CachingClusteredClientTest {
     return Sequences.simple(sequences).flatMerge(seq -> seq, query.getResultOrdering());
   }
 
-  @Test
   @SuppressWarnings("unchecked")
+  @Test
   public void testTopNCachingEmptyResults() {
     final TopNQueryBuilder builder =
         new TopNQueryBuilder()
@@ -1908,7 +1910,7 @@ public class CachingClusteredClientTest {
             .granularity(GRANULARITY)
             .limit(1000)
             .intervals(SEG_SPEC)
-            .dimensions(Collections.singletonList(TOP_DIM))
+            .dimensions(singletonList(TOP_DIM))
             .query("how")
             .context(CONTEXT);
 
@@ -2155,7 +2157,7 @@ public class CachingClusteredClientTest {
             .granularity(GRANULARITY)
             .limit(1000)
             .intervals(SEG_SPEC)
-            .dimensions(Collections.singletonList(TOP_DIM))
+            .dimensions(singletonList(TOP_DIM))
             .query("how")
             .context(CONTEXT);
 
@@ -2535,8 +2537,8 @@ public class CachingClusteredClientTest {
             .setContext(CONTEXT);
 
     final HyperLogLogCollector collector = HyperLogLogCollector.makeLatestCollector();
-    collector.add(hashFn.hashString("abc123", StandardCharsets.UTF_8).asBytes());
-    collector.add(hashFn.hashString("123abc", StandardCharsets.UTF_8).asBytes());
+    collector.add(hashFn.hashString("abc123", UTF_8).asBytes());
+    collector.add(hashFn.hashString("123abc", UTF_8).asBytes());
 
     final GroupByQuery query = builder.randomQueryId().build();
 
@@ -3192,7 +3194,7 @@ public class CachingClusteredClientTest {
       final Interval interval = (Interval) args[i];
       final Iterable<Result<Object>> results = (Iterable<Result<Object>>) args[i + 1];
 
-      if (queryIntervals.size() > 0
+      if (!queryIntervals.isEmpty()
           && interval.equals(queryIntervals.get(queryIntervals.size() - 1))) {
         expectedResults.get(expectedResults.size() - 1).add(results);
       } else {
@@ -3775,7 +3777,7 @@ public class CachingClusteredClientTest {
   }
 
   private Iterable<Result<TopNResultValue>> makeTopNResults(List<String> names, Object... objects) {
-    Preconditions.checkArgument(names.size() == 7);
+    checkArgument(names.size() == 7);
     List<Result<TopNResultValue>> retVal = new ArrayList<>();
     int index = 0;
     while (index < objects.length) {
@@ -4007,46 +4009,46 @@ public class CachingClusteredClientTest {
         super("", Intervals.utc(0, 1), "", null, null, null, NoneShardSpec.instance(), null, 0);
       }
 
-      @Override
       @JsonProperty
+      @Override
       public String getDataSource() {
         return baseSegment.getDataSource();
       }
 
-      @Override
       @JsonProperty
+      @Override
       public Interval getInterval() {
         return baseSegment.getInterval();
       }
 
-      @Override
       @JsonProperty
+      @Override
       public Map<String, Object> getLoadSpec() {
         return baseSegment.getLoadSpec();
       }
 
-      @Override
       @JsonProperty
+      @Override
       public String getVersion() {
         return "version";
       }
 
-      @Override
-      @JsonSerialize
       @JsonProperty
+      @JsonSerialize
+      @Override
       public List<String> getDimensions() {
         return baseSegment.getDimensions();
       }
 
-      @Override
-      @JsonSerialize
       @JsonProperty
+      @JsonSerialize
+      @Override
       public List<String> getMetrics() {
         return baseSegment.getMetrics();
       }
 
-      @Override
       @JsonProperty
+      @Override
       public ShardSpec getShardSpec() {
         try {
           return baseSegment.getShardSpec();
@@ -4055,8 +4057,8 @@ public class CachingClusteredClientTest {
         }
       }
 
-      @Override
       @JsonProperty
+      @Override
       public long getSize() {
         return baseSegment.getSize();
       }
diff --git a/server/src/test/java/org/apache/druid/client/CoordinatorServerViewTest.java b/server/src/test/java/org/apache/druid/client/CoordinatorServerViewTest.java
index 6e5209f09d..d27027255a 100644
--- a/server/src/test/java/org/apache/druid/client/CoordinatorServerViewTest.java
+++ b/server/src/test/java/org/apache/druid/client/CoordinatorServerViewTest.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client;
 
+import static com.google.common.base.Predicates.alwaysTrue;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Function;
-import com.google.common.base.Predicates;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
@@ -264,8 +265,7 @@ public class CoordinatorServerViewTest extends CuratorTestBase {
 
   private void setupViews() throws Exception {
     baseView =
-        new BatchServerInventoryView(
-            zkPathsConfig, curator, jsonMapper, Predicates.alwaysTrue(), "test") {
+        new BatchServerInventoryView(zkPathsConfig, curator, jsonMapper, alwaysTrue(), "test") {
           @Override
           public void registerSegmentCallback(Executor exec, final SegmentCallback callback) {
             super.registerSegmentCallback(
diff --git a/server/src/test/java/org/apache/druid/client/HttpServerInventoryViewTest.java b/server/src/test/java/org/apache/druid/client/HttpServerInventoryViewTest.java
index 0311eae004..15b05a4b37 100644
--- a/server/src/test/java/org/apache/druid/client/HttpServerInventoryViewTest.java
+++ b/server/src/test/java/org/apache/druid/client/HttpServerInventoryViewTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.client;
 
+import static java.util.Collections.emptyMap;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
@@ -27,7 +29,6 @@ import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
 import java.io.ByteArrayInputStream;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.BlockingQueue;
@@ -257,7 +258,7 @@ public class HttpServerInventoryViewTest {
             new DiscoveryDruidNode(
                 new DruidNode("service", "host", false, 8080, null, true, false),
                 NodeRole.INDEXER,
-                Collections.emptyMap())));
+                emptyMap())));
 
     // test removal rogue node (announced a service as a DataNodeService but wasn't a
     // DataNodeService at the key)
diff --git a/server/src/test/java/org/apache/druid/client/JsonParserIteratorTest.java b/server/src/test/java/org/apache/druid/client/JsonParserIteratorTest.java
index bdea7b12ac..8f66854435 100644
--- a/server/src/test/java/org/apache/druid/client/JsonParserIteratorTest.java
+++ b/server/src/test/java/org/apache/druid/client/JsonParserIteratorTest.java
@@ -19,6 +19,11 @@
 
 package org.apache.druid.client;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyInt;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.JavaType;
 import com.fasterxml.jackson.databind.ObjectMapper;
@@ -50,12 +55,10 @@ import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 import org.junit.runners.Parameterized.Parameters;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
 
 @RunWith(Enclosed.class)
 public class JsonParserIteratorTest {
-  private static final JavaType JAVA_TYPE = Mockito.mock(JavaType.class);
+  private static final JavaType JAVA_TYPE = mock(JavaType.class);
   private static final String URL = "url";
   private static final String HOST = "host";
   private static final ObjectMapper OBJECT_MAPPER = new DefaultObjectMapper();
@@ -111,14 +114,11 @@ public class JsonParserIteratorTest {
 
     @Test
     public void testConvertIOExceptionToQueryInterruptedException() throws IOException {
-      InputStream exceptionThrowingStream = Mockito.mock(InputStream.class);
+      InputStream exceptionThrowingStream = mock(InputStream.class);
       IOException ioException = new IOException("ioexception test");
-      Mockito.when(exceptionThrowingStream.read()).thenThrow(ioException);
-      Mockito.when(exceptionThrowingStream.read(ArgumentMatchers.any())).thenThrow(ioException);
-      Mockito.when(
-              exceptionThrowingStream.read(
-                  ArgumentMatchers.any(), ArgumentMatchers.anyInt(), ArgumentMatchers.anyInt()))
-          .thenThrow(ioException);
+      when(exceptionThrowingStream.read()).thenThrow(ioException);
+      when(exceptionThrowingStream.read(any())).thenThrow(ioException);
+      when(exceptionThrowingStream.read(any(), anyInt(), anyInt())).thenThrow(ioException);
       JsonParserIterator<Object> iterator =
           new JsonParserIterator<>(
               JAVA_TYPE,
@@ -221,7 +221,7 @@ public class JsonParserIteratorTest {
       JsonParserIterator<?> iterator =
           new JsonParserIterator<>(
               JAVA_TYPE,
-              Mockito.mock(Future.class),
+              mock(Future.class),
               URL,
               mockQuery("qid", 0L), // should always timeout
               HOST,
@@ -280,10 +280,10 @@ public class JsonParserIteratorTest {
     }
 
     private Query<?> mockQuery(String queryId, long timeoutAt) {
-      Query<?> query = Mockito.mock(Query.class);
-      QueryContext context = Mockito.mock(QueryContext.class);
-      Mockito.when(query.getId()).thenReturn(queryId);
-      Mockito.when(query.context())
+      Query<?> query = mock(Query.class);
+      QueryContext context = mock(QueryContext.class);
+      when(query.getId()).thenReturn(queryId);
+      when(query.context())
           .thenReturn(
               QueryContext.of(ImmutableMap.of(DirectDruidClient.QUERY_FAIL_TIME, timeoutAt)));
       return query;
diff --git a/server/src/test/java/org/apache/druid/client/SimpleServerView.java b/server/src/test/java/org/apache/druid/client/SimpleServerView.java
index dab5ac3b3d..429c445cc5 100644
--- a/server/src/test/java/org/apache/druid/client/SimpleServerView.java
+++ b/server/src/test/java/org/apache/druid/client/SimpleServerView.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.client;
 
+import static java.util.Collections.emptyList;
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.Ordering;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -140,13 +141,12 @@ public class SimpleServerView implements TimelineServerView {
 
   @Override
   public List<ImmutableDruidServer> getDruidServers() {
-    return Collections.emptyList();
+    return emptyList();
   }
 
   @Override
   public <T> QueryRunner<T> getQueryRunner(DruidServer server) {
-    final QueryableDruidServer queryableDruidServer =
-        Preconditions.checkNotNull(servers.get(server), "server");
+    final QueryableDruidServer queryableDruidServer = requireNonNull(servers.get(server), "server");
     return (QueryRunner<T>) queryableDruidServer.getQueryRunner();
   }
 
diff --git a/server/src/test/java/org/apache/druid/client/cache/CachePopulatorTest.java b/server/src/test/java/org/apache/druid/client/cache/CachePopulatorTest.java
index 9cdb67c3f5..da928a4b29 100644
--- a/server/src/test/java/org/apache/druid/client/cache/CachePopulatorTest.java
+++ b/server/src/test/java/org/apache/druid/client/cache/CachePopulatorTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.MappingIterator;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
@@ -30,7 +32,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ExecutorService;
-import java.util.stream.Collectors;
 import org.apache.druid.java.util.common.concurrent.Execs;
 import org.apache.druid.java.util.common.guava.Sequences;
 import org.apache.druid.java.util.common.jackson.JacksonUtils;
@@ -141,7 +142,7 @@ public class CachePopulatorTest {
       Iterators.addAll(retVal, iterator);
 
       // Undo map-wrapping that was done in wrapAndReturn.
-      return retVal.stream().map(m -> m.get("s")).collect(Collectors.toList());
+      return retVal.stream().map(m -> m.get("s")).collect(toList());
     } catch (IOException e) {
       throw new RuntimeException(e);
     }
diff --git a/server/src/test/java/org/apache/druid/client/cache/CaffeineCacheTest.java b/server/src/test/java/org/apache/druid/client/cache/CaffeineCacheTest.java
index eb02cf2264..7ae30ca8f1 100644
--- a/server/src/test/java/org/apache/druid/client/cache/CaffeineCacheTest.java
+++ b/server/src/test/java/org/apache/druid/client/cache/CaffeineCacheTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.Collections.singletonList;
+import static java.util.UUID.randomUUID;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import com.google.common.primitives.Ints;
@@ -26,11 +29,9 @@ import com.google.inject.Inject;
 import com.google.inject.Injector;
 import com.google.inject.name.Names;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Random;
-import java.util.UUID;
 import java.util.concurrent.ForkJoinPool;
 import org.apache.druid.guice.GuiceInjectors;
 import org.apache.druid.guice.JsonConfigProvider;
@@ -94,7 +95,7 @@ public class CaffeineCacheTest {
 
   @Test
   public void testSimpleInjection() {
-    final String uuid = UUID.randomUUID().toString();
+    final String uuid = randomUUID().toString();
     System.setProperty(uuid + ".type", "caffeine");
     final Injector injector =
         Initialization.makeInjectorWithModules(
@@ -160,7 +161,7 @@ public class CaffeineCacheTest {
     Assert.assertEquals(10, Ints.fromByteArray(result.get(key2)));
 
     Cache.NamedKey missingKey = new Cache.NamedKey("missing", HI);
-    result = cache.getBulk(Collections.singletonList(missingKey));
+    result = cache.getBulk(singletonList(missingKey));
     Assert.assertEquals(result.size(), 0);
 
     result = cache.getBulk(new ArrayList<>());
diff --git a/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheBenchmark.java b/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheBenchmark.java
index e84f0ea300..d6a6c82a4c 100644
--- a/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheBenchmark.java
+++ b/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheBenchmark.java
@@ -44,10 +44,10 @@ public class MemcachedCacheBenchmark extends SimpleBenchmark {
       "test_2012-11-26T00:00:00.000Z_2012-11-27T00:00:00.000Z_2012-11-27T04:11:25.979Z_";
   private static byte[] randBytes;
 
-  @Param({"localhost:11211"})
+  @Param("localhost:11211")
   String hosts;
   // object size in kB
-  @Param({"1", "5", "10", "40"})
+  @Param({"1", "10", "40", "5"})
   int objectSize;
 
   @Param({"100", "1000"})
diff --git a/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheTest.java b/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheTest.java
index ce9730f0a2..817540784c 100644
--- a/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheTest.java
+++ b/server/src/test/java/org/apache/druid/client/cache/MemcachedCacheTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.client.cache;
 
+import static java.util.UUID.randomUUID;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Suppliers;
 import com.google.common.collect.ImmutableList;
@@ -36,7 +38,6 @@ import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 import java.util.Set;
-import java.util.UUID;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.CountDownLatch;
@@ -161,7 +162,7 @@ public class MemcachedCacheTest {
 
   @Test
   public void testSimpleInjection() {
-    final String uuid = UUID.randomUUID().toString();
+    final String uuid = randomUUID().toString();
     System.setProperty(uuid + ".type", "memcached");
     System.setProperty(uuid + ".hosts", "localhost");
     final Injector injector =
diff --git a/server/src/test/java/org/apache/druid/client/client/BatchServerInventoryViewTest.java b/server/src/test/java/org/apache/druid/client/client/BatchServerInventoryViewTest.java
index 6fd39f3787..468bb785cb 100644
--- a/server/src/test/java/org/apache/druid/client/client/BatchServerInventoryViewTest.java
+++ b/server/src/test/java/org/apache/druid/client/client/BatchServerInventoryViewTest.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.client.client;
 
+import static com.google.common.base.Predicates.alwaysTrue;
+import static java.util.Comparator.comparing;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Predicate;
-import com.google.common.base.Predicates;
 import com.google.common.base.Stopwatch;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Sets;
@@ -158,7 +160,7 @@ public class BatchServerInventoryViewTest {
             },
             cf,
             jsonMapper,
-            Predicates.alwaysTrue(),
+            alwaysTrue(),
             "test");
 
     batchServerInventoryView.start();
@@ -276,7 +278,7 @@ public class BatchServerInventoryViewTest {
     ServerView.SegmentCallback callback =
         EasyMock.createStrictMock(ServerView.SegmentCallback.class);
     Comparator<DataSegment> dataSegmentComparator =
-        Comparator.comparing(DataSegment::getInterval, Comparators.intervalsByStartThenEnd());
+        comparing(DataSegment::getInterval, Comparators.intervalsByStartThenEnd());
 
     EasyMock.expect(
             callback.segmentAdded(
@@ -360,7 +362,7 @@ public class BatchServerInventoryViewTest {
       throws Exception {
     final Timing forWaitingTiming = TIMING.forWaiting();
     Stopwatch stopwatch = Stopwatch.createStarted();
-    while (Iterables.isEmpty(batchServerInventoryView.getInventory())
+    while (batchServerInventoryView.getInventory().isEmpty()
         || Iterables.size(
                 Iterables.get(batchServerInventoryView.getInventory(), 0).iterateAllSegments())
             != testSegments.size()) {
diff --git a/server/src/test/java/org/apache/druid/client/indexing/HttpIndexingServiceClientTest.java b/server/src/test/java/org/apache/druid/client/indexing/HttpIndexingServiceClientTest.java
index 5b7cb0fdcb..04f278868d 100644
--- a/server/src/test/java/org/apache/druid/client/indexing/HttpIndexingServiceClientTest.java
+++ b/server/src/test/java/org/apache/druid/client/indexing/HttpIndexingServiceClientTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.client.indexing;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.net.URL;
-import java.nio.charset.StandardCharsets;
 import java.util.Map;
 import org.apache.druid.data.input.impl.StringDimensionSchema;
 import org.apache.druid.discovery.DruidLeaderClient;
@@ -104,7 +105,7 @@ public class HttpIndexingServiceClientTest {
     EasyMock.replay(response);
 
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(response, StandardCharsets.UTF_8)
+        new StringFullResponseHolder(response, UTF_8)
             .addChunk(jsonMapper.writeValueAsString(samplerResponse));
 
     EasyMock.expect(druidLeaderClient.go(EasyMock.anyObject(Request.class)))
@@ -163,7 +164,7 @@ public class HttpIndexingServiceClientTest {
     EasyMock.replay(response);
 
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(response, StandardCharsets.UTF_8).addChunk("");
+        new StringFullResponseHolder(response, UTF_8).addChunk("");
 
     EasyMock.expect(druidLeaderClient.go(EasyMock.anyObject(Request.class)))
         .andReturn(responseHolder)
@@ -190,7 +191,7 @@ public class HttpIndexingServiceClientTest {
     Map<String, Object> dummyResponse = ImmutableMap.of("test", "value");
 
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(response, StandardCharsets.UTF_8)
+        new StringFullResponseHolder(response, UTF_8)
             .addChunk(jsonMapper.writeValueAsString(dummyResponse));
 
     EasyMock.expect(druidLeaderClient.go(EasyMock.anyObject(Request.class)))
@@ -221,14 +222,14 @@ public class HttpIndexingServiceClientTest {
         "No task reports were found for this task. "
             + "The task may not exist, or it may not have completed yet.";
     ChannelBuffer buf = ChannelBuffers.buffer(errorMsg.length());
-    buf.writeBytes(errorMsg.getBytes(StandardCharsets.UTF_8));
+    buf.writeBytes(errorMsg.getBytes(UTF_8));
 
     EasyMock.expect(response.getStatus()).andReturn(HttpResponseStatus.NOT_FOUND).anyTimes();
     EasyMock.expect(response.getContent()).andReturn(buf);
     EasyMock.replay(response);
 
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(response, StandardCharsets.UTF_8).addChunk("");
+        new StringFullResponseHolder(response, UTF_8).addChunk("");
 
     EasyMock.expect(druidLeaderClient.go(EasyMock.anyObject(Request.class)))
         .andReturn(responseHolder)
@@ -259,7 +260,7 @@ public class HttpIndexingServiceClientTest {
     EasyMock.replay(response);
 
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(response, StandardCharsets.UTF_8).addChunk("");
+        new StringFullResponseHolder(response, UTF_8).addChunk("");
 
     EasyMock.expect(druidLeaderClient.go(EasyMock.anyObject(Request.class)))
         .andReturn(responseHolder)
@@ -305,7 +306,7 @@ public class HttpIndexingServiceClientTest {
     EasyMock.replay(response);
 
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(response, StandardCharsets.UTF_8)
+        new StringFullResponseHolder(response, UTF_8)
             .addChunk(jsonMapper.writeValueAsString(ImmutableMap.of("task", "aaa")));
 
     EasyMock.expect(druidLeaderClient.makeRequest(HttpMethod.POST, "/druid/indexer/v1/task"))
@@ -368,7 +369,7 @@ public class HttpIndexingServiceClientTest {
     EasyMock.replay(response);
 
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(response, StandardCharsets.UTF_8)
+        new StringFullResponseHolder(response, UTF_8)
             .addChunk(jsonMapper.writeValueAsString(ImmutableMap.of("task", "aaa")));
 
     EasyMock.expect(druidLeaderClient.makeRequest(HttpMethod.POST, "/druid/indexer/v1/task"))
@@ -431,7 +432,7 @@ public class HttpIndexingServiceClientTest {
     IndexingTotalWorkerCapacityInfo indexingTotalWorkerCapacityInfo =
         new IndexingTotalWorkerCapacityInfo(currentClusterCapacity, maximumCapacityWithAutoScale);
     StringFullResponseHolder autoScaleResponseHolder =
-        new StringFullResponseHolder(totalWorkerCapacityResponse, StandardCharsets.UTF_8)
+        new StringFullResponseHolder(totalWorkerCapacityResponse, UTF_8)
             .addChunk(jsonMapper.writeValueAsString(indexingTotalWorkerCapacityInfo));
     EasyMock.expect(druidLeaderClient.go(EasyMock.anyObject(Request.class)))
         .andReturn(autoScaleResponseHolder)
diff --git a/server/src/test/java/org/apache/druid/client/selector/TierSelectorStrategyTest.java b/server/src/test/java/org/apache/druid/client/selector/TierSelectorStrategyTest.java
index 0b2b2442d4..2515adc78f 100644
--- a/server/src/test/java/org/apache/druid/client/selector/TierSelectorStrategyTest.java
+++ b/server/src/test/java/org/apache/druid/client/selector/TierSelectorStrategyTest.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.client.selector;
 
+import static java.util.Collections.shuffle;
+
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -230,7 +231,7 @@ public class TierSelectorStrategyTest {
     for (QueryableDruidServer server : servers) {
       expectedCandidates.add(server.getServer().getMetadata());
     }
-    Collections.shuffle(servers);
+    shuffle(servers);
     for (QueryableDruidServer server : servers) {
       serverSelector.addServerAndUpdateSegment(server, serverSelector.getSegment());
     }
diff --git a/server/src/test/java/org/apache/druid/curator/CuratorModuleTest.java b/server/src/test/java/org/apache/druid/curator/CuratorModuleTest.java
index d3b4652484..ba0d9aafea 100644
--- a/server/src/test/java/org/apache/druid/curator/CuratorModuleTest.java
+++ b/server/src/test/java/org/apache/druid/curator/CuratorModuleTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.curator;
 
+import static org.hamcrest.Matchers.instanceOf;
+
 import com.google.inject.Injector;
 import java.util.List;
 import java.util.Properties;
@@ -34,7 +36,6 @@ import org.apache.logging.log4j.Level;
 import org.apache.logging.log4j.core.LogEvent;
 import org.hamcrest.CoreMatchers;
 import org.hamcrest.MatcherAssert;
-import org.hamcrest.Matchers;
 import org.junit.Assert;
 import org.junit.Ignore;
 import org.junit.Rule;
@@ -59,7 +60,7 @@ public final class CuratorModuleTest {
     Assert.assertEquals(config.getZkConnectionTimeoutMs(), client.getConnectionTimeoutMs());
 
     MatcherAssert.assertThat(
-        client.getRetryPolicy(), Matchers.instanceOf(BoundedExponentialBackoffRetry.class));
+        client.getRetryPolicy(), instanceOf(BoundedExponentialBackoffRetry.class));
     BoundedExponentialBackoffRetry retryPolicy =
         (BoundedExponentialBackoffRetry) client.getRetryPolicy();
     Assert.assertEquals(CuratorModule.BASE_SLEEP_TIME_MS, retryPolicy.getBaseSleepTimeMs());
diff --git a/server/src/test/java/org/apache/druid/curator/inventory/CuratorInventoryManagerTest.java b/server/src/test/java/org/apache/druid/curator/inventory/CuratorInventoryManagerTest.java
index f9592ab04a..c2d8966286 100644
--- a/server/src/test/java/org/apache/druid/curator/inventory/CuratorInventoryManagerTest.java
+++ b/server/src/test/java/org/apache/druid/curator/inventory/CuratorInventoryManagerTest.java
@@ -65,7 +65,7 @@ public class CuratorInventoryManagerTest extends CuratorTestBase {
 
     manager.start();
 
-    Assert.assertTrue(Iterables.isEmpty(manager.getInventory()));
+    Assert.assertTrue(manager.getInventory().isEmpty());
 
     CountDownLatch containerLatch = new CountDownLatch(1);
     strategy.setNewContainerLatch(containerLatch);
diff --git a/server/src/test/java/org/apache/druid/discovery/BaseNodeRoleWatcherTest.java b/server/src/test/java/org/apache/druid/discovery/BaseNodeRoleWatcherTest.java
index 405d808a61..6fe0a1c1aa 100644
--- a/server/src/test/java/org/apache/druid/discovery/BaseNodeRoleWatcherTest.java
+++ b/server/src/test/java/org/apache/druid/discovery/BaseNodeRoleWatcherTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.discovery;
 
+import static java.util.Collections.emptyList;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.LinkedHashMap;
 import java.util.List;
@@ -61,8 +62,8 @@ public class BaseNodeRoleWatcherTest {
     nodeRoleWatcher.registerListener(listener2);
     nodeRoleWatcher.childRemoved(broker2);
 
-    assertListener(listener1, false, Collections.emptyList(), Collections.emptyList());
-    assertListener(listener2, false, Collections.emptyList(), Collections.emptyList());
+    assertListener(listener1, false, emptyList(), emptyList());
+    assertListener(listener2, false, emptyList(), emptyList());
 
     nodeRoleWatcher.cacheInitialized();
 
@@ -73,9 +74,9 @@ public class BaseNodeRoleWatcherTest {
     Assert.assertTrue(presentNodes.contains(broker1));
     Assert.assertTrue(presentNodes.contains(broker3));
 
-    assertListener(listener1, true, presentNodes, Collections.emptyList());
-    assertListener(listener2, true, presentNodes, Collections.emptyList());
-    assertListener(listener3, true, presentNodes, Collections.emptyList());
+    assertListener(listener1, true, presentNodes, emptyList());
+    assertListener(listener2, true, presentNodes, emptyList());
+    assertListener(listener3, true, presentNodes, emptyList());
 
     nodeRoleWatcher.childRemoved(notBroker);
     nodeRoleWatcher.childRemoved(broker2);
diff --git a/server/src/test/java/org/apache/druid/discovery/DruidLeaderClientTest.java b/server/src/test/java/org/apache/druid/discovery/DruidLeaderClientTest.java
index e8a35e86c7..24159c5aba 100644
--- a/server/src/test/java/org/apache/druid/discovery/DruidLeaderClientTest.java
+++ b/server/src/test/java/org/apache/druid/discovery/DruidLeaderClientTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.discovery;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.inject.Binder;
@@ -31,7 +33,6 @@ import com.google.inject.name.Names;
 import com.google.inject.servlet.GuiceFilter;
 import java.io.IOException;
 import java.net.URI;
-import java.nio.charset.StandardCharsets;
 import javax.ws.rs.GET;
 import javax.ws.rs.POST;
 import javax.ws.rs.Path;
@@ -120,7 +121,7 @@ public class DruidLeaderClientTest extends BaseJettyTest {
     druidLeaderClient.start();
 
     Request request = druidLeaderClient.makeRequest(HttpMethod.POST, "/simple/direct");
-    request.setContent("hello".getBytes(StandardCharsets.UTF_8));
+    request.setContent("hello".getBytes(UTF_8));
     Assert.assertEquals("hello", druidLeaderClient.go(request).getContent());
   }
 
@@ -165,7 +166,7 @@ public class DruidLeaderClientTest extends BaseJettyTest {
     druidLeaderClient.start();
 
     Request request = druidLeaderClient.makeRequest(HttpMethod.POST, "/simple/redirect");
-    request.setContent("hello".getBytes(StandardCharsets.UTF_8));
+    request.setContent("hello".getBytes(UTF_8));
     Assert.assertEquals("hello", druidLeaderClient.go(request).getContent());
   }
 
@@ -195,7 +196,7 @@ public class DruidLeaderClientTest extends BaseJettyTest {
     druidLeaderClient.start();
 
     Request request = druidLeaderClient.makeRequest(HttpMethod.POST, "/simple/redirect");
-    request.setContent("hello".getBytes(StandardCharsets.UTF_8));
+    request.setContent("hello".getBytes(UTF_8));
     Assert.assertEquals("hello", druidLeaderClient.go(request).getContent());
   }
 
diff --git a/server/src/test/java/org/apache/druid/guice/FirehoseModuleTest.java b/server/src/test/java/org/apache/druid/guice/FirehoseModuleTest.java
index be64de16ed..659f378fab 100644
--- a/server/src/test/java/org/apache/druid/guice/FirehoseModuleTest.java
+++ b/server/src/test/java/org/apache/druid/guice/FirehoseModuleTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.guice;
 
+import static java.util.stream.Collectors.toSet;
+
 import com.fasterxml.jackson.databind.Module;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.cfg.MapperConfig;
@@ -32,7 +34,6 @@ import java.net.URLClassLoader;
 import java.util.Collection;
 import java.util.Set;
 import java.util.function.Predicate;
-import java.util.stream.Collectors;
 import org.apache.druid.data.input.FirehoseFactory;
 import org.apache.druid.segment.realtime.firehose.ClippedFirehoseFactory;
 import org.apache.druid.utils.JvmUtils;
@@ -70,7 +71,7 @@ public class FirehoseModuleTest {
     return subtypes.stream()
         .map(NamedType::getType)
         .filter(c -> !c.equals(parentClass))
-        .collect(Collectors.toSet());
+        .collect(toSet());
   }
 
   @SuppressWarnings("UnstableApiUsage") // for ClassPath
@@ -83,6 +84,6 @@ public class FirehoseModuleTest {
     return classPath.getTopLevelClasses(packageName).stream()
         .map(ClassPath.ClassInfo::load)
         .filter(IS_FIREHOSE_FACTORY)
-        .collect(Collectors.toSet());
+        .collect(toSet());
   }
 }
diff --git a/server/src/test/java/org/apache/druid/guice/JoinableFactoryModuleTest.java b/server/src/test/java/org/apache/druid/guice/JoinableFactoryModuleTest.java
index babac82323..24c41ede07 100644
--- a/server/src/test/java/org/apache/druid/guice/JoinableFactoryModuleTest.java
+++ b/server/src/test/java/org/apache/druid/guice/JoinableFactoryModuleTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.guice;
 
+import static java.util.Collections.emptyMap;
+
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Guice;
 import com.google.inject.Injector;
@@ -26,7 +28,6 @@ import com.google.inject.Key;
 import com.google.inject.Module;
 import com.google.inject.Scopes;
 import com.google.inject.TypeLiteral;
-import java.util.Collections;
 import java.util.Map;
 import java.util.Set;
 import org.apache.druid.query.DataSource;
@@ -117,7 +118,7 @@ public class JoinableFactoryModuleTest {
 
   private Injector makeInjectorWithProperties(Module... otherModules) {
     final LookupExtractorFactoryContainerProvider lookupProvider =
-        LookupEnabledTestExprMacroTable.createTestLookupProvider(Collections.emptyMap());
+        LookupEnabledTestExprMacroTable.createTestLookupProvider(emptyMap());
 
     final ImmutableList.Builder<Module> modulesBuilder =
         ImmutableList.<Module>builder()
diff --git a/server/src/test/java/org/apache/druid/guice/JsonConfigTesterBase.java b/server/src/test/java/org/apache/druid/guice/JsonConfigTesterBase.java
index 395018ee0e..2e2b9cb749 100644
--- a/server/src/test/java/org/apache/druid/guice/JsonConfigTesterBase.java
+++ b/server/src/test/java/org/apache/druid/guice/JsonConfigTesterBase.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.guice;
 
+import static java.util.UUID.randomUUID;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Binder;
@@ -33,7 +35,6 @@ import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Properties;
-import java.util.UUID;
 import org.apache.druid.initialization.Initialization;
 import org.apache.druid.java.util.common.StringUtils;
 import org.easymock.EasyMock;
@@ -60,7 +61,7 @@ public abstract class JsonConfigTesterBase<T> {
 
   protected static String getPropertyKey(Field field) {
     JsonProperty jsonProperty = field.getAnnotation(JsonProperty.class);
-    if (null != jsonProperty) {
+    if (jsonProperty != null) {
       return getPropertyKey(
           (jsonProperty.value() == null || jsonProperty.value().isEmpty())
               ? field.getName()
@@ -84,7 +85,7 @@ public abstract class JsonConfigTesterBase<T> {
       throws IllegalAccessException, NoSuchMethodException, InvocationTargetException {
     for (Field field : clazz.getDeclaredFields()) {
       final String propertyKey = getPropertyKey(field);
-      if (null != propertyKey) {
+      if (propertyKey != null) {
         field.setAccessible(true);
         String getter =
             StringUtils.format(
@@ -93,7 +94,7 @@ public abstract class JsonConfigTesterBase<T> {
                 field.getName().substring(1));
         Method method = clazz.getDeclaredMethod(getter);
         final String value;
-        if (null != method) {
+        if (method != null) {
           value = method.invoke(config).toString();
         } else {
           value = field.get(config).toString();
@@ -116,11 +117,11 @@ public abstract class JsonConfigTesterBase<T> {
     testProperties.clear();
     for (Field field : clazz.getDeclaredFields()) {
       final String propertyKey = getPropertyKey(field);
-      if (null != propertyKey) {
+      if (propertyKey != null) {
         field.setAccessible(true);
         Class<?> fieldType = field.getType();
         if (String.class.isAssignableFrom(fieldType)) {
-          propertyValues.put(propertyKey, UUID.randomUUID().toString());
+          propertyValues.put(propertyKey, randomUUID().toString());
         } else if (Collection.class.isAssignableFrom(fieldType)) {
           propertyValues.put(propertyKey, "[]");
         } else if (Map.class.isAssignableFrom(fieldType)) {
diff --git a/server/src/test/java/org/apache/druid/guice/StorageNodeModuleTest.java b/server/src/test/java/org/apache/druid/guice/StorageNodeModuleTest.java
index f8bed62be2..8000539fba 100644
--- a/server/src/test/java/org/apache/druid/guice/StorageNodeModuleTest.java
+++ b/server/src/test/java/org/apache/druid/guice/StorageNodeModuleTest.java
@@ -19,13 +19,18 @@
 
 package org.apache.druid.guice;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singletonList;
+import static org.mockito.Answers.RETURNS_MOCKS;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Injector;
 import com.google.inject.Key;
 import com.google.inject.ProvisionException;
 import com.google.inject.name.Names;
 import com.google.inject.util.Modules;
-import java.util.Collections;
 import org.apache.druid.discovery.DataNodeService;
 import org.apache.druid.guice.annotations.Self;
 import org.apache.druid.initialization.Initialization;
@@ -41,9 +46,7 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
-import org.mockito.Answers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -52,10 +55,10 @@ public class StorageNodeModuleTest {
 
   @Rule public ExpectedException exceptionRule = ExpectedException.none();
 
-  @Mock(answer = Answers.RETURNS_MOCKS)
+  @Mock(answer = RETURNS_MOCKS)
   private DruidNode self;
 
-  @Mock(answer = Answers.RETURNS_MOCKS)
+  @Mock(answer = RETURNS_MOCKS)
   private ServerTypeConfig serverTypeConfig;
 
   @Mock private DruidProcessingConfig druidProcessingConfig;
@@ -67,8 +70,7 @@ public class StorageNodeModuleTest {
 
   @Before
   public void setUp() {
-    Mockito.when(segmentLoaderConfig.getLocations())
-        .thenReturn(Collections.singletonList(storageLocation));
+    when(segmentLoaderConfig.getLocations()).thenReturn(singletonList(storageLocation));
 
     target = new StorageNodeModule();
     injector = makeInjector(INJECT_SERVER_TYPE_CONFIG);
@@ -106,7 +108,7 @@ public class StorageNodeModuleTest {
   public void getDataNodeServiceWithNoSegmentCacheConfiguredThrowProvisionException() {
     exceptionRule.expect(ProvisionException.class);
     exceptionRule.expectMessage("druid.segmentCache.locations must be set on historicals.");
-    Mockito.doReturn(ServerType.HISTORICAL).when(serverTypeConfig).getServerType();
+    doReturn(ServerType.HISTORICAL).when(serverTypeConfig).getServerType();
     mockSegmentCacheNotConfigured();
     injector.getInstance(DataNodeService.class);
   }
@@ -174,6 +176,6 @@ public class StorageNodeModuleTest {
   }
 
   private void mockSegmentCacheNotConfigured() {
-    Mockito.doReturn(Collections.emptyList()).when(segmentLoaderConfig).getLocations();
+    doReturn(emptyList()).when(segmentLoaderConfig).getLocations();
   }
 }
diff --git a/server/src/test/java/org/apache/druid/indexing/NoopSupervisorSpecTest.java b/server/src/test/java/org/apache/druid/indexing/NoopSupervisorSpecTest.java
index 2b17d75b6b..5e7dd368cd 100644
--- a/server/src/test/java/org/apache/druid/indexing/NoopSupervisorSpecTest.java
+++ b/server/src/test/java/org/apache/druid/indexing/NoopSupervisorSpecTest.java
@@ -19,7 +19,8 @@
 
 package org.apache.druid.indexing;
 
-import java.util.Collections;
+import static java.util.Collections.singletonList;
+
 import java.util.concurrent.Callable;
 import org.apache.druid.indexing.overlord.supervisor.NoopSupervisorSpec;
 import org.apache.druid.indexing.overlord.supervisor.Supervisor;
@@ -34,7 +35,7 @@ public class NoopSupervisorSpecTest {
     Exception e = null;
     try {
       NoopSupervisorSpec noopSupervisorSpec =
-          new NoopSupervisorSpec(null, Collections.singletonList("datasource1"));
+          new NoopSupervisorSpec(null, singletonList("datasource1"));
       Supervisor supervisor = noopSupervisorSpec.createSupervisor();
       SupervisorTaskAutoScaler autoscaler = noopSupervisorSpec.createAutoscaler(supervisor);
       Assert.assertNull(autoscaler);
@@ -65,7 +66,7 @@ public class NoopSupervisorSpecTest {
   @Test
   public void testInputSourceResources() {
     NoopSupervisorSpec noopSupervisorSpec =
-        new NoopSupervisorSpec(null, Collections.singletonList("datasource1"));
+        new NoopSupervisorSpec(null, singletonList("datasource1"));
     Assert.assertTrue(noopSupervisorSpec.getInputSourceResources().isEmpty());
   }
 }
diff --git a/server/src/test/java/org/apache/druid/indexing/overlord/supervisor/SupervisorSpecTest.java b/server/src/test/java/org/apache/druid/indexing/overlord/supervisor/SupervisorSpecTest.java
index f043c5c766..d6089bed87 100644
--- a/server/src/test/java/org/apache/druid/indexing/overlord/supervisor/SupervisorSpecTest.java
+++ b/server/src/test/java/org/apache/druid/indexing/overlord/supervisor/SupervisorSpecTest.java
@@ -55,6 +55,6 @@ public class SupervisorSpecTest {
 
   @Test
   public void test() {
-    Assert.assertThrows(UOE.class, () -> SUPERVISOR_SPEC.getInputSourceResources());
+    Assert.assertThrows(UOE.class, SUPERVISOR_SPEC::getInputSourceResources);
   }
 }
diff --git a/server/src/test/java/org/apache/druid/initialization/ComposingEmitterModuleTest.java b/server/src/test/java/org/apache/druid/initialization/ComposingEmitterModuleTest.java
index 4c32fb3bfb..e0290c479d 100644
--- a/server/src/test/java/org/apache/druid/initialization/ComposingEmitterModuleTest.java
+++ b/server/src/test/java/org/apache/druid/initialization/ComposingEmitterModuleTest.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.initialization;
 
+import static java.util.Collections.singletonList;
+
 import com.google.inject.Binder;
 import com.google.inject.Guice;
 import com.google.inject.Injector;
 import com.google.inject.Key;
 import com.google.inject.Module;
 import com.google.inject.name.Names;
-import java.util.Collections;
 import java.util.Properties;
 import javax.validation.Validation;
 import javax.validation.Validator;
@@ -53,9 +54,7 @@ public class ComposingEmitterModuleTest {
   @Test
   public void testGetEmitter() {
     ComposingEmitterConfig config = EasyMock.createMock(ComposingEmitterConfig.class);
-    EasyMock.expect(config.getEmitters())
-        .andReturn(Collections.singletonList(testEmitterType))
-        .anyTimes();
+    EasyMock.expect(config.getEmitters()).andReturn(singletonList(testEmitterType)).anyTimes();
 
     Injector injector = EasyMock.createMock(Injector.class);
     EasyMock.expect(injector.getInstance(Key.get(Emitter.class, Names.named(testEmitterType))))
diff --git a/server/src/test/java/org/apache/druid/initialization/ServerInjectorBuilderTest.java b/server/src/test/java/org/apache/druid/initialization/ServerInjectorBuilderTest.java
index 64777d60e6..fa615fdfe0 100644
--- a/server/src/test/java/org/apache/druid/initialization/ServerInjectorBuilderTest.java
+++ b/server/src/test/java/org/apache/druid/initialization/ServerInjectorBuilderTest.java
@@ -196,7 +196,7 @@ public class ServerInjectorBuilderTest {
         "I am Druid", injector.getInstance(Key.get(String.class, Names.named("emperor"))));
   }
 
-  @LoadScope(roles = {"emperor", "druid"})
+  @LoadScope(roles = {"druid", "emperor"})
   private static class LoadOnAnnotationTestModule implements com.google.inject.Module {
     @Override
     public void configure(Binder binder) {
diff --git a/server/src/test/java/org/apache/druid/initialization/ZkPathsConfigTest.java b/server/src/test/java/org/apache/druid/initialization/ZkPathsConfigTest.java
index d38075e02c..361c834510 100644
--- a/server/src/test/java/org/apache/druid/initialization/ZkPathsConfigTest.java
+++ b/server/src/test/java/org/apache/druid/initialization/ZkPathsConfigTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.initialization;
 
+import static java.util.UUID.randomUUID;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.inject.Key;
 import java.io.IOException;
 import java.lang.reflect.InvocationTargetException;
-import java.util.UUID;
 import org.apache.curator.utils.ZKPaths;
 import org.apache.druid.guice.JsonConfigProvider;
 import org.apache.druid.guice.JsonConfigTesterBase;
@@ -44,7 +45,7 @@ public class ZkPathsConfigTest extends JsonConfigTesterBase<ZkPathsConfig> {
     JsonConfigProvider<ZkPathsConfig> zkPathsConfig =
         JsonConfigProvider.of(CONFIG_PREFIX, ZkPathsConfig.class);
     testProperties.clear();
-    String base = UUID.randomUUID().toString();
+    String base = randomUUID().toString();
     testProperties.put(StringUtils.format("%s.base", CONFIG_PREFIX), base);
     zkPathsConfig.inject(testProperties, configurator);
 
diff --git a/server/src/test/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java b/server/src/test/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java
index c857ece603..d1de3d1563 100644
--- a/server/src/test/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinatorTest.java
@@ -19,22 +19,26 @@
 
 package org.apache.druid.metadata;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singleton;
+import static java.util.Collections.singletonList;
+import static java.util.Comparator.naturalOrder;
+import static java.util.stream.Collectors.toList;
+import static org.assertj.core.api.Assertions.assertThat;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
 import java.io.IOException;
-import java.nio.charset.StandardCharsets;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.Collections;
-import java.util.Comparator;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicLong;
-import java.util.stream.Collectors;
 import org.apache.druid.data.input.StringTuple;
 import org.apache.druid.indexing.overlord.DataSourceMetadata;
 import org.apache.druid.indexing.overlord.ObjectMetadata;
@@ -60,7 +64,6 @@ import org.apache.druid.timeline.partition.NumberedShardSpec;
 import org.apache.druid.timeline.partition.PartialShardSpec;
 import org.apache.druid.timeline.partition.PartitionIds;
 import org.apache.druid.timeline.partition.SingleDimensionShardSpec;
-import org.assertj.core.api.Assertions;
 import org.joda.time.DateTime;
 import org.joda.time.Interval;
 import org.junit.Assert;
@@ -449,7 +452,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     coordinator.announceHistoricalSegments(SEGMENTS);
     for (DataSegment segment : SEGMENTS) {
       Assert.assertArrayEquals(
-          mapper.writeValueAsString(segment).getBytes(StandardCharsets.UTF_8),
+          mapper.writeValueAsString(segment).getBytes(UTF_8),
           derbyConnector.lookup(
               derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
               "id",
@@ -485,7 +488,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     coordinator.announceHistoricalSegments(segments);
     for (DataSegment segment : segments) {
       Assert.assertArrayEquals(
-          mapper.writeValueAsString(segment).getBytes(StandardCharsets.UTF_8),
+          mapper.writeValueAsString(segment).getBytes(UTF_8),
           derbyConnector.lookup(
               derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
               "id",
@@ -496,8 +499,8 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     List<String> segmentIds =
         segments.stream()
             .map(segment -> segment.getId().toString())
-            .sorted(Comparator.naturalOrder())
-            .collect(Collectors.toList());
+            .sorted(naturalOrder())
+            .collect(toList());
 
     Assert.assertEquals(segmentIds, retrieveUsedSegmentIds());
 
@@ -514,7 +517,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
 
     for (DataSegment segment : segments) {
       Assert.assertArrayEquals(
-          mapper.writeValueAsString(segment).getBytes(StandardCharsets.UTF_8),
+          mapper.writeValueAsString(segment).getBytes(UTF_8),
           derbyConnector.lookup(
               derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
               "id",
@@ -538,7 +541,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     Assert.assertEquals(SegmentPublishResult.ok(ImmutableSet.of(defaultSegment)), result1);
 
     Assert.assertArrayEquals(
-        mapper.writeValueAsString(defaultSegment).getBytes(StandardCharsets.UTF_8),
+        mapper.writeValueAsString(defaultSegment).getBytes(UTF_8),
         derbyConnector.lookup(
             derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
             "id",
@@ -555,7 +558,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     Assert.assertEquals(SegmentPublishResult.ok(ImmutableSet.of(defaultSegment2)), result2);
 
     Assert.assertArrayEquals(
-        mapper.writeValueAsString(defaultSegment2).getBytes(StandardCharsets.UTF_8),
+        mapper.writeValueAsString(defaultSegment2).getBytes(UTF_8),
         derbyConnector.lookup(
             derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
             "id",
@@ -605,7 +608,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     Assert.assertEquals(SegmentPublishResult.ok(ImmutableSet.of(defaultSegment)), result1);
 
     Assert.assertArrayEquals(
-        mapper.writeValueAsString(defaultSegment).getBytes(StandardCharsets.UTF_8),
+        mapper.writeValueAsString(defaultSegment).getBytes(UTF_8),
         derbyConnector.lookup(
             derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
             "id",
@@ -625,7 +628,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     Assert.assertEquals(SegmentPublishResult.ok(ImmutableSet.of(defaultSegment2)), result2);
 
     Assert.assertArrayEquals(
-        mapper.writeValueAsString(defaultSegment2).getBytes(StandardCharsets.UTF_8),
+        mapper.writeValueAsString(defaultSegment2).getBytes(UTF_8),
         derbyConnector.lookup(
             derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
             "id",
@@ -714,7 +717,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
 
     for (DataSegment segment : SEGMENTS) {
       Assert.assertArrayEquals(
-          mapper.writeValueAsString(segment).getBytes(StandardCharsets.UTF_8),
+          mapper.writeValueAsString(segment).getBytes(UTF_8),
           derbyConnector.lookup(
               derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
               "id",
@@ -824,21 +827,21 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     coordinator.announceHistoricalSegments(SEGMENTS);
     coordinator.announceHistoricalSegments(ImmutableSet.of(defaultSegment3));
 
-    Assertions.assertThat(
+    assertThat(
             coordinator.retrieveUsedSegmentsForIntervals(
                 defaultSegment.getDataSource(),
                 ImmutableList.of(defaultSegment.getInterval()),
                 Segments.ONLY_VISIBLE))
         .containsOnlyOnce(SEGMENTS.toArray(new DataSegment[0]));
 
-    Assertions.assertThat(
+    assertThat(
             coordinator.retrieveUsedSegmentsForIntervals(
                 defaultSegment.getDataSource(),
                 ImmutableList.of(defaultSegment3.getInterval()),
                 Segments.ONLY_VISIBLE))
         .containsOnlyOnce(defaultSegment3);
 
-    Assertions.assertThat(
+    assertThat(
             coordinator.retrieveUsedSegmentsForIntervals(
                 defaultSegment.getDataSource(),
                 ImmutableList.of(defaultSegment.getInterval(), defaultSegment3.getInterval()),
@@ -846,7 +849,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
         .containsOnlyOnce(defaultSegment, defaultSegment2, defaultSegment3);
 
     // case to check no duplication if two intervals overlapped with the interval of same segment.
-    Assertions.assertThat(
+    assertThat(
             coordinator.retrieveUsedSegmentsForIntervals(
                 defaultSegment.getDataSource(),
                 ImmutableList.of(
@@ -1271,7 +1274,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
 
     for (DataSegment segment : segments) {
       Assert.assertArrayEquals(
-          mapper.writeValueAsString(segment).getBytes(StandardCharsets.UTF_8),
+          mapper.writeValueAsString(segment).getBytes(UTF_8),
           derbyConnector.lookup(
               derbyConnectorRule.metadataTablesConfigSupplier().get().getSegmentsTable(),
               "id",
@@ -1280,7 +1283,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     }
 
     Assert.assertEquals(
-        segments.stream().map(segment -> segment.getId().toString()).collect(Collectors.toList()),
+        segments.stream().map(segment -> segment.getId().toString()).collect(toList()),
         retrieveUsedSegmentIds());
 
     // Should not update dataSource metadata.
@@ -1668,8 +1671,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
         new SegmentCreateRequest(sequenceName, null, "v1", partialShardSpec);
     final SegmentIdWithShardSpec segmentId0 =
         coordinator
-            .allocatePendingSegments(
-                dataSource, interval, false, Collections.singletonList(request))
+            .allocatePendingSegments(dataSource, interval, false, singletonList(request))
             .get(request);
 
     Assert.assertEquals(
@@ -1680,8 +1682,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
             sequenceName, segmentId0.toString(), segmentId0.getVersion(), partialShardSpec);
     final SegmentIdWithShardSpec segmentId1 =
         coordinator
-            .allocatePendingSegments(
-                dataSource, interval, false, Collections.singletonList(request1))
+            .allocatePendingSegments(dataSource, interval, false, singletonList(request1))
             .get(request1);
 
     Assert.assertEquals(
@@ -1692,8 +1693,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
             sequenceName, segmentId1.toString(), segmentId1.getVersion(), partialShardSpec);
     final SegmentIdWithShardSpec segmentId2 =
         coordinator
-            .allocatePendingSegments(
-                dataSource, interval, false, Collections.singletonList(request2))
+            .allocatePendingSegments(dataSource, interval, false, singletonList(request2))
             .get(request2);
 
     Assert.assertEquals(
@@ -1704,8 +1704,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
             sequenceName, segmentId1.toString(), segmentId1.getVersion(), partialShardSpec);
     final SegmentIdWithShardSpec segmentId3 =
         coordinator
-            .allocatePendingSegments(
-                dataSource, interval, false, Collections.singletonList(request3))
+            .allocatePendingSegments(dataSource, interval, false, singletonList(request3))
             .get(request3);
 
     Assert.assertEquals(
@@ -1716,8 +1715,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
         new SegmentCreateRequest("seq1", null, "v1", partialShardSpec);
     final SegmentIdWithShardSpec segmentId4 =
         coordinator
-            .allocatePendingSegments(
-                dataSource, interval, false, Collections.singletonList(request4))
+            .allocatePendingSegments(dataSource, interval, false, singletonList(request4))
             .get(request4);
 
     Assert.assertEquals(
@@ -1809,14 +1807,14 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
           identifier.toString());
       prevSegmentId = identifier.toString();
       final Set<DataSegment> toBeAnnounced =
-          Collections.singleton(
+          singleton(
               new DataSegment(
                   identifier.getDataSource(),
                   identifier.getInterval(),
                   identifier.getVersion(),
                   null,
-                  Collections.emptyList(),
-                  Collections.emptyList(),
+                  emptyList(),
+                  emptyList(),
                   ((NumberedOverwriteShardSpec) identifier.getShardSpec())
                       .withAtomicUpdateGroupSize(1),
                   0,
@@ -1836,8 +1834,8 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
             interval,
             "version",
             null,
-            Collections.emptyList(),
-            Collections.emptyList(),
+            emptyList(),
+            emptyList(),
             new NumberedOverwriteShardSpec(
                 9 + PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 1, (short) 9, (short) 1),
             0,
@@ -1862,14 +1860,14 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     Assert.assertEquals(5, shardSpec.getNumBuckets());
 
     coordinator.announceHistoricalSegments(
-        Collections.singleton(
+        singleton(
             new DataSegment(
                 id.getDataSource(),
                 id.getInterval(),
                 id.getVersion(),
                 null,
-                Collections.emptyList(),
-                Collections.emptyList(),
+                emptyList(),
+                emptyList(),
                 id.getShardSpec(),
                 0,
                 10L)));
@@ -1884,14 +1882,14 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
     Assert.assertEquals(5, shardSpec.getNumBuckets());
 
     coordinator.announceHistoricalSegments(
-        Collections.singleton(
+        singleton(
             new DataSegment(
                 id.getDataSource(),
                 id.getInterval(),
                 id.getVersion(),
                 null,
-                Collections.emptyList(),
-                Collections.emptyList(),
+                emptyList(),
+                emptyList(),
                 id.getShardSpec(),
                 0,
                 10L)));
@@ -1931,7 +1929,7 @@ public class IndexerSQLMetadataStorageCoordinatorTest {
               dimensions,
               metrics,
               new DimensionRangeShardSpec(
-                  Collections.singletonList("dim"),
+                  singletonList("dim"),
                   i == 0 ? null : StringTuple.create(String.valueOf(i - 1)),
                   i == 5 ? null : StringTuple.create(String.valueOf(i)),
                   i,
diff --git a/server/src/test/java/org/apache/druid/metadata/SQLMetadataRuleManagerTest.java b/server/src/test/java/org/apache/druid/metadata/SQLMetadataRuleManagerTest.java
index f8fe9ff758..62a23bf019 100644
--- a/server/src/test/java/org/apache/druid/metadata/SQLMetadataRuleManagerTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/SQLMetadataRuleManagerTest.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Suppliers;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import org.apache.druid.audit.AuditEntry;
@@ -98,7 +99,7 @@ public class SQLMetadataRuleManagerTest {
   @Test
   public void testRuleInsert() {
     List<Rule> rules =
-        Collections.singletonList(
+        singletonList(
             new IntervalLoadRule(
                 Intervals.of("2015-01-01/2015-02-01"),
                 ImmutableMap.of(DruidServer.DEFAULT_TIER, DruidServer.DEFAULT_NUM_REPLICANTS)));
@@ -115,7 +116,7 @@ public class SQLMetadataRuleManagerTest {
   @Test
   public void testAuditEntryCreated() throws Exception {
     List<Rule> rules =
-        Collections.singletonList(
+        singletonList(
             new IntervalLoadRule(
                 Intervals.of("2015-01-01/2015-02-01"),
                 ImmutableMap.of(DruidServer.DEFAULT_TIER, DruidServer.DEFAULT_NUM_REPLICANTS)));
@@ -141,7 +142,7 @@ public class SQLMetadataRuleManagerTest {
   @Test
   public void testFetchAuditEntriesForAllDataSources() throws Exception {
     List<Rule> rules =
-        Collections.singletonList(
+        singletonList(
             new IntervalLoadRule(
                 Intervals.of("2015-01-01/2015-02-01"),
                 ImmutableMap.of(DruidServer.DEFAULT_TIER, DruidServer.DEFAULT_NUM_REPLICANTS)));
diff --git a/server/src/test/java/org/apache/druid/metadata/SQLMetadataStorageActionHandlerTest.java b/server/src/test/java/org/apache/druid/metadata/SQLMetadataStorageActionHandlerTest.java
index 27bc1fac48..8fca387fae 100644
--- a/server/src/test/java/org/apache/druid/metadata/SQLMetadataStorageActionHandlerTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/SQLMetadataStorageActionHandlerTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.UUID.randomUUID;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Optional;
@@ -26,13 +29,12 @@ import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.sql.ResultSet;
+import java.util.EnumMap;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import java.util.UUID;
-import java.util.stream.Collectors;
 import org.apache.druid.indexer.TaskIdentifier;
 import org.apache.druid.indexer.TaskInfo;
 import org.apache.druid.indexer.TaskLocation;
@@ -149,7 +151,7 @@ public class SQLMetadataStorageActionHandlerTest {
         ImmutableList.of(Pair.of(entry, status1)),
         handler.getTaskInfos(ActiveTaskLookup.getInstance(), null).stream()
             .map(taskInfo -> Pair.of(taskInfo.getTask(), taskInfo.getStatus()))
-            .collect(Collectors.toList()));
+            .collect(toList()));
 
     Assert.assertTrue(handler.setStatus(entryId, true, status2));
 
@@ -157,7 +159,7 @@ public class SQLMetadataStorageActionHandlerTest {
         ImmutableList.of(Pair.of(entry, status2)),
         handler.getTaskInfos(ActiveTaskLookup.getInstance(), null).stream()
             .map(taskInfo -> Pair.of(taskInfo.getTask(), taskInfo.getStatus()))
-            .collect(Collectors.toList()));
+            .collect(toList()));
 
     Assert.assertEquals(
         ImmutableList.of(),
@@ -187,7 +189,7 @@ public class SQLMetadataStorageActionHandlerTest {
                 CompleteTaskLookup.withTasksCreatedPriorTo(null, DateTimes.of("2014-01-01")), null)
             .stream()
             .map(TaskInfo::getStatus)
-            .collect(Collectors.toList()));
+            .collect(toList()));
   }
 
   @Test
@@ -416,8 +418,8 @@ public class SQLMetadataStorageActionHandlerTest {
     Assert.assertEquals(
         ImmutableList.of(entryId2),
         handler.getTaskInfos(ActiveTaskLookup.getInstance(), null).stream()
-            .map(taskInfo -> taskInfo.getId())
-            .collect(Collectors.toList()));
+            .map(TaskInfo::getId)
+            .collect(toList()));
 
     Assert.assertEquals(
         ImmutableList.of(entryId3, entryId1),
@@ -425,24 +427,24 @@ public class SQLMetadataStorageActionHandlerTest {
             .getTaskInfos(
                 CompleteTaskLookup.withTasksCreatedPriorTo(null, DateTimes.of("2014-01-01")), null)
             .stream()
-            .map(taskInfo -> taskInfo.getId())
-            .collect(Collectors.toList()));
+            .map(TaskInfo::getId)
+            .collect(toList()));
 
     handler.removeTasksOlderThan(DateTimes.of("2014-01-02").getMillis());
     // active task not removed.
     Assert.assertEquals(
         ImmutableList.of(entryId2),
         handler.getTaskInfos(ActiveTaskLookup.getInstance(), null).stream()
-            .map(taskInfo -> taskInfo.getId())
-            .collect(Collectors.toList()));
+            .map(TaskInfo::getId)
+            .collect(toList()));
     Assert.assertEquals(
         ImmutableList.of(entryId3),
         handler
             .getTaskInfos(
                 CompleteTaskLookup.withTasksCreatedPriorTo(null, DateTimes.of("2014-01-01")), null)
             .stream()
-            .map(taskInfo -> taskInfo.getId())
-            .collect(Collectors.toList()));
+            .map(TaskInfo::getId)
+            .collect(toList()));
 
     // tasklogs
     Assert.assertEquals(0, handler.getLogs(entryId1).size());
@@ -486,7 +488,8 @@ public class SQLMetadataStorageActionHandlerTest {
         createRandomTaskInfo(false);
     insertTaskInfo(completedAltered, true);
 
-    Map<TaskLookup.TaskLookupType, TaskLookup> taskLookups = new HashMap<>();
+    Map<TaskLookup.TaskLookupType, TaskLookup> taskLookups =
+        new EnumMap<>(TaskLookup.TaskLookupType.class);
     taskLookups.put(TaskLookup.TaskLookupType.ACTIVE, ActiveTaskLookup.getInstance());
     taskLookups.put(
         TaskLookup.TaskLookupType.COMPLETE, CompleteTaskLookup.of(null, Duration.millis(86400000)));
@@ -548,11 +551,11 @@ public class SQLMetadataStorageActionHandlerTest {
   }
 
   private TaskInfo<Map<String, Object>, Map<String, Object>> createRandomTaskInfo(boolean active) {
-    String id = UUID.randomUUID().toString();
+    String id = randomUUID().toString();
     DateTime createdTime = DateTime.now(DateTimeZone.UTC);
-    String datasource = UUID.randomUUID().toString();
-    String type = UUID.randomUUID().toString();
-    String groupId = UUID.randomUUID().toString();
+    String datasource = randomUUID().toString();
+    String type = randomUUID().toString();
+    String groupId = randomUUID().toString();
 
     Map<String, Object> payload = new HashMap<>();
     payload.put("id", id);
@@ -563,8 +566,8 @@ public class SQLMetadataStorageActionHandlerTest {
     status.put("id", id);
     status.put("status", active ? TaskState.RUNNING : TaskState.SUCCESS);
     status.put("duration", RANDOM.nextLong());
-    status.put("location", TaskLocation.create(UUID.randomUUID().toString(), 8080, 995));
-    status.put("errorMsg", UUID.randomUUID().toString());
+    status.put("location", TaskLocation.create(randomUUID().toString(), 8080, 995));
+    status.put("errorMsg", randomUUID().toString());
 
     return new TaskInfo<>(id, createdTime, status, datasource, payload);
   }
diff --git a/server/src/test/java/org/apache/druid/metadata/SQLMetadataSupervisorManagerTest.java b/server/src/test/java/org/apache/druid/metadata/SQLMetadataSupervisorManagerTest.java
index 7c227a01dc..3fcfd56361 100644
--- a/server/src/test/java/org/apache/druid/metadata/SQLMetadataSupervisorManagerTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/SQLMetadataSupervisorManagerTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Suppliers;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import org.apache.druid.indexing.overlord.supervisor.NoopSupervisorSpec;
@@ -384,7 +385,7 @@ public class SQLMetadataSupervisorManagerTest {
 
     @Override
     public List<String> getDataSources() {
-      return Collections.singletonList(dataSource);
+      return singletonList(dataSource);
     }
 
     @Override
diff --git a/server/src/test/java/org/apache/druid/metadata/SqlSegmentsMetadataManagerEmptyTest.java b/server/src/test/java/org/apache/druid/metadata/SqlSegmentsMetadataManagerEmptyTest.java
index 70cfd77f2a..36871f1910 100644
--- a/server/src/test/java/org/apache/druid/metadata/SqlSegmentsMetadataManagerEmptyTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/SqlSegmentsMetadataManagerEmptyTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Suppliers;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
-import java.util.stream.Collectors;
 import org.apache.druid.client.ImmutableDruidDataSource;
 import org.apache.druid.segment.TestHelper;
 import org.joda.time.Period;
@@ -80,7 +81,7 @@ public class SqlSegmentsMetadataManagerEmptyTest {
         ImmutableList.of(),
         sqlSegmentsMetadataManager.getImmutableDataSourcesWithAllUsedSegments().stream()
             .map(ImmutableDruidDataSource::getName)
-            .collect(Collectors.toList()));
+            .collect(toList()));
     Assert.assertEquals(
         null, sqlSegmentsMetadataManager.getImmutableDataSourceWithUsedSegments("wikipedia"));
     Assert.assertEquals(
diff --git a/server/src/test/java/org/apache/druid/metadata/SqlSegmentsMetadataManagerTest.java b/server/src/test/java/org/apache/druid/metadata/SqlSegmentsMetadataManagerTest.java
index 96617cb09b..8fcf6ddf34 100644
--- a/server/src/test/java/org/apache/druid/metadata/SqlSegmentsMetadataManagerTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/SqlSegmentsMetadataManagerTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Optional;
 import com.google.common.base.Suppliers;
@@ -28,7 +30,6 @@ import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
 import java.io.IOException;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.client.DataSourcesSnapshot;
 import org.apache.druid.client.ImmutableDruidDataSource;
 import org.apache.druid.java.util.common.DateTimes;
@@ -153,7 +154,7 @@ public class SqlSegmentsMetadataManagerTest {
         ImmutableList.of("wikipedia"),
         dataSourcesSnapshot.getDataSourcesWithAllUsedSegments().stream()
             .map(ImmutableDruidDataSource::getName)
-            .collect(Collectors.toList()));
+            .collect(toList()));
     Assert.assertEquals(
         ImmutableSet.of(segment1, segment2),
         ImmutableSet.copyOf(dataSourcesSnapshot.getDataSource("wikipedia").getSegments()));
@@ -182,7 +183,7 @@ public class SqlSegmentsMetadataManagerTest {
         ImmutableList.of("wikipedia"),
         dataSourcesSnapshot.getDataSourcesWithAllUsedSegments().stream()
             .map(ImmutableDruidDataSource::getName)
-            .collect(Collectors.toList()));
+            .collect(toList()));
     Assert.assertEquals(
         ImmutableSet.of(segment1, segment2),
         ImmutableSet.copyOf(dataSourcesSnapshot.getDataSource("wikipedia").getSegments()));
@@ -207,7 +208,7 @@ public class SqlSegmentsMetadataManagerTest {
         ImmutableList.of("wikipedia"),
         dataSourcesSnapshot.getDataSourcesWithAllUsedSegments().stream()
             .map(ImmutableDruidDataSource::getName)
-            .collect(Collectors.toList()));
+            .collect(toList()));
     final String newDataSource2 = "wikipedia2";
     final DataSegment newSegment2 = createNewSegment1(newDataSource2);
     publisher.publishSegment(newSegment2);
@@ -224,7 +225,7 @@ public class SqlSegmentsMetadataManagerTest {
         ImmutableList.of("wikipedia2", "wikipedia"),
         dataSourcesSnapshot.getDataSourcesWithAllUsedSegments().stream()
             .map(ImmutableDruidDataSource::getName)
-            .collect(Collectors.toList()));
+            .collect(toList()));
 
     final String newDataSource3 = "wikipedia3";
     final DataSegment newSegment3 = createNewSegment1(newDataSource3);
@@ -244,7 +245,7 @@ public class SqlSegmentsMetadataManagerTest {
         ImmutableList.of("wikipedia2", "wikipedia3", "wikipedia"),
         dataSourcesSnapshot.getDataSourcesWithAllUsedSegments().stream()
             .map(ImmutableDruidDataSource::getName)
-            .collect(Collectors.toList()));
+            .collect(toList()));
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/metadata/TestDerbyConnector.java b/server/src/test/java/org/apache/druid/metadata/TestDerbyConnector.java
index 11cc28b251..3ffe3e201e 100644
--- a/server/src/test/java/org/apache/druid/metadata/TestDerbyConnector.java
+++ b/server/src/test/java/org/apache/druid/metadata/TestDerbyConnector.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.metadata;
 
+import static java.util.UUID.randomUUID;
+
 import com.google.common.base.Supplier;
 import com.google.common.base.Suppliers;
 import java.sql.SQLException;
-import java.util.UUID;
 import org.apache.druid.java.util.common.StringUtils;
 import org.apache.druid.metadata.storage.derby.DerbyConnector;
 import org.junit.Assert;
@@ -65,7 +66,7 @@ public class TestDerbyConnector extends DerbyConnector {
   }
 
   public static String dbSafeUUID() {
-    return StringUtils.removeChar(UUID.randomUUID().toString(), '-');
+    return StringUtils.removeChar(randomUUID().toString(), '-');
   }
 
   public String getJdbcUri() {
diff --git a/server/src/test/java/org/apache/druid/metadata/TestSupervisorSpec.java b/server/src/test/java/org/apache/druid/metadata/TestSupervisorSpec.java
index 163cfff77c..ecef88248f 100644
--- a/server/src/test/java/org/apache/druid/metadata/TestSupervisorSpec.java
+++ b/server/src/test/java/org/apache/druid/metadata/TestSupervisorSpec.java
@@ -37,8 +37,8 @@ public class TestSupervisorSpec implements SupervisorSpec {
     this.data = data;
   }
 
-  @Override
   @JsonProperty
+  @Override
   public String getId() {
     return id;
   }
diff --git a/server/src/test/java/org/apache/druid/metadata/input/InputSourceModuleTest.java b/server/src/test/java/org/apache/druid/metadata/input/InputSourceModuleTest.java
index c12046b5ea..e694c29dc3 100644
--- a/server/src/test/java/org/apache/druid/metadata/input/InputSourceModuleTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/input/InputSourceModuleTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.metadata.input;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.Module;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.cfg.MapperConfig;
@@ -31,7 +33,6 @@ import com.google.inject.Guice;
 import com.google.inject.Injector;
 import java.util.List;
 import java.util.Properties;
-import java.util.stream.Collectors;
 import javax.validation.Validation;
 import javax.validation.Validator;
 import org.apache.druid.data.input.impl.HttpInputSourceConfig;
@@ -67,7 +68,7 @@ public class InputSourceModuleTest {
             .collectAndResolveSubtypesByClass(config, annotatedClass)
             .stream()
             .map(NamedType::getName)
-            .collect(Collectors.toList());
+            .collect(toList());
     Assert.assertNotNull(subtypes);
     Assert.assertEquals(SQL_NAMED_TYPE, Iterables.getOnlyElement(subtypes));
   }
diff --git a/server/src/test/java/org/apache/druid/metadata/input/SqlEntityTest.java b/server/src/test/java/org/apache/druid/metadata/input/SqlEntityTest.java
index fa2b13d130..03b9f0e8ef 100644
--- a/server/src/test/java/org/apache/druid/metadata/input/SqlEntityTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/input/SqlEntityTest.java
@@ -19,14 +19,15 @@
 
 package org.apache.druid.metadata.input;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.Module;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.nio.charset.StandardCharsets;
-import java.util.Collections;
 import org.apache.commons.io.IOUtils;
 import org.apache.druid.data.input.InputEntity;
 import org.apache.druid.data.input.InputRow;
@@ -67,10 +68,9 @@ public class SqlEntityTest {
         SqlEntity.openCleanableFile(
             VALID_SQL, testUtils.getDerbyFirehoseConnector(), mapper, true, tmpFile);
     InputStream queryInputStream = new FileInputStream(queryResult.file());
-    String actualJson = IOUtils.toString(queryInputStream, StandardCharsets.UTF_8);
+    String actualJson = IOUtils.toString(queryInputStream, UTF_8);
     String expectedJson =
-        mapper.writeValueAsString(
-            Collections.singletonList(((MapBasedInputRow) expectedRow).getEvent()));
+        mapper.writeValueAsString(singletonList(((MapBasedInputRow) expectedRow).getEvent()));
     Assert.assertEquals(actualJson, expectedJson);
     testUtils.dropTable(TABLE_NAME_1);
   }
diff --git a/server/src/test/java/org/apache/druid/metadata/input/SqlInputSourceTest.java b/server/src/test/java/org/apache/druid/metadata/input/SqlInputSourceTest.java
index 1e59f5879f..8ac4018f64 100644
--- a/server/src/test/java/org/apache/druid/metadata/input/SqlInputSourceTest.java
+++ b/server/src/test/java/org/apache/druid/metadata/input/SqlInputSourceTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.metadata.input;
 
+import static java.util.Collections.singleton;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.fasterxml.jackson.annotation.JsonTypeName;
 import com.fasterxml.jackson.databind.Module;
@@ -29,11 +32,9 @@ import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.Objects;
 import java.util.Set;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 import nl.jqno.equalsverifier.EqualsVerifier;
 import org.apache.commons.dbcp2.BasicDataSource;
@@ -126,7 +127,7 @@ public class SqlInputSourceTest {
     final SqlInputSource sqlInputSource =
         new SqlInputSource(
             SqlTestUtils.selectFrom(TABLE_1), true, testSerdeFirehoseConnector, mapper);
-    Assert.assertEquals(Collections.singleton(SqlInputSource.TYPE_KEY), sqlInputSource.getTypes());
+    Assert.assertEquals(singleton(SqlInputSource.TYPE_KEY), sqlInputSource.getTypes());
   }
 
   @Test
@@ -188,7 +189,7 @@ public class SqlInputSourceTest {
         new SqlInputSource(sqls, true, testUtils.getDerbyFirehoseConnector(), mapper);
     InputFormat inputFormat = EasyMock.createMock(InputFormat.class);
     Stream<InputSplit<String>> sqlSplits = sqlInputSource.createSplits(inputFormat, null);
-    Assert.assertEquals(sqls, sqlSplits.map(InputSplit::get).collect(Collectors.toList()));
+    Assert.assertEquals(sqls, sqlSplits.map(InputSplit::get).collect(toList()));
     Assert.assertEquals(2, sqlInputSource.estimateNumSplits(inputFormat, null));
   }
 
diff --git a/server/src/test/java/org/apache/druid/metadata/input/SqlTestUtils.java b/server/src/test/java/org/apache/druid/metadata/input/SqlTestUtils.java
index d90ad67ac5..972a3aa46d 100644
--- a/server/src/test/java/org/apache/druid/metadata/input/SqlTestUtils.java
+++ b/server/src/test/java/org/apache/druid/metadata/input/SqlTestUtils.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.metadata.input;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.annotation.JsonProperty;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
@@ -28,7 +30,6 @@ import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import org.apache.commons.dbcp2.BasicDataSource;
 import org.apache.druid.data.input.InputRow;
@@ -115,7 +116,7 @@ public class SqlTestUtils {
                   return new MapBasedInputRow(
                       DateTimes.of(timestamp), Arrays.asList("timestamp", "a", "b"), event);
                 })
-            .collect(Collectors.toList());
+            .collect(toList());
 
     derbyConnector
         .getDBI()
diff --git a/server/src/test/java/org/apache/druid/query/QueryRunnerBasedOnClusteredClientTestBase.java b/server/src/test/java/org/apache/druid/query/QueryRunnerBasedOnClusteredClientTestBase.java
index 5f49d0d531..3d502b820e 100644
--- a/server/src/test/java/org/apache/druid/query/QueryRunnerBasedOnClusteredClientTestBase.java
+++ b/server/src/test/java/org/apache/druid/query/QueryRunnerBasedOnClusteredClientTestBase.java
@@ -19,16 +19,17 @@
 
 package org.apache.druid.query;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.UUID.randomUUID;
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.UUID;
 import java.util.concurrent.ForkJoinPool;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import org.apache.druid.client.CachingClusteredClient;
 import org.apache.druid.client.DirectDruidClient;
@@ -172,7 +173,7 @@ public abstract class QueryRunnerBasedOnClusteredClientTestBase {
   }
 
   protected void prepareCluster(int numServers) {
-    Preconditions.checkArgument(numServers < 25, "Cannot be larger than 24");
+    checkArgument(numServers < 25, "Cannot be larger than 24");
     for (int i = 0; i < numServers; i++) {
       final int partitionId = i % 2;
       final int intervalIndex = i / 2;
@@ -203,7 +204,7 @@ public abstract class QueryRunnerBasedOnClusteredClientTestBase {
         .context(
             ImmutableMap.of(DirectDruidClient.QUERY_FAIL_TIME, System.currentTimeMillis() + 10000))
         .build()
-        .withId(UUID.randomUUID().toString());
+        .withId(randomUUID().toString());
   }
 
   protected static List<Result<TimeseriesResultValue>> expectedTimeseriesResult(
@@ -214,7 +215,7 @@ public abstract class QueryRunnerBasedOnClusteredClientTestBase {
                 new Result<>(
                     DateTimes.of(StringUtils.format("2000-01-01T%02d", i / 2)),
                     new TimeseriesResultValue(ImmutableMap.of("rows", 10))))
-        .collect(Collectors.toList());
+        .collect(toList());
   }
 
   protected static ResponseContext responseContext() {
diff --git a/server/src/test/java/org/apache/druid/query/dimension/LookupDimensionSpecTest.java b/server/src/test/java/org/apache/druid/query/dimension/LookupDimensionSpecTest.java
index 853c3848b0..320958689d 100644
--- a/server/src/test/java/org/apache/druid/query/dimension/LookupDimensionSpecTest.java
+++ b/server/src/test/java/org/apache/druid/query/dimension/LookupDimensionSpecTest.java
@@ -173,8 +173,8 @@ public class LookupDimensionSpecTest {
     };
   }
 
-  @Test
   @Parameters
+  @Test
   public void testApply(DimensionSpec dimensionSpec, Map<String, String> map) {
     for (Map.Entry<String, String> entry : map.entrySet()) {
       Assert.assertEquals(
@@ -225,8 +225,8 @@ public class LookupDimensionSpecTest {
     };
   }
 
-  @Test
   @Parameters
+  @Test
   public void testGetCacheKey(DimensionSpec dimensionSpec, boolean expectedResult) {
     Assert.assertEquals(
         expectedResult, Arrays.equals(lookupDimSpec.getCacheKey(), dimensionSpec.getCacheKey()));
diff --git a/server/src/test/java/org/apache/druid/query/expression/LookupEnabledTestExprMacroTable.java b/server/src/test/java/org/apache/druid/query/expression/LookupEnabledTestExprMacroTable.java
index da0759c153..be0f011c6f 100644
--- a/server/src/test/java/org/apache/druid/query/expression/LookupEnabledTestExprMacroTable.java
+++ b/server/src/test/java/org/apache/druid/query/expression/LookupEnabledTestExprMacroTable.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.query.expression;
 
+import static java.util.Collections.singletonList;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
@@ -53,7 +54,7 @@ public class LookupEnabledTestExprMacroTable extends ExprMacroTable {
     return Lists.newArrayList(
         Iterables.concat(
             TestExprMacroTable.INSTANCE.getMacros(),
-            Collections.singletonList(new LookupExprMacro(createTestLookupProvider(theLookup)))));
+            singletonList(new LookupExprMacro(createTestLookupProvider(theLookup)))));
   }
 
   /**
diff --git a/server/src/test/java/org/apache/druid/query/lookup/LookupReferencesManagerTest.java b/server/src/test/java/org/apache/druid/query/lookup/LookupReferencesManagerTest.java
index 6b302de127..dafcc69b51 100644
--- a/server/src/test/java/org/apache/druid/query/lookup/LookupReferencesManagerTest.java
+++ b/server/src/test/java/org/apache/druid/query/lookup/LookupReferencesManagerTest.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.query.lookup;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.io.IOException;
 import java.net.URL;
-import java.nio.charset.StandardCharsets;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Optional;
@@ -101,8 +102,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -161,8 +161,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -201,8 +200,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -234,8 +232,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -263,8 +260,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -295,8 +291,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -333,8 +328,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -363,8 +357,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -392,8 +385,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -432,8 +424,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -476,8 +467,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -544,8 +534,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
     EasyMock.replay(druidLeaderClient);
@@ -633,8 +622,7 @@ public class LookupReferencesManagerTest {
                 HttpMethod.GET, "/druid/coordinator/v1/lookups/config/lookupTier?detailed=true"))
         .andReturn(request);
     StringFullResponseHolder responseHolder =
-        new StringFullResponseHolder(
-                newEmptyResponse(HttpResponseStatus.OK), StandardCharsets.UTF_8)
+        new StringFullResponseHolder(newEmptyResponse(HttpResponseStatus.OK), UTF_8)
             .addChunk(strResult);
     EasyMock.expect(druidLeaderClient.go(request)).andReturn(responseHolder);
 
diff --git a/server/src/test/java/org/apache/druid/query/lookup/LookupSnapshotTakerTest.java b/server/src/test/java/org/apache/druid/query/lookup/LookupSnapshotTakerTest.java
index 23da52597b..7bcc527e36 100644
--- a/server/src/test/java/org/apache/druid/query/lookup/LookupSnapshotTakerTest.java
+++ b/server/src/test/java/org/apache/druid/query/lookup/LookupSnapshotTakerTest.java
@@ -19,12 +19,14 @@
 
 package org.apache.druid.query.lookup;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.io.Files;
 import java.io.File;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import org.apache.druid.java.util.common.ISE;
 import org.apache.druid.java.util.common.StringUtils;
@@ -70,9 +72,9 @@ public class LookupSnapshotTakerTest {
             null,
             new LookupExtractorFactoryContainer(
                 "v1", new MapLookupExtractorFactory(ImmutableMap.of("key", "value"), true)));
-    List<LookupBean> lookupBeanList1 = Collections.singletonList(lookupBean1);
+    List<LookupBean> lookupBeanList1 = singletonList(lookupBean1);
     lookupSnapshotTaker.takeSnapshot(TIER1, lookupBeanList1);
-    List<LookupBean> lookupBeanList2 = Collections.singletonList(lookupBean2);
+    List<LookupBean> lookupBeanList2 = singletonList(lookupBean2);
     lookupSnapshotTaker.takeSnapshot(TIER2, lookupBeanList2);
     Assert.assertEquals(lookupBeanList1, lookupSnapshotTaker.pullExistingSnapshot(TIER1));
     Assert.assertEquals(lookupBeanList2, lookupSnapshotTaker.pullExistingSnapshot(TIER2));
@@ -94,7 +96,7 @@ public class LookupSnapshotTakerTest {
             null,
             new LookupExtractorFactoryContainer(
                 "v1", new MapLookupExtractorFactory(ImmutableMap.of("key", "value"), true)));
-    List<LookupBean> lookupBeanList = Collections.singletonList(lookupBean);
+    List<LookupBean> lookupBeanList = singletonList(lookupBean);
 
     expectedException.expect(ISE.class);
     expectedException.expectMessage("Exception during serialization of lookups");
@@ -105,7 +107,7 @@ public class LookupSnapshotTakerTest {
   public void tesLookupPullingFromEmptyFile() throws IOException {
     File snapshotFile = lookupSnapshotTaker.getPersistFile(TIER1);
     Assert.assertTrue(snapshotFile.createNewFile());
-    Assert.assertEquals(Collections.emptyList(), lookupSnapshotTaker.pullExistingSnapshot(TIER1));
+    Assert.assertEquals(emptyList(), lookupSnapshotTaker.pullExistingSnapshot(TIER1));
   }
 
   @Test(expected = ISE.class)
@@ -123,6 +125,6 @@ public class LookupSnapshotTakerTest {
     LookupSnapshotTaker lookupSnapshotTaker =
         new LookupSnapshotTaker(mapper, directory.getAbsolutePath());
     List<LookupBean> actualList = lookupSnapshotTaker.pullExistingSnapshot(TIER1);
-    Assert.assertEquals(Collections.emptyList(), actualList);
+    Assert.assertEquals(emptyList(), actualList);
   }
 }
diff --git a/server/src/test/java/org/apache/druid/rpc/DiscoveryServiceLocatorTest.java b/server/src/test/java/org/apache/druid/rpc/DiscoveryServiceLocatorTest.java
index 95eb689771..6c2b23f123 100644
--- a/server/src/test/java/org/apache/druid/rpc/DiscoveryServiceLocatorTest.java
+++ b/server/src/test/java/org/apache/druid/rpc/DiscoveryServiceLocatorTest.java
@@ -19,13 +19,15 @@
 
 package org.apache.druid.rpc;
 
-import com.google.common.collect.ImmutableList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.emptySet;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableSet;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.errorprone.annotations.concurrent.GuardedBy;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.function.Consumer;
 import org.apache.druid.discovery.DiscoveryDruidNode;
@@ -38,7 +40,6 @@ import org.junit.Assert;
 import org.junit.Rule;
 import org.junit.Test;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnit;
 import org.mockito.junit.MockitoRule;
 
@@ -47,13 +48,13 @@ public class DiscoveryServiceLocatorTest {
       new DiscoveryDruidNode(
           new DruidNode("test-service", "node1.example.com", false, -1, 8888, false, true),
           NodeRole.BROKER,
-          Collections.emptyMap());
+          emptyMap());
 
   private static final DiscoveryDruidNode NODE2 =
       new DiscoveryDruidNode(
           new DruidNode("test-service", "node2.example.com", false, -1, 8888, false, true),
           NodeRole.BROKER,
-          Collections.emptyMap());
+          emptyMap());
 
   @Rule public MockitoRule mockitoRule = MockitoJUnit.rule();
 
@@ -71,7 +72,7 @@ public class DiscoveryServiceLocatorTest {
   @Test
   public void test_locate_initializeEmpty() throws Exception {
     final TestDiscovery discovery = new TestDiscovery();
-    Mockito.when(discoveryProvider.getForNodeRole(NodeRole.BROKER)).thenReturn(discovery);
+    when(discoveryProvider.getForNodeRole(NodeRole.BROKER)).thenReturn(discovery);
     locator = new DiscoveryServiceLocator(discoveryProvider, NodeRole.BROKER);
     locator.start();
 
@@ -79,13 +80,13 @@ public class DiscoveryServiceLocatorTest {
     Assert.assertFalse(future.isDone());
 
     discovery.fire(DruidNodeDiscovery.Listener::nodeViewInitialized);
-    Assert.assertEquals(ServiceLocations.forLocations(Collections.emptySet()), future.get());
+    Assert.assertEquals(ServiceLocations.forLocations(emptySet()), future.get());
   }
 
   @Test
   public void test_locate_initializeNonEmpty() throws Exception {
     final TestDiscovery discovery = new TestDiscovery();
-    Mockito.when(discoveryProvider.getForNodeRole(NodeRole.BROKER)).thenReturn(discovery);
+    when(discoveryProvider.getForNodeRole(NodeRole.BROKER)).thenReturn(discovery);
     locator = new DiscoveryServiceLocator(discoveryProvider, NodeRole.BROKER);
     locator.start();
 
@@ -110,7 +111,7 @@ public class DiscoveryServiceLocatorTest {
   @Test
   public void test_locate_removeAfterAdd() throws Exception {
     final TestDiscovery discovery = new TestDiscovery();
-    Mockito.when(discoveryProvider.getForNodeRole(NodeRole.BROKER)).thenReturn(discovery);
+    when(discoveryProvider.getForNodeRole(NodeRole.BROKER)).thenReturn(discovery);
     locator = new DiscoveryServiceLocator(discoveryProvider, NodeRole.BROKER);
     locator.start();
 
@@ -131,7 +132,7 @@ public class DiscoveryServiceLocatorTest {
   @Test
   public void test_locate_closed() throws Exception {
     final TestDiscovery discovery = new TestDiscovery();
-    Mockito.when(discoveryProvider.getForNodeRole(NodeRole.BROKER)).thenReturn(discovery);
+    when(discoveryProvider.getForNodeRole(NodeRole.BROKER)).thenReturn(discovery);
     locator = new DiscoveryServiceLocator(discoveryProvider, NodeRole.BROKER);
     locator.start();
 
@@ -169,7 +170,7 @@ public class DiscoveryServiceLocatorTest {
     }
 
     public synchronized List<Listener> getListeners() {
-      return ImmutableList.copyOf(listeners);
+      return listeners;
     }
 
     public synchronized void fire(Consumer<Listener> f) {
diff --git a/server/src/test/java/org/apache/druid/rpc/ServiceClientImplTest.java b/server/src/test/java/org/apache/druid/rpc/ServiceClientImplTest.java
index 892466a245..0430a8da0f 100644
--- a/server/src/test/java/org/apache/druid/rpc/ServiceClientImplTest.java
+++ b/server/src/test/java/org/apache/druid/rpc/ServiceClientImplTest.java
@@ -19,6 +19,13 @@
 
 package org.apache.druid.rpc;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.argThat;
+import static org.mockito.ArgumentMatchers.eq;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.util.concurrent.Futures;
@@ -26,7 +33,6 @@ import com.google.common.util.concurrent.ListenableFuture;
 import com.google.common.util.concurrent.SettableFuture;
 import java.io.IOException;
 import java.nio.ByteBuffer;
-import java.nio.charset.StandardCharsets;
 import java.util.Map;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.ScheduledExecutorService;
@@ -53,9 +59,7 @@ import org.junit.Before;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.internal.matchers.ThrowableMessageMatcher;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnit;
 import org.mockito.junit.MockitoRule;
 import org.mockito.quality.Strictness;
@@ -464,7 +468,7 @@ public class ServiceClientImplTest {
     final ImmutableMap<String, String> expectedResponseObject = ImmutableMap.of("foo", "bar");
 
     // Service unavailable at first, then available.
-    Mockito.when(serviceLocator.locate())
+    when(serviceLocator.locate())
         .thenReturn(Futures.immediateFuture(locations()))
         .thenReturn(Futures.immediateFuture(locations(SERVER1)));
     expectHttpCall(requestBuilder, SERVER1).thenReturn(valueResponse(expectedResponseObject));
@@ -650,15 +654,15 @@ public class ServiceClientImplTest {
       final RequestBuilder requestBuilder, final ServiceLocation location) {
     final Request expectedRequest = requestBuilder.build(location);
 
-    return Mockito.when(
+    return when(
         httpClient.go(
-            ArgumentMatchers.argThat(
+            argThat(
                 request ->
                     request != null
                         && expectedRequest.getMethod().equals(request.getMethod())
                         && expectedRequest.getUrl().equals(request.getUrl())),
-            ArgumentMatchers.any(ObjectOrErrorResponseHandler.class),
-            ArgumentMatchers.eq(RequestBuilder.DEFAULT_TIMEOUT)));
+            any(ObjectOrErrorResponseHandler.class),
+            eq(RequestBuilder.DEFAULT_TIMEOUT)));
   }
 
   private void stubLocatorCall(final ServiceLocations locations) {
@@ -666,7 +670,7 @@ public class ServiceClientImplTest {
   }
 
   private void stubLocatorCall(final ListenableFuture<ServiceLocations> locations) {
-    Mockito.doReturn(locations).when(serviceLocator).locate();
+    doReturn(locations).when(serviceLocator).locate();
   }
 
   private ServiceClient makeServiceClient(final ServiceRetryPolicy retryPolicy) {
@@ -707,8 +711,7 @@ public class ServiceClientImplTest {
           ChannelBuffers.wrappedBuffer(ByteBuffer.wrap(StringUtils.toUtf8(content))));
     }
 
-    final StringFullResponseHolder errorHolder =
-        new StringFullResponseHolder(response, StandardCharsets.UTF_8);
+    final StringFullResponseHolder errorHolder = new StringFullResponseHolder(response, UTF_8);
     return Futures.immediateFuture(Either.error(errorHolder));
   }
 
diff --git a/server/src/test/java/org/apache/druid/rpc/ServiceLocationsTest.java b/server/src/test/java/org/apache/druid/rpc/ServiceLocationsTest.java
index 2d1c36940e..a03e28923a 100644
--- a/server/src/test/java/org/apache/druid/rpc/ServiceLocationsTest.java
+++ b/server/src/test/java/org/apache/druid/rpc/ServiceLocationsTest.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.rpc;
 
+import static java.util.Collections.emptySet;
+
 import com.google.common.collect.ImmutableSet;
-import java.util.Collections;
 import nl.jqno.equalsverifier.EqualsVerifier;
 import org.junit.Assert;
 import org.junit.Test;
@@ -51,7 +52,7 @@ public class ServiceLocationsTest {
   public void test_closed() {
     final ServiceLocations locations = ServiceLocations.closed();
 
-    Assert.assertEquals(Collections.emptySet(), locations.getLocations());
+    Assert.assertEquals(emptySet(), locations.getLocations());
     Assert.assertTrue(locations.isClosed());
   }
 
diff --git a/server/src/test/java/org/apache/druid/rpc/indexing/SpecificTaskServiceLocatorTest.java b/server/src/test/java/org/apache/druid/rpc/indexing/SpecificTaskServiceLocatorTest.java
index d8601df7d6..969ccf99c1 100644
--- a/server/src/test/java/org/apache/druid/rpc/indexing/SpecificTaskServiceLocatorTest.java
+++ b/server/src/test/java/org/apache/druid/rpc/indexing/SpecificTaskServiceLocatorTest.java
@@ -19,10 +19,12 @@
 
 package org.apache.druid.rpc.indexing;
 
+import static java.util.Collections.emptySet;
+import static org.mockito.Mockito.when;
+
 import com.google.common.util.concurrent.Futures;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.common.util.concurrent.SettableFuture;
-import java.util.Collections;
 import java.util.concurrent.ExecutionException;
 import org.apache.druid.client.indexing.TaskStatusResponse;
 import org.apache.druid.indexer.TaskLocation;
@@ -39,7 +41,6 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.internal.matchers.ThrowableMessageMatcher;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnit;
 import org.mockito.junit.MockitoRule;
 import org.mockito.quality.Strictness;
@@ -56,18 +57,18 @@ public class SpecificTaskServiceLocatorTest {
 
   @Test
   public void test_locate_noLocationYet() throws Exception {
-    Mockito.when(overlordClient.taskStatus(TASK_ID))
+    when(overlordClient.taskStatus(TASK_ID))
         .thenReturn(makeResponse(TaskState.RUNNING, TaskLocation.unknown()));
 
     final SpecificTaskServiceLocator locator =
         new SpecificTaskServiceLocator(TASK_ID, overlordClient);
     final ListenableFuture<ServiceLocations> future = locator.locate();
-    Assert.assertEquals(ServiceLocations.forLocations(Collections.emptySet()), future.get());
+    Assert.assertEquals(ServiceLocations.forLocations(emptySet()), future.get());
   }
 
   @Test
   public void test_locate_taskRunning() throws Exception {
-    Mockito.when(overlordClient.taskStatus(TASK_ID))
+    when(overlordClient.taskStatus(TASK_ID))
         .thenReturn(makeResponse(TaskState.RUNNING, TASK_LOCATION1));
 
     final SpecificTaskServiceLocator locator =
@@ -77,7 +78,7 @@ public class SpecificTaskServiceLocatorTest {
 
   @Test
   public void test_locate_taskNotFound() throws Exception {
-    Mockito.when(overlordClient.taskStatus(TASK_ID))
+    when(overlordClient.taskStatus(TASK_ID))
         .thenReturn(Futures.immediateFuture(new TaskStatusResponse(TASK_ID, null)));
 
     final SpecificTaskServiceLocator locator =
@@ -88,7 +89,7 @@ public class SpecificTaskServiceLocatorTest {
 
   @Test
   public void test_locate_taskSuccess() throws Exception {
-    Mockito.when(overlordClient.taskStatus(TASK_ID))
+    when(overlordClient.taskStatus(TASK_ID))
         .thenReturn(makeResponse(TaskState.SUCCESS, TaskLocation.unknown()));
 
     final SpecificTaskServiceLocator locator =
@@ -99,7 +100,7 @@ public class SpecificTaskServiceLocatorTest {
 
   @Test
   public void test_locate_taskFailed() throws Exception {
-    Mockito.when(overlordClient.taskStatus(TASK_ID))
+    when(overlordClient.taskStatus(TASK_ID))
         .thenReturn(makeResponse(TaskState.FAILED, TaskLocation.unknown()));
 
     final SpecificTaskServiceLocator locator =
@@ -110,7 +111,7 @@ public class SpecificTaskServiceLocatorTest {
 
   @Test
   public void test_locate_overlordError() {
-    Mockito.when(overlordClient.taskStatus(TASK_ID))
+    when(overlordClient.taskStatus(TASK_ID))
         .thenReturn(Futures.immediateFailedFuture(new ISE("oh no")));
 
     final SpecificTaskServiceLocator locator =
@@ -128,7 +129,7 @@ public class SpecificTaskServiceLocatorTest {
   public void test_locate_afterClose() throws Exception {
     // Overlord call will never return.
     final SettableFuture<TaskStatusResponse> overlordFuture = SettableFuture.create();
-    Mockito.when(overlordClient.taskStatus(TASK_ID)).thenReturn(overlordFuture);
+    when(overlordClient.taskStatus(TASK_ID)).thenReturn(overlordFuture);
 
     final SpecificTaskServiceLocator locator =
         new SpecificTaskServiceLocator(TASK_ID, overlordClient);
diff --git a/server/src/test/java/org/apache/druid/segment/handoff/CoordinatorBasedSegmentHandoffNotifierTest.java b/server/src/test/java/org/apache/druid/segment/handoff/CoordinatorBasedSegmentHandoffNotifierTest.java
index e37a83c93f..208a17e66d 100644
--- a/server/src/test/java/org/apache/druid/segment/handoff/CoordinatorBasedSegmentHandoffNotifierTest.java
+++ b/server/src/test/java/org/apache/druid/segment/handoff/CoordinatorBasedSegmentHandoffNotifierTest.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.segment.handoff;
 
+import static java.util.Collections.singletonList;
+
 import com.google.common.collect.Sets;
-import java.util.Collections;
 import java.util.concurrent.atomic.AtomicBoolean;
 import org.apache.druid.client.ImmutableSegmentLoadInfo;
 import org.apache.druid.client.coordinator.CoordinatorClient;
@@ -100,7 +101,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
     Interval interval = Intervals.of("2011-04-01/2011-04-02");
     Assert.assertFalse(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 2),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -108,7 +109,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
 
     Assert.assertTrue(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v2", 2),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -116,7 +117,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
 
     Assert.assertTrue(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 2),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -128,7 +129,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
     Interval interval = Intervals.of("2011-04-01/2011-04-02");
     Assert.assertTrue(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 2),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -136,7 +137,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
 
     Assert.assertTrue(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 2),
                     Sets.newHashSet(createRealtimeServerMetadata("a")))),
@@ -148,7 +149,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
     Interval interval = Intervals.of("2011-04-01/2011-04-02");
     Assert.assertTrue(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 1),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -156,7 +157,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
 
     Assert.assertFalse(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 1),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -168,7 +169,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
 
     Assert.assertFalse(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(Intervals.of("2011-04-01/2011-04-02"), "v1", 1),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -176,7 +177,7 @@ public class CoordinatorBasedSegmentHandoffNotifierTest {
 
     Assert.assertTrue(
         CoordinatorBasedSegmentHandoffNotifier.isHandOffComplete(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(Intervals.of("2011-04-01/2011-04-04"), "v1", 1),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
diff --git a/server/src/test/java/org/apache/druid/segment/indexing/DataSchemaTest.java b/server/src/test/java/org/apache/druid/segment/indexing/DataSchemaTest.java
index e2d71d5294..c46d9c15b1 100644
--- a/server/src/test/java/org/apache/druid/segment/indexing/DataSchemaTest.java
+++ b/server/src/test/java/org/apache/druid/segment/indexing/DataSchemaTest.java
@@ -19,6 +19,12 @@
 
 package org.apache.druid.segment.indexing;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.emptyMap;
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
 import com.fasterxml.jackson.databind.JsonMappingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.exc.ValueInstantiationException;
@@ -27,9 +33,7 @@ import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.io.IOException;
 import java.nio.ByteBuffer;
-import java.nio.charset.StandardCharsets;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -60,8 +64,6 @@ import org.junit.Assert;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
 
 public class DataSchemaTest extends InitializedNullHandlingTest {
   @Rule public ExpectedException expectedException = ExpectedException.none();
@@ -177,8 +179,7 @@ public class DataSchemaTest extends InitializedNullHandlingTest {
     final InputRow row1bb =
         parser
             .parseBatch(
-                ByteBuffer.wrap(
-                    "{\"time\":\"2000-01-01\",\"dimA\":\"foo\"}".getBytes(StandardCharsets.UTF_8)))
+                ByteBuffer.wrap("{\"time\":\"2000-01-01\",\"dimA\":\"foo\"}".getBytes(UTF_8)))
             .get(0);
     Assert.assertEquals(DateTimes.of("2000-01-01"), row1bb.getTimestamp());
     Assert.assertEquals("foo", row1bb.getRaw("dimA"));
@@ -191,9 +192,7 @@ public class DataSchemaTest extends InitializedNullHandlingTest {
 
     final InputRow row2 =
         parser
-            .parseBatch(
-                ByteBuffer.wrap(
-                    "{\"time\":\"2000-01-01\",\"dimA\":\"x\"}".getBytes(StandardCharsets.UTF_8)))
+            .parseBatch(ByteBuffer.wrap("{\"time\":\"2000-01-01\",\"dimA\":\"x\"}".getBytes(UTF_8)))
             .get(0);
     Assert.assertNull(row2);
   }
@@ -420,8 +419,7 @@ public class DataSchemaTest extends InitializedNullHandlingTest {
   private void testInvalidWhitespaceDatasourceHelper(String dataSource, String invalidChar) {
     String testFailMsg = "dataSource contain invalid whitespace character: " + invalidChar;
     try {
-      DataSchema schema =
-          new DataSchema(dataSource, Collections.emptyMap(), null, null, null, jsonMapper);
+      DataSchema schema = new DataSchema(dataSource, emptyMap(), null, null, null, jsonMapper);
       Assert.fail(testFailMsg);
     } catch (IllegalArgumentException errorMsg) {
       String expectedMsg = "dataSource cannot contain whitespace character except space.";
@@ -601,16 +599,15 @@ public class DataSchemaTest extends InitializedNullHandlingTest {
 
   @Test
   public void testWithDimensionSpec() {
-    TimestampSpec tsSpec = Mockito.mock(TimestampSpec.class);
-    GranularitySpec gSpec = Mockito.mock(GranularitySpec.class);
-    DimensionsSpec oldDimSpec = Mockito.mock(DimensionsSpec.class);
-    DimensionsSpec newDimSpec = Mockito.mock(DimensionsSpec.class);
-    AggregatorFactory aggFactory = Mockito.mock(AggregatorFactory.class);
-    Mockito.when(aggFactory.getName()).thenReturn("myAgg");
-    TransformSpec transSpec = Mockito.mock(TransformSpec.class);
-    Map<String, Object> parserMap = Mockito.mock(Map.class);
-    Mockito.when(newDimSpec.withDimensionExclusions(ArgumentMatchers.any(Set.class)))
-        .thenReturn(newDimSpec);
+    TimestampSpec tsSpec = mock(TimestampSpec.class);
+    GranularitySpec gSpec = mock(GranularitySpec.class);
+    DimensionsSpec oldDimSpec = mock(DimensionsSpec.class);
+    DimensionsSpec newDimSpec = mock(DimensionsSpec.class);
+    AggregatorFactory aggFactory = mock(AggregatorFactory.class);
+    when(aggFactory.getName()).thenReturn("myAgg");
+    TransformSpec transSpec = mock(TransformSpec.class);
+    Map<String, Object> parserMap = mock(Map.class);
+    when(newDimSpec.withDimensionExclusions(any(Set.class))).thenReturn(newDimSpec);
 
     DataSchema oldSchema =
         new DataSchema(
diff --git a/server/src/test/java/org/apache/druid/segment/indexing/RealtimeTuningConfigTest.java b/server/src/test/java/org/apache/druid/segment/indexing/RealtimeTuningConfigTest.java
index dc7cd218ad..527bb7f075 100644
--- a/server/src/test/java/org/apache/druid/segment/indexing/RealtimeTuningConfigTest.java
+++ b/server/src/test/java/org/apache/druid/segment/indexing/RealtimeTuningConfigTest.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.segment.indexing;
 
+import static java.util.UUID.randomUUID;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.io.File;
-import java.util.UUID;
 import org.apache.druid.segment.IndexSpec;
 import org.apache.druid.segment.TestHelper;
 import org.apache.druid.segment.data.CompressionStrategy;
@@ -37,7 +38,7 @@ public class RealtimeTuningConfigTest {
   public void testErrorMessageIsMeaningfulWhenUnableToCreateTemporaryDirectory() {
     String propertyName = "java.io.tmpdir";
     String originalValue = System.getProperty(propertyName);
-    String nonExistedDirectory = "/tmp/" + UUID.randomUUID();
+    String nonExistedDirectory = "/tmp/" + randomUUID();
     try {
       System.setProperty(propertyName, nonExistedDirectory);
       RealtimeTuningConfig.makeDefaultTuningConfig(null);
diff --git a/server/src/test/java/org/apache/druid/segment/indexing/granularity/UniformGranularityTest.java b/server/src/test/java/org/apache/druid/segment/indexing/granularity/UniformGranularityTest.java
index 134c7d021f..3bb0f4c43f 100644
--- a/server/src/test/java/org/apache/druid/segment/indexing/granularity/UniformGranularityTest.java
+++ b/server/src/test/java/org/apache/druid/segment/indexing/granularity/UniformGranularityTest.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.segment.indexing.granularity;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Optional;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import org.apache.druid.jackson.DefaultObjectMapper;
@@ -56,7 +57,7 @@ public class UniformGranularityTest {
 
     Assert.assertTrue(spec.isRollup());
 
-    Assert.assertEquals(inputIntervals, Lists.newArrayList(spec.inputIntervals()));
+    Assert.assertEquals(inputIntervals, new ArrayList<>(spec.inputIntervals()));
 
     Assert.assertEquals(
         Lists.newArrayList(
@@ -291,9 +292,7 @@ public class UniformGranularityTest {
     // created
     final GranularitySpec spec =
         new UniformGranularitySpec(
-            Granularities.SECOND,
-            null,
-            Collections.singletonList(Intervals.of("2012-01-01T00Z/P10Y")));
+            Granularities.SECOND, null, singletonList(Intervals.of("2012-01-01T00Z/P10Y")));
 
     Assert.assertTrue(spec != null);
 
diff --git a/server/src/test/java/org/apache/druid/segment/loading/LocalDataSegmentKillerTest.java b/server/src/test/java/org/apache/druid/segment/loading/LocalDataSegmentKillerTest.java
index 59f169f23c..94e49748ce 100644
--- a/server/src/test/java/org/apache/druid/segment/loading/LocalDataSegmentKillerTest.java
+++ b/server/src/test/java/org/apache/druid/segment/loading/LocalDataSegmentKillerTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.UUID.randomUUID;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.io.File;
 import java.io.IOException;
-import java.util.UUID;
 import org.apache.druid.java.util.common.FileUtils;
 import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.timeline.DataSegment;
@@ -111,7 +112,7 @@ public class LocalDataSegmentKillerTest {
   public void testKillUniquePath() throws Exception {
     final LocalDataSegmentKiller killer =
         new LocalDataSegmentKiller(new LocalDataSegmentPusherConfig());
-    final String uuid = UUID.randomUUID().toString().substring(0, 5);
+    final String uuid = randomUUID().toString().substring(0, 5);
     final File emptyParentDir = temporaryFolder.newFolder();
     final File dataSourceDir = new File(emptyParentDir, DATASOURCE_NAME);
     final File intervalDir = new File(dataSourceDir, "interval");
@@ -139,7 +140,7 @@ public class LocalDataSegmentKillerTest {
     // Verify that
     final LocalDataSegmentKiller killer =
         new LocalDataSegmentKiller(new LocalDataSegmentPusherConfig());
-    final String uuid = UUID.randomUUID().toString().substring(0, 5);
+    final String uuid = randomUUID().toString().substring(0, 5);
     final File emptyParentDir = temporaryFolder.newFolder();
     final File dataSourceDir = new File(emptyParentDir, DATASOURCE_NAME + "_wrong");
     final File intervalDir = new File(dataSourceDir, "interval");
diff --git a/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentArchiverTest.java b/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentArchiverTest.java
index fe362e4571..f683cfc27f 100644
--- a/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentArchiverTest.java
+++ b/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentArchiverTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.segment.loading;
 
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.inject.Inject;
@@ -31,25 +35,24 @@ import org.apache.druid.guice.LazySingleton;
 import org.apache.druid.timeline.DataSegment;
 import org.junit.Assert;
 import org.junit.Test;
-import org.mockito.Mockito;
 
 public class OmniDataSegmentArchiverTest {
   @Test
   public void testArchiveSegmentWithType() throws SegmentLoadingException {
-    final DataSegmentArchiver archiver = Mockito.mock(DataSegmentArchiver.class);
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "sane"));
+    final DataSegmentArchiver archiver = mock(DataSegmentArchiver.class);
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "sane"));
 
     final Injector injector = createInjector(archiver);
     final DataSegmentArchiver segmentArchiver = injector.getInstance(OmniDataSegmentArchiver.class);
     segmentArchiver.archive(segment);
-    Mockito.verify(archiver, Mockito.times(1)).archive(segment);
+    verify(archiver).archive(segment);
   }
 
   @Test
   public void testArchiveSegmentUnknowType() {
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "unknown-type"));
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "unknown-type"));
 
     final Injector injector = createInjector(null);
     final OmniDataSegmentArchiver segmentArchiver =
@@ -62,8 +65,8 @@ public class OmniDataSegmentArchiverTest {
 
   @Test
   public void testBadSegmentArchiverAccessException() {
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "bad"));
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "bad"));
 
     final Injector injector = createInjector(null);
     final OmniDataSegmentArchiver segmentArchiver =
diff --git a/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentKillerTest.java b/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentKillerTest.java
index 97d520a63c..19d4b07fe0 100644
--- a/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentKillerTest.java
+++ b/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentKillerTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.segment.loading;
 
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.inject.Inject;
@@ -33,26 +37,25 @@ import org.apache.druid.timeline.DataSegment;
 import org.apache.druid.timeline.partition.TombstoneShardSpec;
 import org.junit.Assert;
 import org.junit.Test;
-import org.mockito.Mockito;
 
 public class OmniDataSegmentKillerTest {
   @Test
   public void testKillSegmentWithType() throws SegmentLoadingException {
-    final DataSegmentKiller killer = Mockito.mock(DataSegmentKiller.class);
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.isTombstone()).thenReturn(false);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "sane"));
+    final DataSegmentKiller killer = mock(DataSegmentKiller.class);
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.isTombstone()).thenReturn(false);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "sane"));
 
     final Injector injector = createInjector(killer);
     final OmniDataSegmentKiller segmentKiller = injector.getInstance(OmniDataSegmentKiller.class);
     segmentKiller.kill(segment);
-    Mockito.verify(killer, Mockito.times(1)).kill(segment);
+    verify(killer).kill(segment);
   }
 
   @Test
   public void testKillSegmentUnknowType() {
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "unknown-type"));
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "unknown-type"));
 
     final Injector injector = createInjector(null);
     final OmniDataSegmentKiller segmentKiller = injector.getInstance(OmniDataSegmentKiller.class);
@@ -64,8 +67,8 @@ public class OmniDataSegmentKillerTest {
 
   @Test
   public void testBadSegmentKillerAccessException() {
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "bad"));
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "bad"));
 
     final Injector injector = createInjector(null);
     final OmniDataSegmentKiller segmentKiller = injector.getInstance(OmniDataSegmentKiller.class);
diff --git a/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentMoverTest.java b/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentMoverTest.java
index 4d8a8545bc..d321b3fb91 100644
--- a/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentMoverTest.java
+++ b/server/src/test/java/org/apache/druid/segment/loading/OmniDataSegmentMoverTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.segment.loading;
 
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.inject.Inject;
@@ -32,25 +36,24 @@ import org.apache.druid.guice.LazySingleton;
 import org.apache.druid.timeline.DataSegment;
 import org.junit.Assert;
 import org.junit.Test;
-import org.mockito.Mockito;
 
 public class OmniDataSegmentMoverTest {
   @Test
   public void testMoveSegmentWithType() throws SegmentLoadingException {
-    final DataSegmentMover mover = Mockito.mock(DataSegmentMover.class);
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "sane"));
+    final DataSegmentMover mover = mock(DataSegmentMover.class);
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "sane"));
 
     final Injector injector = createInjector(mover);
     final OmniDataSegmentMover segmentMover = injector.getInstance(OmniDataSegmentMover.class);
     segmentMover.move(segment, ImmutableMap.of());
-    Mockito.verify(mover, Mockito.times(1)).move(segment, ImmutableMap.of());
+    verify(mover).move(segment, ImmutableMap.of());
   }
 
   @Test
   public void testMoveSegmentUnknownType() {
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "unknown-type"));
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "unknown-type"));
 
     final Injector injector = createInjector(null);
     final OmniDataSegmentMover segmentMover = injector.getInstance(OmniDataSegmentMover.class);
@@ -62,8 +65,8 @@ public class OmniDataSegmentMoverTest {
 
   @Test
   public void testBadSegmentMoverAccessException() {
-    final DataSegment segment = Mockito.mock(DataSegment.class);
-    Mockito.when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "bad"));
+    final DataSegment segment = mock(DataSegment.class);
+    when(segment.getLoadSpec()).thenReturn(ImmutableMap.of("type", "bad"));
 
     final Injector injector = createInjector(null);
     final OmniDataSegmentMover segmentMover = injector.getInstance(OmniDataSegmentMover.class);
diff --git a/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheLoaderTest.java b/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheLoaderTest.java
index 4a03c1c546..b016d33380 100644
--- a/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheLoaderTest.java
+++ b/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheLoaderTest.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableMap;
 import java.io.IOException;
-import java.util.Collections;
 import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.segment.ReferenceCountingSegment;
 import org.apache.druid.segment.TestHelper;
@@ -50,8 +51,7 @@ public class SegmentLocalCacheLoaderTest {
 
     SegmentLoaderConfig config =
         new SegmentLoaderConfig()
-            .withLocations(
-                Collections.singletonList(storageLoc.toStorageLocationConfig(MAX_SIZE, null)))
+            .withLocations(singletonList(storageLoc.toStorageLocationConfig(MAX_SIZE, null)))
             .withInfoDir(storageLoc.getInfoDir());
 
     objectMapper = TestHelper.makeJsonMapper();
diff --git a/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheManagerConcurrencyTest.java b/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheManagerConcurrencyTest.java
index 55b7014447..3bea14eb33 100644
--- a/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheManagerConcurrencyTest.java
+++ b/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheManagerConcurrencyTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.InjectableValues;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.jsontype.NamedType;
@@ -32,7 +34,6 @@ import java.util.List;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Future;
-import java.util.stream.Collectors;
 import org.apache.druid.jackson.DefaultObjectMapper;
 import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.java.util.common.FileUtils;
@@ -130,7 +131,7 @@ public class SegmentLocalCacheManagerConcurrencyTest {
     final List<Future> futures =
         segmentsToLoad.stream()
             .map(segment -> executorService.submit(() -> manager.getSegmentFiles(segment)))
-            .collect(Collectors.toList());
+            .collect(toList());
 
     expectedException.expect(ExecutionException.class);
     expectedException.expectCause(CoreMatchers.instanceOf(SegmentLoadingException.class));
diff --git a/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheManagerTest.java b/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheManagerTest.java
index 122bf75e60..c265bdadc1 100644
--- a/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheManagerTest.java
+++ b/server/src/test/java/org/apache/druid/segment/loading/SegmentLocalCacheManagerTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.Collections.singletonMap;
+
 import com.fasterxml.jackson.databind.InjectableValues;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.jsontype.NamedType;
@@ -28,7 +30,6 @@ import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.Executors;
 import org.apache.druid.jackson.DefaultObjectMapper;
@@ -124,7 +125,7 @@ public class SegmentLocalCacheManagerTest {
             .dataSource("foo")
             .interval(Intervals.of("2014-10-20T00:00:00Z/P1D"))
             .version("version")
-            .loadSpec(Collections.singletonMap("type", DataSegment.TOMBSTONE_LOADSPEC_TYPE))
+            .loadSpec(singletonMap("type", DataSegment.TOMBSTONE_LOADSPEC_TYPE))
             .shardSpec(TombstoneShardSpec.INSTANCE)
             .size(1)
             .build();
@@ -582,9 +583,7 @@ public class SegmentLocalCacheManagerTest {
 
     final File localStorageFolder = tmpFolder.newFolder(localPath);
     localStorageFolder.setWritable(writable);
-    final StorageLocationConfig locationConfig =
-        new StorageLocationConfig(localStorageFolder, maxSize, 1.0);
-    return locationConfig;
+    return new StorageLocationConfig(localStorageFolder, maxSize, 1.0);
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/segment/loading/StorageLocationTest.java b/server/src/test/java/org/apache/druid/segment/loading/StorageLocationTest.java
index ffbe645f66..d4249ade20 100644
--- a/server/src/test/java/org/apache/druid/segment/loading/StorageLocationTest.java
+++ b/server/src/test/java/org/apache/druid/segment/loading/StorageLocationTest.java
@@ -19,10 +19,13 @@
 
 package org.apache.druid.segment.loading;
 
+import static java.util.Collections.singletonList;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+
 import com.google.common.collect.ImmutableMap;
 import java.io.File;
 import java.io.IOException;
-import java.util.Collections;
 import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.java.util.emitter.EmittingLogger;
 import org.apache.druid.java.util.emitter.service.AlertBuilder;
@@ -36,14 +39,13 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
 import org.mockito.ArgumentCaptor;
-import org.mockito.Mockito;
 
 /** */
 public class StorageLocationTest {
   @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
 
-  @Test
   @SuppressWarnings("GuardedBy")
+  @Test
   public void testStorageLocationFreePercent() {
     // free space ignored only maxSize matters
     StorageLocation locationPlain = fakeLocation(100_000, 5_000, 10_000, null);
@@ -61,8 +63,8 @@ public class StorageLocationTest {
     Assert.assertFalse(locationFull.canHandle(newSegmentId("2012/2013").toString(), 6_000));
   }
 
-  @Test
   @SuppressWarnings("GuardedBy")
+  @Test
   public void testStorageLocationRealFileSystem() throws IOException {
     File file = temporaryFolder.newFolder();
     StorageLocation location = new StorageLocation(file, 10_000, 100.0d);
@@ -115,7 +117,7 @@ public class StorageLocationTest {
 
   @Test
   public void testMaybeReserve() throws IOException {
-    ServiceEmitter emitter = Mockito.mock(ServiceEmitter.class);
+    ServiceEmitter emitter = mock(ServiceEmitter.class);
     ArgumentCaptor<ServiceEventBuilder> argumentCaptor =
         ArgumentCaptor.forClass(ServiceEventBuilder.class);
     EmittingLogger.registerEmitter(emitter);
@@ -147,7 +149,7 @@ public class StorageLocationTest {
     expectedAvail -= 999;
     verifyLoc(expectedAvail, loc);
 
-    Mockito.verify(emitter).emit(argumentCaptor.capture());
+    verify(emitter).emit(argumentCaptor.capture());
     AlertBuilder alertBuilder = (AlertBuilder) argumentCaptor.getValue();
     String description = alertBuilder.build(ImmutableMap.of()).getDescription();
     Assert.assertNotNull(description);
@@ -190,8 +192,8 @@ public class StorageLocationTest {
         Intervals.of(intervalString),
         "1",
         ImmutableMap.of(),
-        Collections.singletonList("d"),
-        Collections.singletonList("m"),
+        singletonList("d"),
+        singletonList("m"),
         null,
         null,
         size);
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/FireHydrantTest.java b/server/src/test/java/org/apache/druid/segment/realtime/FireHydrantTest.java
index 6b82464cd1..ea0bddf910 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/FireHydrantTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/FireHydrantTest.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.segment.realtime;
 
+import static java.util.function.Function.identity;
+
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.Optional;
-import java.util.function.Function;
 import javax.annotation.Nullable;
 import org.apache.druid.java.util.common.ISE;
 import org.apache.druid.java.util.common.Pair;
@@ -109,7 +110,7 @@ public class FireHydrantTest extends InitializedNullHandlingTest {
     Assert.assertEquals(0, incrementalSegmentReference.getNumReferences());
 
     Optional<Pair<SegmentReference, Closeable>> maybeSegmentAndCloseable =
-        hydrant.getSegmentForQuery(Function.identity());
+        hydrant.getSegmentForQuery(identity());
     Assert.assertTrue(maybeSegmentAndCloseable.isPresent());
     Assert.assertEquals(1, incrementalSegmentReference.getNumReferences());
 
@@ -127,7 +128,7 @@ public class FireHydrantTest extends InitializedNullHandlingTest {
     Assert.assertEquals(0, queryableSegmentReference.getNumReferences());
 
     Optional<Pair<SegmentReference, Closeable>> maybeSegmentAndCloseable =
-        hydrant.getSegmentForQuery(Function.identity());
+        hydrant.getSegmentForQuery(identity());
     Assert.assertTrue(maybeSegmentAndCloseable.isPresent());
     Assert.assertEquals(0, incrementalSegmentReference.getNumReferences());
     Assert.assertEquals(1, queryableSegmentReference.getNumReferences());
@@ -192,11 +193,11 @@ public class FireHydrantTest extends InitializedNullHandlingTest {
     incrementalSegmentReference.close();
 
     Optional<Pair<SegmentReference, Closeable>> maybeSegmentAndCloseable =
-        hydrant.getSegmentForQuery(Function.identity());
+        hydrant.getSegmentForQuery(identity());
   }
 
-  @Test
   @SuppressWarnings("ReturnValueIgnored")
+  @Test
   public void testToStringWhenSwappedWithNull() {
     hydrant.swapSegment(null);
     hydrant.toString();
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/ClosedSegmentsSinksBatchAppenderatorDriverTest.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/ClosedSegmentsSinksBatchAppenderatorDriverTest.java
index 12fc04eafc..9762e2bdab 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/ClosedSegmentsSinksBatchAppenderatorDriverTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/ClosedSegmentsSinksBatchAppenderatorDriverTest.java
@@ -19,18 +19,20 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Collections.emptySet;
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toSet;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
-import java.util.function.Function;
-import java.util.stream.Collectors;
 import org.apache.druid.common.config.NullHandling;
 import org.apache.druid.data.input.InputRow;
 import org.apache.druid.data.input.MapBasedInputRow;
@@ -120,7 +122,7 @@ public class ClosedSegmentsSinksBatchAppenderatorDriverTest extends EasyMockSupp
 
     final SegmentsAndCommitMetadata published =
         driver
-            .publishAll(null, null, Collections.emptySet(), makeOkPublisher(), Function.identity())
+            .publishAll(null, null, emptySet(), makeOkPublisher(), identity())
             .get(TIMEOUT, TimeUnit.MILLISECONDS);
 
     Assert.assertEquals(
@@ -131,7 +133,7 @@ public class ClosedSegmentsSinksBatchAppenderatorDriverTest extends EasyMockSupp
                 DATA_SOURCE, Intervals.of("2000T01/PT1H"), VERSION, new NumberedShardSpec(0, 0))),
         published.getSegments().stream()
             .map(SegmentIdWithShardSpec::fromDataSegment)
-            .collect(Collectors.toSet()));
+            .collect(toSet()));
 
     Assert.assertNull(published.getCommitMetadata());
   }
@@ -154,7 +156,7 @@ public class ClosedSegmentsSinksBatchAppenderatorDriverTest extends EasyMockSupp
 
     final SegmentsAndCommitMetadata published =
         driver
-            .publishAll(null, null, Collections.emptySet(), makeOkPublisher(), Function.identity())
+            .publishAll(null, null, emptySet(), makeOkPublisher(), identity())
             .get(TIMEOUT, TimeUnit.MILLISECONDS);
 
     Assert.assertEquals(
@@ -167,7 +169,7 @@ public class ClosedSegmentsSinksBatchAppenderatorDriverTest extends EasyMockSupp
                 DATA_SOURCE, Intervals.of("2000T01/PT1H"), VERSION, new NumberedShardSpec(1, 0))),
         published.getSegments().stream()
             .map(SegmentIdWithShardSpec::fromDataSegment)
-            .collect(Collectors.toSet()));
+            .collect(toSet()));
 
     Assert.assertNull(published.getCommitMetadata());
   }
@@ -188,7 +190,7 @@ public class ClosedSegmentsSinksBatchAppenderatorDriverTest extends EasyMockSupp
         segmentsForSequence
             .allSegmentStateStream()
             .filter(segmentWithState -> segmentWithState.getState() == expectedState)
-            .collect(Collectors.toList());
+            .collect(toList());
 
     Assert.assertEquals(expectedNumSegmentsInState, segmentWithStates.size());
   }
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/ClosedSegmentsSinksBatchAppenderatorTest.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/ClosedSegmentsSinksBatchAppenderatorTest.java
index 3c954d097e..7d2ef65d4d 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/ClosedSegmentsSinksBatchAppenderatorTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/ClosedSegmentsSinksBatchAppenderatorTest.java
@@ -19,14 +19,16 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.util.concurrent.ListenableFuture;
 import java.io.File;
-import java.util.Collections;
 import java.util.List;
-import java.util.stream.Collectors;
 import org.apache.druid.common.config.NullHandling;
 import org.apache.druid.data.input.InputRow;
 import org.apache.druid.data.input.MapBasedInputRow;
@@ -80,7 +82,7 @@ public class ClosedSegmentsSinksBatchAppenderatorTest extends InitializedNullHan
       // getSegments
       Assert.assertEquals(
           IDENTIFIERS.subList(0, 2),
-          appenderator.getSegments().stream().sorted().collect(Collectors.toList()));
+          appenderator.getSegments().stream().sorted().collect(toList()));
 
       // add #3, this hits max rows in memory:
       Assert.assertEquals(
@@ -93,9 +95,9 @@ public class ClosedSegmentsSinksBatchAppenderatorTest extends InitializedNullHan
       // etc.)
       // above should be cleared now
       Assert.assertEquals(
-          Collections.emptyList(),
+          emptyList(),
           ((BatchAppenderator) appenderator)
-              .getInMemorySegments().stream().sorted().collect(Collectors.toList()));
+              .getInMemorySegments().stream().sorted().collect(toList()));
 
       // add #4, this will add one more temporary segment:
       Assert.assertEquals(
@@ -113,10 +115,10 @@ public class ClosedSegmentsSinksBatchAppenderatorTest extends InitializedNullHan
                   segmentsAndCommitMetadata.getSegments(), SegmentIdWithShardSpec::fromDataSegment)
               .stream()
               .sorted()
-              .collect(Collectors.toList()));
+              .collect(toList()));
       Assert.assertEquals(
-          tester.getPushedSegments().stream().sorted().collect(Collectors.toList()),
-          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(Collectors.toList()));
+          tester.getPushedSegments().stream().sorted().collect(toList()),
+          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(toList()));
 
       appenderator.close();
       Assert.assertTrue(appenderator.getSegments().isEmpty());
@@ -159,29 +161,29 @@ public class ClosedSegmentsSinksBatchAppenderatorTest extends InitializedNullHan
 
       // getSegments
       Assert.assertEquals(
-          Collections.singletonList(segmentIdWithNonUTCTime),
-          appenderator.getSegments().stream().sorted().collect(Collectors.toList()));
+          singletonList(segmentIdWithNonUTCTime),
+          appenderator.getSegments().stream().sorted().collect(toList()));
 
       // since we just added one row and the max rows in memory is one, all the segments (sinks etc)
       // above should be cleared now
       Assert.assertEquals(
-          Collections.emptyList(),
+          emptyList(),
           ((BatchAppenderator) appenderator)
-              .getInMemorySegments().stream().sorted().collect(Collectors.toList()));
+              .getInMemorySegments().stream().sorted().collect(toList()));
 
       // push all
       final SegmentsAndCommitMetadata segmentsAndCommitMetadata =
           appenderator.push(appenderator.getSegments(), null, false).get();
       Assert.assertEquals(
-          Collections.singletonList(segmentIdWithNonUTCTime),
+          singletonList(segmentIdWithNonUTCTime),
           Lists.transform(
                   segmentsAndCommitMetadata.getSegments(), SegmentIdWithShardSpec::fromDataSegment)
               .stream()
               .sorted()
-              .collect(Collectors.toList()));
+              .collect(toList()));
       Assert.assertEquals(
-          tester.getPushedSegments().stream().sorted().collect(Collectors.toList()),
-          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(Collectors.toList()));
+          tester.getPushedSegments().stream().sorted().collect(toList()),
+          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(toList()));
 
       appenderator.close();
       Assert.assertTrue(appenderator.getSegments().isEmpty());
@@ -557,7 +559,7 @@ public class ClosedSegmentsSinksBatchAppenderatorTest extends InitializedNullHan
                   segmentsAndCommitMetadata.getSegments(), SegmentIdWithShardSpec::fromDataSegment)
               .stream()
               .sorted()
-              .collect(Collectors.toList()));
+              .collect(toList()));
       Assert.assertEquals(0, ((BatchAppenderator) appenderator).getRowsInMemory());
       appenderator.close();
     }
@@ -763,26 +765,25 @@ public class ClosedSegmentsSinksBatchAppenderatorTest extends InitializedNullHan
 
       // push only a single segment
       final SegmentsAndCommitMetadata segmentsAndCommitMetadata =
-          appenderator.push(Collections.singletonList(IDENTIFIERS.get(0)), null, false).get();
+          appenderator.push(singletonList(IDENTIFIERS.get(0)), null, false).get();
 
       // only one segment must have been pushed:
       Assert.assertEquals(
-          Collections.singletonList(IDENTIFIERS.get(0)),
+          singletonList(IDENTIFIERS.get(0)),
           Lists.transform(
                   segmentsAndCommitMetadata.getSegments(), SegmentIdWithShardSpec::fromDataSegment)
               .stream()
               .sorted()
-              .collect(Collectors.toList()));
+              .collect(toList()));
 
       Assert.assertEquals(
-          tester.getPushedSegments().stream().sorted().collect(Collectors.toList()),
-          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(Collectors.toList()));
+          tester.getPushedSegments().stream().sorted().collect(toList()),
+          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(toList()));
       // the responsability for dropping is in the BatchAppenderatorDriver, drop manually:
       appenderator.drop(IDENTIFIERS.get(0));
 
       // and the segment that was not pushed should still be active
-      Assert.assertEquals(
-          Collections.singletonList(IDENTIFIERS.get(1)), appenderator.getSegments());
+      Assert.assertEquals(singletonList(IDENTIFIERS.get(1)), appenderator.getSegments());
     }
   }
 
@@ -800,12 +801,12 @@ public class ClosedSegmentsSinksBatchAppenderatorTest extends InitializedNullHan
 
       // push only a single segment
       ListenableFuture<SegmentsAndCommitMetadata> firstFuture =
-          appenderator.push(Collections.singletonList(IDENTIFIERS.get(0)), null, false);
+          appenderator.push(singletonList(IDENTIFIERS.get(0)), null, false);
 
       // push remaining segments:
       appenderator.add(IDENTIFIERS.get(1), createInputRow("2000", "bar3", 1), null);
       ListenableFuture<SegmentsAndCommitMetadata> secondFuture =
-          appenderator.push(Collections.singletonList(IDENTIFIERS.get(1)), null, false);
+          appenderator.push(singletonList(IDENTIFIERS.get(1)), null, false);
 
       // close should wait for all pushes and persists to end:
       appenderator.close();
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/OpenAndClosedSegmentsBatchAppenderatorDriverTest.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/OpenAndClosedSegmentsBatchAppenderatorDriverTest.java
index 7dc6b53e19..586765a4a8 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/OpenAndClosedSegmentsBatchAppenderatorDriverTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/OpenAndClosedSegmentsBatchAppenderatorDriverTest.java
@@ -19,14 +19,16 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toSet;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.util.Arrays;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
-import java.util.function.Function;
-import java.util.stream.Collectors;
 import org.apache.druid.common.config.NullHandling;
 import org.apache.druid.data.input.InputRow;
 import org.apache.druid.data.input.MapBasedInputRow;
@@ -116,7 +118,7 @@ public class OpenAndClosedSegmentsBatchAppenderatorDriverTest extends EasyMockSu
 
     final SegmentsAndCommitMetadata published =
         driver
-            .publishAll(null, null, null, makeOkPublisher(), Function.identity())
+            .publishAll(null, null, null, makeOkPublisher(), identity())
             .get(TIMEOUT, TimeUnit.MILLISECONDS);
 
     Assert.assertEquals(
@@ -127,7 +129,7 @@ public class OpenAndClosedSegmentsBatchAppenderatorDriverTest extends EasyMockSu
                 DATA_SOURCE, Intervals.of("2000T01/PT1H"), VERSION, new NumberedShardSpec(0, 0))),
         published.getSegments().stream()
             .map(SegmentIdWithShardSpec::fromDataSegment)
-            .collect(Collectors.toSet()));
+            .collect(toSet()));
 
     Assert.assertNull(published.getCommitMetadata());
   }
@@ -150,7 +152,7 @@ public class OpenAndClosedSegmentsBatchAppenderatorDriverTest extends EasyMockSu
 
     final SegmentsAndCommitMetadata published =
         driver
-            .publishAll(null, null, null, makeOkPublisher(), Function.identity())
+            .publishAll(null, null, null, makeOkPublisher(), identity())
             .get(TIMEOUT, TimeUnit.MILLISECONDS);
 
     Assert.assertEquals(
@@ -163,7 +165,7 @@ public class OpenAndClosedSegmentsBatchAppenderatorDriverTest extends EasyMockSu
                 DATA_SOURCE, Intervals.of("2000T01/PT1H"), VERSION, new NumberedShardSpec(1, 0))),
         published.getSegments().stream()
             .map(SegmentIdWithShardSpec::fromDataSegment)
-            .collect(Collectors.toSet()));
+            .collect(toSet()));
 
     Assert.assertNull(published.getCommitMetadata());
   }
@@ -184,7 +186,7 @@ public class OpenAndClosedSegmentsBatchAppenderatorDriverTest extends EasyMockSu
         segmentsForSequence
             .allSegmentStateStream()
             .filter(segmentWithState -> segmentWithState.getState() == expectedState)
-            .collect(Collectors.toList());
+            .collect(toList());
 
     Assert.assertEquals(expectedNumSegmentsInState, segmentWithStates.size());
   }
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/OpenAndClosedSegmentsBatchAppenderatorTest.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/OpenAndClosedSegmentsBatchAppenderatorTest.java
index 2d5f5de97f..57283d24b0 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/OpenAndClosedSegmentsBatchAppenderatorTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/OpenAndClosedSegmentsBatchAppenderatorTest.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.base.Function;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import java.util.List;
-import java.util.stream.Collectors;
 import org.apache.druid.data.input.InputRow;
 import org.apache.druid.data.input.MapBasedInputRow;
 import org.apache.druid.java.util.common.DateTimes;
@@ -78,7 +79,7 @@ public class OpenAndClosedSegmentsBatchAppenderatorTest extends InitializedNullH
       // getSegments
       Assert.assertEquals(
           IDENTIFIERS.subList(0, 2),
-          appenderator.getSegments().stream().sorted().collect(Collectors.toList()));
+          appenderator.getSegments().stream().sorted().collect(toList()));
 
       // getRowCount
       Assert.assertEquals(2, appenderator.getRowCount(IDENTIFIERS.get(0)));
@@ -106,10 +107,10 @@ public class OpenAndClosedSegmentsBatchAppenderatorTest extends InitializedNullH
                   })
               .stream()
               .sorted()
-              .collect(Collectors.toList()));
+              .collect(toList()));
       Assert.assertEquals(
-          tester.getPushedSegments().stream().sorted().collect(Collectors.toList()),
-          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(Collectors.toList()));
+          tester.getPushedSegments().stream().sorted().collect(toList()),
+          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(toList()));
 
       appenderator.clear();
       Assert.assertTrue(appenderator.getSegments().isEmpty());
@@ -152,7 +153,7 @@ public class OpenAndClosedSegmentsBatchAppenderatorTest extends InitializedNullH
       // getSegments
       Assert.assertEquals(
           IDENTIFIERS.subList(0, 2),
-          appenderator.getSegments().stream().sorted().collect(Collectors.toList()));
+          appenderator.getSegments().stream().sorted().collect(toList()));
 
       // getRowCount
       Assert.assertEquals(2, appenderator.getRowCount(IDENTIFIERS.get(0)));
@@ -180,10 +181,10 @@ public class OpenAndClosedSegmentsBatchAppenderatorTest extends InitializedNullH
                   })
               .stream()
               .sorted()
-              .collect(Collectors.toList()));
+              .collect(toList()));
       Assert.assertEquals(
-          tester.getPushedSegments().stream().sorted().collect(Collectors.toList()),
-          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(Collectors.toList()));
+          tester.getPushedSegments().stream().sorted().collect(toList()),
+          segmentsAndCommitMetadata.getSegments().stream().sorted().collect(toList()));
 
       appenderator.clear();
       Assert.assertTrue(appenderator.getSegments().isEmpty());
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriverFailTest.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriverFailTest.java
index be175caa72..afb0726382 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriverFailTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriverFailTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Supplier;
 import com.google.common.collect.ImmutableList;
@@ -37,7 +39,6 @@ import java.util.TreeMap;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
-import java.util.stream.Collectors;
 import org.apache.druid.data.input.Committer;
 import org.apache.druid.data.input.InputRow;
 import org.apache.druid.data.input.MapBasedInputRow;
@@ -465,7 +466,7 @@ public class StreamAppenderatorDriverFailTest extends EasyMockSupport {
                             id.getShardSpec(),
                             0,
                             0))
-                .collect(Collectors.toList());
+                .collect(toList());
         return Futures.transform(
             persistAll(committer),
             commitMetadata -> new SegmentsAndCommitMetadata(segments, commitMetadata),
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriverTest.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriverTest.java
index a5fdf1d28b..b3db0630b3 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriverTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorDriverTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Collections.emptySet;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Supplier;
 import com.google.common.collect.ImmutableList;
@@ -28,7 +30,6 @@ import com.google.common.collect.Iterables;
 import com.google.common.util.concurrent.ListenableFuture;
 import java.io.IOException;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -351,7 +352,7 @@ public class StreamAppenderatorDriverTest extends EasyMockSupport {
 
   static TransactionalSegmentPublisher makeOkPublisher() {
     return (segmentsToBeOverwritten, segmentsToBeDropped, segmentsToPublish, commitMetadata) ->
-        SegmentPublishResult.ok(Collections.emptySet());
+        SegmentPublishResult.ok(emptySet());
   }
 
   static TransactionalSegmentPublisher makeFailingPublisher(boolean failWithException) {
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorTest.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorTest.java
index 0b3d9e38cf..2d4c8d9ae8 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorTest.java
@@ -25,6 +25,7 @@ import com.google.common.base.Suppliers;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
@@ -1181,7 +1182,7 @@ public class StreamAppenderatorTest extends InitializedNullHandlingTest {
     return new Supplier<Committer>() {
       @Override
       public Committer get() {
-        final Map<String, String> mapCopy = ImmutableMap.copyOf(map);
+        final Map<String, String> mapCopy = map;
 
         return new Committer() {
           @Override
@@ -1199,7 +1200,7 @@ public class StreamAppenderatorTest extends InitializedNullHandlingTest {
   }
 
   private static <T> List<T> sorted(final List<T> xs) {
-    final List<T> xsSorted = Lists.newArrayList(xs);
+    final List<T> xsSorted = new ArrayList<>(xs);
     Collections.sort(
         xsSorted,
         (T a, T b) -> {
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorTester.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorTester.java
index 501bba54ac..1f445127b2 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorTester.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/StreamAppenderatorTester.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Objects.requireNonNull;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import java.io.File;
 import java.io.IOException;
@@ -309,7 +310,7 @@ public class StreamAppenderatorTester implements AutoCloseable {
       return new StreamAppenderatorTester(
           maxRowsInMemory,
           maxSizeInBytes,
-          Preconditions.checkNotNull(basePersistDirectory, "basePersistDirectory"),
+          requireNonNull(basePersistDirectory, "basePersistDirectory"),
           enablePushFailure,
           rowIngestionMeters == null ? new SimpleRowIngestionMeters() : rowIngestionMeters,
           skipBytesInMemoryOverheadCheck);
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/UnifiedIndexerAppenderatorsManagerTest.java b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/UnifiedIndexerAppenderatorsManagerTest.java
index bac561d4a6..bf2bd9080f 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/appenderator/UnifiedIndexerAppenderatorsManagerTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/appenderator/UnifiedIndexerAppenderatorsManagerTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.segment.realtime.appenderator;
 
+import static java.util.Collections.emptyList;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.io.File;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import javax.annotation.Nullable;
 import org.apache.druid.client.cache.CacheConfig;
@@ -100,7 +101,7 @@ public class UnifiedIndexerAppenderatorsManagerTest extends InitializedNullHandl
                 null,
                 null,
                 new UniformGranularitySpec(
-                    Granularities.HOUR, Granularities.HOUR, false, Collections.emptyList()),
+                    Granularities.HOUR, Granularities.HOUR, false, emptyList()),
                 null),
             appenderatorConfig,
             new FireDepartmentMetrics(),
diff --git a/server/src/test/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseTest.java b/server/src/test/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseTest.java
index abf9a10ec8..507e895de6 100644
--- a/server/src/test/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseTest.java
+++ b/server/src/test/java/org/apache/druid/segment/realtime/firehose/EventReceiverFirehoseTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.segment.realtime.firehose;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterables;
 import java.io.IOException;
 import java.io.InputStream;
-import java.nio.charset.StandardCharsets;
 import java.util.Map;
 import java.util.concurrent.Callable;
 import java.util.concurrent.ExecutionException;
@@ -96,7 +97,7 @@ public class EventReceiverFirehoseTest {
   public void testSingleThread() throws IOException, InterruptedException {
     for (int i = 0; i < NUM_EVENTS; ++i) {
       setUpRequestExpectations(null, null);
-      final InputStream inputStream = IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+      final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
       firehose.addAll(inputStream, req);
       Assert.assertEquals(i + 1, firehose.getCurrentBufferSize());
       inputStream.close();
@@ -158,8 +159,7 @@ public class EventReceiverFirehoseTest {
               @Override
               public Boolean call() throws Exception {
                 for (int i = 0; i < NUM_EVENTS; ++i) {
-                  final InputStream inputStream =
-                      IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+                  final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
                   firehose.addAll(inputStream, req);
                   inputStream.close();
                 }
@@ -168,7 +168,7 @@ public class EventReceiverFirehoseTest {
             });
 
     for (int i = 0; i < NUM_EVENTS; ++i) {
-      final InputStream inputStream = IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+      final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
       firehose.addAll(inputStream, req);
       inputStream.close();
     }
@@ -289,7 +289,7 @@ public class EventReceiverFirehoseTest {
     for (int i = 0; i < NUM_EVENTS; ++i) {
       setUpRequestExpectations("producer", String.valueOf(i));
 
-      final InputStream inputStream = IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+      final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
       firehose.addAll(inputStream, req);
       Assert.assertEquals(i + 1, firehose.getCurrentBufferSize());
       inputStream.close();
@@ -328,7 +328,7 @@ public class EventReceiverFirehoseTest {
     for (int i = 0; i < NUM_EVENTS; ++i) {
       setUpRequestExpectations("producer", "1");
 
-      final InputStream inputStream = IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+      final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
       final Response response = firehose.addAll(inputStream, req);
       Assert.assertEquals(Response.Status.OK.getStatusCode(), response.getStatus());
       Assert.assertEquals(1, firehose.getCurrentBufferSize());
@@ -344,7 +344,7 @@ public class EventReceiverFirehoseTest {
   public void testMissingProducerSequence() throws IOException {
     setUpRequestExpectations("producer", null);
 
-    final InputStream inputStream = IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+    final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
     final Response response = firehose.addAll(inputStream, req);
 
     Assert.assertEquals(Response.Status.BAD_REQUEST.getStatusCode(), response.getStatus());
@@ -361,7 +361,7 @@ public class EventReceiverFirehoseTest {
     for (int i = 0; i < EventReceiverFirehoseFactory.MAX_FIREHOSE_PRODUCERS - 1; i++) {
       setUpRequestExpectations("producer-" + i, "0");
 
-      final InputStream inputStream = IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+      final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
       final Response response = firehose.addAll(inputStream, req);
       Assert.assertEquals(Response.Status.OK.getStatusCode(), response.getStatus());
       inputStream.close();
@@ -371,7 +371,7 @@ public class EventReceiverFirehoseTest {
 
     setUpRequestExpectations("toomany", "0");
 
-    final InputStream inputStream = IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+    final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
     final Response response = firehose.addAll(inputStream, req);
     Assert.assertEquals(Response.Status.FORBIDDEN.getStatusCode(), response.getStatus());
     inputStream.close();
@@ -385,7 +385,7 @@ public class EventReceiverFirehoseTest {
   public void testNaNProducerSequence() throws IOException {
     setUpRequestExpectations("producer", "foo");
 
-    final InputStream inputStream = IOUtils.toInputStream(inputRow, StandardCharsets.UTF_8);
+    final InputStream inputStream = IOUtils.toInputStream(inputRow, UTF_8);
     final Response response = firehose.addAll(inputStream, req);
 
     Assert.assertEquals(Response.Status.BAD_REQUEST.getStatusCode(), response.getStatus());
diff --git a/server/src/test/java/org/apache/druid/server/AsyncManagementForwardingServletTest.java b/server/src/test/java/org/apache/druid/server/AsyncManagementForwardingServletTest.java
index f551175449..8bf2d9e857 100644
--- a/server/src/test/java/org/apache/druid/server/AsyncManagementForwardingServletTest.java
+++ b/server/src/test/java/org/apache/druid/server/AsyncManagementForwardingServletTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
@@ -28,7 +30,6 @@ import java.io.IOException;
 import java.io.OutputStream;
 import java.net.HttpURLConnection;
 import java.net.URL;
-import java.nio.charset.StandardCharsets;
 import java.util.Map;
 import java.util.zip.Deflater;
 import javax.annotation.Nullable;
@@ -91,8 +92,8 @@ public class AsyncManagementForwardingServletTest extends BaseJettyTest {
     }
   }
 
-  @Override
   @Before
+  @Override
   public void setup() throws Exception {
     super.setup();
 
@@ -264,7 +265,7 @@ public class AsyncManagementForwardingServletTest extends BaseJettyTest {
 
     connection.setDoOutput(true);
     OutputStream os = connection.getOutputStream();
-    os.write(COORDINATOR_EXPECTED_REQUEST.body.getBytes(StandardCharsets.UTF_8));
+    os.write(COORDINATOR_EXPECTED_REQUEST.body.getBytes(UTF_8));
     os.close();
 
     Assert.assertEquals(200, connection.getResponseCode());
@@ -294,7 +295,7 @@ public class AsyncManagementForwardingServletTest extends BaseJettyTest {
 
     connection.setDoOutput(true);
     OutputStream os = connection.getOutputStream();
-    os.write(OVERLORD_EXPECTED_REQUEST.body.getBytes(StandardCharsets.UTF_8));
+    os.write(OVERLORD_EXPECTED_REQUEST.body.getBytes(UTF_8));
     os.close();
 
     Assert.assertEquals(200, connection.getResponseCode());
diff --git a/server/src/test/java/org/apache/druid/server/ClientQuerySegmentWalkerTest.java b/server/src/test/java/org/apache/druid/server/ClientQuerySegmentWalkerTest.java
index fc1b5ccf26..922015df4b 100644
--- a/server/src/test/java/org/apache/druid/server/ClientQuerySegmentWalkerTest.java
+++ b/server/src/test/java/org/apache/druid/server/ClientQuerySegmentWalkerTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server;
 
+import static java.util.Collections.singletonList;
+import static java.util.Comparator.naturalOrder;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
@@ -26,8 +29,6 @@ import com.google.common.collect.ImmutableSet;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
@@ -274,7 +275,7 @@ public class ClientQuerySegmentWalkerTest {
             Druids.newTimeseriesQueryBuilder()
                 .dataSource(FOO)
                 .granularity(Granularities.ALL)
-                .intervals(Collections.singletonList(INTERVAL))
+                .intervals(singletonList(INTERVAL))
                 .aggregators(new LongSumAggregatorFactory("sum", "n"))
                 .context(ImmutableMap.of(TimeseriesQuery.CTX_GRAND_TOTAL, false))
                 .build()
@@ -298,7 +299,7 @@ public class ClientQuerySegmentWalkerTest {
             Druids.newTimeseriesQueryBuilder()
                 .dataSource(GLOBAL)
                 .granularity(Granularities.ALL)
-                .intervals(Collections.singletonList(INTERVAL))
+                .intervals(singletonList(INTERVAL))
                 .aggregators(new LongSumAggregatorFactory("sum", "n"))
                 .context(ImmutableMap.of(TimeseriesQuery.CTX_GRAND_TOTAL, false))
                 .build()
@@ -310,7 +311,7 @@ public class ClientQuerySegmentWalkerTest {
             Druids.newTimeseriesQueryBuilder()
                 .dataSource(new GlobalTableDataSource(GLOBAL))
                 .granularity(Granularities.ALL)
-                .intervals(Collections.singletonList(INTERVAL))
+                .intervals(singletonList(INTERVAL))
                 .aggregators(new LongSumAggregatorFactory("sum", "n"))
                 .context(ImmutableMap.of(TimeseriesQuery.CTX_GRAND_TOTAL, false))
                 .build()
@@ -334,7 +335,7 @@ public class ClientQuerySegmentWalkerTest {
             Druids.newTimeseriesQueryBuilder()
                 .dataSource(FOO_INLINE)
                 .granularity(Granularities.ALL)
-                .intervals(Collections.singletonList(INTERVAL))
+                .intervals(singletonList(INTERVAL))
                 .aggregators(new LongSumAggregatorFactory("sum", "n"))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -356,7 +357,7 @@ public class ClientQuerySegmentWalkerTest {
         GroupByQuery.builder()
             .setDataSource(FOO)
             .setGranularity(Granularities.ALL)
-            .setInterval(Collections.singletonList(INTERVAL))
+            .setInterval(singletonList(INTERVAL))
             .setDimensions(DefaultDimensionSpec.of("s"))
             .build();
 
@@ -398,7 +399,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(FOO)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(DefaultDimensionSpec.of("s"))
                 .build()
                 .withId("queryId");
@@ -495,7 +496,7 @@ public class ClientQuerySegmentWalkerTest {
         GroupByQuery.builder()
             .setDataSource(FOO)
             .setGranularity(Granularities.ALL)
-            .setInterval(Collections.singletonList(INTERVAL))
+            .setInterval(singletonList(INTERVAL))
             .setDimensions(DefaultDimensionSpec.of("s"))
             .setDimFilter(new SelectorDimFilter("s", "y", null))
             .build();
@@ -557,7 +558,7 @@ public class ClientQuerySegmentWalkerTest {
         GroupByQuery.builder()
             .setDataSource(unionDataSource)
             .setGranularity(Granularities.ALL)
-            .setInterval(Collections.singletonList(INTERVAL))
+            .setInterval(singletonList(INTERVAL))
             .setDimensions(DefaultDimensionSpec.of("s"))
             .setDimFilter(new SelectorDimFilter("s", "y", null))
             .build();
@@ -762,7 +763,7 @@ public class ClientQuerySegmentWalkerTest {
         GroupByQuery.builder()
             .setDataSource(FOO)
             .setGranularity(Granularities.ALL)
-            .setInterval(Collections.singletonList(INTERVAL))
+            .setInterval(singletonList(INTERVAL))
             .setDimensions(DefaultDimensionSpec.of("s"))
             .build();
 
@@ -789,7 +790,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(new DefaultDimensionSpec("ad", "ad", ColumnType.DOUBLE_ARRAY))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -811,7 +812,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(DefaultDimensionSpec.of("ad"))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -833,7 +834,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY_UNKNOWN)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(DefaultDimensionSpec.of("ad"))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -865,7 +866,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(new DefaultDimensionSpec("al", "al", ColumnType.LONG_ARRAY))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -887,7 +888,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(DefaultDimensionSpec.of("al"))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -910,7 +911,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY_UNKNOWN)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(DefaultDimensionSpec.of("al"))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -942,7 +943,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(new DefaultDimensionSpec("as", "as", ColumnType.STRING_ARRAY))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -964,7 +965,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(DefaultDimensionSpec.of("as"))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -986,7 +987,7 @@ public class ClientQuerySegmentWalkerTest {
             GroupByQuery.builder()
                 .setDataSource(ARRAY_UNKNOWN)
                 .setGranularity(Granularities.ALL)
-                .setInterval(Collections.singletonList(INTERVAL))
+                .setInterval(singletonList(INTERVAL))
                 .setDimensions(DefaultDimensionSpec.of("as"))
                 .build()
                 .withId(DUMMY_QUERY_ID);
@@ -1195,7 +1196,7 @@ public class ClientQuerySegmentWalkerTest {
             Druids.newTimeseriesQueryBuilder()
                 .dataSource(ARRAY)
                 .granularity(Granularities.ALL)
-                .intervals(Collections.singletonList(INTERVAL))
+                .intervals(singletonList(INTERVAL))
                 .aggregators(new LongSumAggregatorFactory("sum", "al"))
                 .context(ImmutableMap.of(TimeseriesQuery.CTX_GRAND_TOTAL, false))
                 .build()
@@ -1221,7 +1222,7 @@ public class ClientQuerySegmentWalkerTest {
             Druids.newTimeseriesQueryBuilder()
                 .dataSource(ARRAY_UNKNOWN)
                 .granularity(Granularities.ALL)
-                .intervals(Collections.singletonList(INTERVAL))
+                .intervals(singletonList(INTERVAL))
                 .aggregators(new LongSumAggregatorFactory("sum", "al"))
                 .context(ImmutableMap.of(TimeseriesQuery.CTX_GRAND_TOTAL, false))
                 .build()
@@ -1435,7 +1436,7 @@ public class ClientQuerySegmentWalkerTest {
   private static VersionedIntervalTimeline<String, ReferenceCountingSegment> makeTimeline(
       final String name, final InlineDataSource dataSource) {
     final VersionedIntervalTimeline<String, ReferenceCountingSegment> timeline =
-        new VersionedIntervalTimeline<>(Comparator.naturalOrder());
+        new VersionedIntervalTimeline<>(naturalOrder());
 
     timeline.add(
         INTERVAL,
diff --git a/server/src/test/java/org/apache/druid/server/ConsistentHasherTest.java b/server/src/test/java/org/apache/druid/server/ConsistentHasherTest.java
index 3894dd7a8b..ba4ea3ced5 100644
--- a/server/src/test/java/org/apache/druid/server/ConsistentHasherTest.java
+++ b/server/src/test/java/org/apache/druid/server/ConsistentHasherTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server;
 
+import static java.util.UUID.randomUUID;
+
 import com.google.common.hash.HashFunction;
 import com.google.common.hash.Hashing;
 import java.util.HashMap;
@@ -52,7 +54,7 @@ public class ConsistentHasherTest {
     hasher.updateKeys(nodes);
 
     for (int i = 0; i < NUM_ITERATIONS; i++) {
-      UUID objectId = UUID.randomUUID();
+      UUID objectId = randomUUID();
       String targetServer = hasher.findKey(StringUtils.toUtf8(objectId.toString()));
       uuidServerMap.put(objectId.toString(), targetServer);
     }
@@ -80,7 +82,7 @@ public class ConsistentHasherTest {
     Map<String, String> uuidServerMap = new HashMap<>();
 
     for (int i = 0; i < NUM_ITERATIONS; i++) {
-      UUID objectId = UUID.randomUUID();
+      UUID objectId = randomUUID();
       String targetServer = hasher.findKey(StringUtils.toUtf8(objectId.toString()));
       uuidServerMap.put(objectId.toString(), targetServer);
     }
@@ -121,7 +123,7 @@ public class ConsistentHasherTest {
     Map<String, String> uuidServerMap = new HashMap<>();
 
     for (int i = 0; i < NUM_ITERATIONS; i++) {
-      UUID objectId = UUID.randomUUID();
+      UUID objectId = randomUUID();
       String targetServer = hasher.findKey(StringUtils.toUtf8(objectId.toString()));
       uuidServerMap.put(objectId.toString(), targetServer);
     }
@@ -219,7 +221,7 @@ public class ConsistentHasherTest {
 
     Map<String, String> uuidServerMap = new HashMap<>();
     for (int i = 0; i < NUM_ITERATIONS; i++) {
-      UUID objectId = UUID.randomUUID();
+      UUID objectId = randomUUID();
       String targetServer = hasher.findKey(StringUtils.toUtf8(objectId.toString()));
       uuidServerMap.put(objectId.toString(), targetServer);
     }
diff --git a/server/src/test/java/org/apache/druid/server/QueryResourceTest.java b/server/src/test/java/org/apache/druid/server/QueryResourceTest.java
index 7f5ce8e753..4f04063fe2 100644
--- a/server/src/test/java/org/apache/druid/server/QueryResourceTest.java
+++ b/server/src/test/java/org/apache/druid/server/QueryResourceTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.emptyList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.jaxrs.smile.SmileMediaTypes;
@@ -33,9 +36,7 @@ import com.google.inject.Key;
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
-import java.nio.charset.StandardCharsets;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Executors;
@@ -350,9 +351,7 @@ public class QueryResourceTest {
 
     MockHttpServletResponse response =
         expectAsyncRequestFlow(
-            testServletRequest,
-            SIMPLE_TIMESERIES_QUERY.getBytes(StandardCharsets.UTF_8),
-            queryResource);
+            testServletRequest, SIMPLE_TIMESERIES_QUERY.getBytes(UTF_8), queryResource);
     Assert.assertEquals(1, queryResource.getInterruptedQueryCount());
     Assert.assertEquals(HttpStatus.SC_INTERNAL_SERVER_ERROR, response.getStatus());
     final String expectedException =
@@ -376,9 +375,7 @@ public class QueryResourceTest {
 
     final MockHttpServletResponse response =
         expectAsyncRequestFlow(
-            testServletRequest,
-            SIMPLE_TIMESERIES_QUERY.getBytes(StandardCharsets.UTF_8),
-            queryResource);
+            testServletRequest, SIMPLE_TIMESERIES_QUERY.getBytes(UTF_8), queryResource);
     Assert.assertEquals(HttpStatus.SC_OK, response.getStatus());
   }
 
@@ -462,7 +459,7 @@ public class QueryResourceTest {
   public void testBadQuery() throws IOException {
     Response response =
         queryResource.doPost(
-            new ByteArrayInputStream("Meka Leka Hi Meka Hiney Ho".getBytes(StandardCharsets.UTF_8)),
+            new ByteArrayInputStream("Meka Leka Hi Meka Hiney Ho".getBytes(UTF_8)),
             null /*pretty*/,
             testServletRequest);
     Assert.assertNotNull(response);
@@ -545,7 +542,7 @@ public class QueryResourceTest {
 
     try {
       queryResource.doPost(
-          new ByteArrayInputStream(SIMPLE_TIMESERIES_QUERY.getBytes(StandardCharsets.UTF_8)),
+          new ByteArrayInputStream(SIMPLE_TIMESERIES_QUERY.getBytes(UTF_8)),
           null /*pretty*/,
           testServletRequest.mimic());
       Assert.fail("doPost did not throw ForbiddenException for an unauthorized query");
@@ -612,9 +609,7 @@ public class QueryResourceTest {
 
     final Response response =
         expectSynchronousRequestFlow(
-            testServletRequest,
-            SIMPLE_TIMESERIES_QUERY.getBytes(StandardCharsets.UTF_8),
-            timeoutQueryResource);
+            testServletRequest, SIMPLE_TIMESERIES_QUERY.getBytes(UTF_8), timeoutQueryResource);
     Assert.assertEquals(QueryTimeoutException.STATUS_CODE, response.getStatus());
 
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
@@ -648,7 +643,7 @@ public class QueryResourceTest {
                   AuthenticationResult authenticationResult, Resource resource, Action action) {
                 // READ action corresponds to the query
                 // WRITE corresponds to cancellation of query
-                if (action.equals(Action.READ)) {
+                if (action == Action.READ) {
                   try {
                     // Countdown startAwaitLatch as we want query cancellation to happen
                     // after we enter isAuthorized method so that we can handle the
@@ -706,7 +701,7 @@ public class QueryResourceTest {
                   try {
                     responseFromEndpoint.set(
                         queryResource.doPost(
-                            new ByteArrayInputStream(queryString.getBytes(StandardCharsets.UTF_8)),
+                            new ByteArrayInputStream(queryString.getBytes(UTF_8)),
                             null,
                             testServletRequest));
                     return null;
@@ -754,7 +749,7 @@ public class QueryResourceTest {
                   AuthenticationResult authenticationResult, Resource resource, Action action) {
                 // READ action corresponds to the query
                 // WRITE corresponds to cancellation of query
-                if (action.equals(Action.READ)) {
+                if (action == Action.READ) {
                   try {
                     waitForCancellationLatch.await();
                   } catch (InterruptedException e) {
@@ -805,9 +800,7 @@ public class QueryResourceTest {
                     final MockHttpServletResponse retVal =
                         MockHttpServletResponse.forRequest(localRequest);
                     queryResource.doPost(
-                        new ByteArrayInputStream(queryString.getBytes(StandardCharsets.UTF_8)),
-                        null,
-                        localRequest);
+                        new ByteArrayInputStream(queryString.getBytes(UTF_8)), null, localRequest);
                     return retVal;
                   } catch (IOException e) {
                     throw new RuntimeException(e);
@@ -847,8 +840,7 @@ public class QueryResourceTest {
             NoQueryLaningStrategy.INSTANCE,
             new ServerConfig());
 
-    createScheduledQueryResource(
-        laningScheduler, Collections.emptyList(), ImmutableList.of(waitTwoScheduled));
+    createScheduledQueryResource(laningScheduler, emptyList(), ImmutableList.of(waitTwoScheduled));
     assertAsyncResponseAndCountdownOrBlockForever(
         SIMPLE_TIMESERIES_QUERY,
         waitAllFinished,
@@ -1058,7 +1050,7 @@ public class QueryResourceTest {
   @Nonnull
   private MockHttpServletResponse expectAsyncRequestFlow(String query, MockHttpServletRequest req)
       throws IOException {
-    return expectAsyncRequestFlow(req, query.getBytes(StandardCharsets.UTF_8));
+    return expectAsyncRequestFlow(req, query.getBytes(UTF_8));
   }
 
   @Nonnull
@@ -1086,9 +1078,7 @@ public class QueryResourceTest {
               try {
                 asserts.accept(
                     expectSynchronousRequestFlow(
-                        testServletRequest.mimic(),
-                        query.getBytes(StandardCharsets.UTF_8),
-                        queryResource));
+                        testServletRequest.mimic(), query.getBytes(UTF_8), queryResource));
               } catch (IOException e) {
                 throw new RuntimeException(e);
               }
diff --git a/server/src/test/java/org/apache/druid/server/QuerySchedulerTest.java b/server/src/test/java/org/apache/druid/server/QuerySchedulerTest.java
index 7aae7553d9..983298bf86 100644
--- a/server/src/test/java/org/apache/druid/server/QuerySchedulerTest.java
+++ b/server/src/test/java/org/apache/druid/server/QuerySchedulerTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server;
 
+import static java.util.UUID.randomUUID;
+
 import com.fasterxml.jackson.databind.InjectableValues;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
@@ -35,7 +37,6 @@ import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Properties;
-import java.util.UUID;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Future;
 import java.util.concurrent.ThreadLocalRandom;
@@ -481,20 +482,18 @@ public class QuerySchedulerTest {
   }
 
   private TopNQuery makeDefaultQuery() {
-    return makeBaseBuilder()
-        .context(ImmutableMap.of("queryId", "default-" + UUID.randomUUID()))
-        .build();
+    return makeBaseBuilder().context(ImmutableMap.of("queryId", "default-" + randomUUID())).build();
   }
 
   private TopNQuery makeInteractiveQuery() {
     return makeBaseBuilder()
-        .context(ImmutableMap.of("priority", 10, "queryId", "high-" + UUID.randomUUID()))
+        .context(ImmutableMap.of("priority", 10, "queryId", "high-" + randomUUID()))
         .build();
   }
 
   private TopNQuery makeReportQuery() {
     return makeBaseBuilder()
-        .context(ImmutableMap.of("priority", -1, "queryId", "low-" + UUID.randomUUID()))
+        .context(ImmutableMap.of("priority", -1, "queryId", "low-" + randomUUID()))
         .build();
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/QueryStackTests.java b/server/src/test/java/org/apache/druid/server/QueryStackTests.java
index 8645fa2df9..8781dd8c44 100644
--- a/server/src/test/java/org/apache/druid/server/QueryStackTests.java
+++ b/server/src/test/java/org/apache/druid/server/QueryStackTests.java
@@ -255,48 +255,44 @@ public class QueryStackTests {
             groupByBuffers,
             processingConfig);
 
-    final QueryRunnerFactoryConglomerate conglomerate =
-        new DefaultQueryRunnerFactoryConglomerate(
-            ImmutableMap.<Class<? extends Query>, QueryRunnerFactory>builder()
-                .put(
-                    SegmentMetadataQuery.class,
-                    new SegmentMetadataQueryRunnerFactory(
-                        new SegmentMetadataQueryQueryToolChest(
-                            new SegmentMetadataQueryConfig("P1W")),
-                        QueryRunnerTestHelper.NOOP_QUERYWATCHER))
-                .put(
-                    ScanQuery.class,
-                    new ScanQueryRunnerFactory(
-                        new ScanQueryQueryToolChest(
-                            new ScanQueryConfig(), new DefaultGenericQueryMetricsFactory()),
-                        new ScanQueryEngine(),
-                        new ScanQueryConfig()))
-                .put(
-                    TimeseriesQuery.class,
-                    new TimeseriesQueryRunnerFactory(
-                        new TimeseriesQueryQueryToolChest(),
-                        new TimeseriesQueryEngine(),
-                        QueryRunnerTestHelper.NOOP_QUERYWATCHER))
-                .put(
-                    TopNQuery.class,
-                    new TopNQueryRunnerFactory(
-                        testBufferPool,
-                        new TopNQueryQueryToolChest(
-                            new TopNQueryConfig() {
-                              @Override
-                              public int getMinTopNThreshold() {
-                                return minTopNThresholdSupplier.get();
-                              }
-                            }),
-                        QueryRunnerTestHelper.NOOP_QUERYWATCHER))
-                .put(GroupByQuery.class, groupByQueryRunnerFactory)
-                .put(
-                    TimeBoundaryQuery.class,
-                    new TimeBoundaryQueryRunnerFactory(QueryRunnerTestHelper.NOOP_QUERYWATCHER))
-                .put(WindowOperatorQuery.class, new WindowOperatorQueryQueryRunnerFactory())
-                .build());
-
-    return conglomerate;
+    return new DefaultQueryRunnerFactoryConglomerate(
+        ImmutableMap.<Class<? extends Query>, QueryRunnerFactory>builder()
+            .put(
+                SegmentMetadataQuery.class,
+                new SegmentMetadataQueryRunnerFactory(
+                    new SegmentMetadataQueryQueryToolChest(new SegmentMetadataQueryConfig("P1W")),
+                    QueryRunnerTestHelper.NOOP_QUERYWATCHER))
+            .put(
+                ScanQuery.class,
+                new ScanQueryRunnerFactory(
+                    new ScanQueryQueryToolChest(
+                        new ScanQueryConfig(), new DefaultGenericQueryMetricsFactory()),
+                    new ScanQueryEngine(),
+                    new ScanQueryConfig()))
+            .put(
+                TimeseriesQuery.class,
+                new TimeseriesQueryRunnerFactory(
+                    new TimeseriesQueryQueryToolChest(),
+                    new TimeseriesQueryEngine(),
+                    QueryRunnerTestHelper.NOOP_QUERYWATCHER))
+            .put(
+                TopNQuery.class,
+                new TopNQueryRunnerFactory(
+                    testBufferPool,
+                    new TopNQueryQueryToolChest(
+                        new TopNQueryConfig() {
+                          @Override
+                          public int getMinTopNThreshold() {
+                            return minTopNThresholdSupplier.get();
+                          }
+                        }),
+                    QueryRunnerTestHelper.NOOP_QUERYWATCHER))
+            .put(GroupByQuery.class, groupByQueryRunnerFactory)
+            .put(
+                TimeBoundaryQuery.class,
+                new TimeBoundaryQueryRunnerFactory(QueryRunnerTestHelper.NOOP_QUERYWATCHER))
+            .put(WindowOperatorQuery.class, new WindowOperatorQueryQueryRunnerFactory())
+            .build());
   }
 
   public static JoinableFactory makeJoinableFactoryForLookup(
diff --git a/server/src/test/java/org/apache/druid/server/RendezvousHasherTest.java b/server/src/test/java/org/apache/druid/server/RendezvousHasherTest.java
index 7eb4173973..cf0c0ba7ef 100644
--- a/server/src/test/java/org/apache/druid/server/RendezvousHasherTest.java
+++ b/server/src/test/java/org/apache/druid/server/RendezvousHasherTest.java
@@ -19,13 +19,15 @@
 
 package org.apache.druid.server;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Objects.requireNonNull;
+import static java.util.UUID.randomUUID;
+
 import com.google.common.collect.ImmutableMap;
 import java.io.IOException;
-import java.nio.charset.StandardCharsets;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 import java.util.UUID;
 import org.apache.commons.io.IOUtils;
@@ -53,7 +55,7 @@ public class RendezvousHasherTest {
     Map<String, String> uuidServerMap = new HashMap<>();
 
     for (int i = 0; i < NUM_ITERATIONS; i++) {
-      UUID objectId = UUID.randomUUID();
+      UUID objectId = randomUUID();
       String targetServer = hasher.chooseNode(nodes, StringUtils.toUtf8(objectId.toString()));
       uuidServerMap.put(objectId.toString(), targetServer);
     }
@@ -80,7 +82,7 @@ public class RendezvousHasherTest {
     Map<String, String> uuidServerMap = new HashMap<>();
 
     for (int i = 0; i < NUM_ITERATIONS; i++) {
-      UUID objectId = UUID.randomUUID();
+      UUID objectId = randomUUID();
       String targetServer = hasher.chooseNode(nodes, StringUtils.toUtf8(objectId.toString()));
       uuidServerMap.put(objectId.toString(), targetServer);
     }
@@ -118,7 +120,7 @@ public class RendezvousHasherTest {
     Map<String, String> uuidServerMap = new HashMap<>();
 
     for (int i = 0; i < NUM_ITERATIONS; i++) {
-      UUID objectId = UUID.randomUUID();
+      UUID objectId = randomUUID();
       String targetServer = hasher.chooseNode(nodes, StringUtils.toUtf8(objectId.toString()));
       uuidServerMap.put(objectId.toString(), targetServer);
     }
@@ -212,7 +214,7 @@ public class RendezvousHasherTest {
     RendezvousHasher hasher = new RendezvousHasher();
     Map<String, String> uuidServerMap = new HashMap<>();
     for (int i = 0; i < NUM_ITERATIONS; i++) {
-      UUID objectId = UUID.randomUUID();
+      UUID objectId = randomUUID();
       String targetServer = hasher.chooseNode(nodes, StringUtils.toUtf8(objectId.toString()));
       uuidServerMap.put(objectId.toString(), targetServer);
     }
@@ -248,9 +250,9 @@ public class RendezvousHasherTest {
   public void testDistribution() throws IOException {
     String[] uuids =
         IOUtils.toString(
-                Objects.requireNonNull(
+                requireNonNull(
                     this.getClass().getClassLoader().getResourceAsStream("random_uuid_100")),
-                StandardCharsets.UTF_8)
+                UTF_8)
             .split("\n");
     RendezvousHasher hasher = new RendezvousHasher();
     Set<String> nodes = new HashSet<>();
@@ -261,7 +263,7 @@ public class RendezvousHasherTest {
     Map<String, Integer> brokerToConnectionCount = new HashMap<>();
     for (String uuid : uuids) {
       brokerToConnectionCount.merge(
-          hasher.chooseNode(nodes, uuid.getBytes(StandardCharsets.UTF_8)), 1, Integer::sum);
+          hasher.chooseNode(nodes, uuid.getBytes(UTF_8)), 1, Integer::sum);
     }
     Map<String, Integer> expectedDistribution =
         ImmutableMap.<String, Integer>builder()
diff --git a/server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java b/server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java
index 1d2e868b97..ed20519741 100644
--- a/server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java
+++ b/server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.InjectableValues;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.jsontype.NamedType;
@@ -28,7 +32,6 @@ import com.google.common.collect.ImmutableSet;
 import java.io.File;
 import java.io.IOException;
 import java.nio.ByteBuffer;
-import java.util.Collections;
 import java.util.List;
 import java.util.Optional;
 import java.util.Set;
@@ -128,8 +131,7 @@ public class SegmentManagerBroadcastJoinIndexedTableTest extends InitializedNull
             new SegmentLoaderConfig() {
               @Override
               public List<StorageLocationConfig> getLocations() {
-                return Collections.singletonList(
-                    new StorageLocationConfig(segmentCacheDir, null, null));
+                return singletonList(new StorageLocationConfig(segmentCacheDir, null, null));
               }
             },
             objectMapper);
@@ -298,9 +300,9 @@ public class SegmentManagerBroadcastJoinIndexedTableTest extends InitializedNull
             TABLE_NAME,
             Intervals.of(interval),
             version,
-            Collections.emptyMap(),
-            Collections.emptyList(),
-            Collections.emptyList(),
+            emptyMap(),
+            emptyList(),
+            emptyList(),
             new NumberedShardSpec(0, 0),
             9,
             100);
diff --git a/server/src/test/java/org/apache/druid/server/SegmentManagerTest.java b/server/src/test/java/org/apache/druid/server/SegmentManagerTest.java
index 043d20365c..4e8ef60e56 100644
--- a/server/src/test/java/org/apache/druid/server/SegmentManagerTest.java
+++ b/server/src/test/java/org/apache/druid/server/SegmentManagerTest.java
@@ -19,6 +19,12 @@
 
 package org.apache.druid.server;
 
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toMap;
+import static java.util.stream.Collectors.toSet;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Ordering;
@@ -31,7 +37,6 @@ import java.util.Set;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Future;
-import java.util.stream.Collectors;
 import org.apache.druid.java.util.common.Intervals;
 import org.apache.druid.java.util.common.MapUtils;
 import org.apache.druid.java.util.common.concurrent.Execs;
@@ -55,7 +60,6 @@ import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
-import org.mockito.Mockito;
 
 public class SegmentManagerTest {
 
@@ -86,8 +90,8 @@ public class SegmentManagerTest {
     SegmentForTesting(String version, Interval interval) {
       this.version = version;
       this.interval = interval;
-      storageAdapter = Mockito.mock(StorageAdapter.class);
-      Mockito.when(storageAdapter.getNumRows()).thenReturn(1);
+      storageAdapter = mock(StorageAdapter.class);
+      when(storageAdapter.getNumRows()).thenReturn(1);
     }
 
     public String getVersion() {
@@ -201,7 +205,7 @@ public class SegmentManagerTest {
                         () ->
                             segmentManager.loadSegment(
                                 segment, false, SegmentLazyLoadFailCallback.NOOP)))
-            .collect(Collectors.toList());
+            .collect(toList());
 
     for (Future<Boolean> eachFuture : futures) {
       Assert.assertTrue(eachFuture.get());
@@ -227,7 +231,7 @@ public class SegmentManagerTest {
                           segmentManager.dropSegment(segment);
                           return (Void) null;
                         }))
-            .collect(Collectors.toList());
+            .collect(toList());
 
     for (Future<Void> eachFuture : futures) {
       eachFuture.get();
@@ -252,7 +256,7 @@ public class SegmentManagerTest {
                         () ->
                             segmentManager.loadSegment(
                                 segment, false, SegmentLazyLoadFailCallback.NOOP)))
-            .collect(Collectors.toList());
+            .collect(toList());
     final List<Future<Void>> dropFutures =
         ImmutableList.of(SEGMENTS.get(0), SEGMENTS.get(2)).stream()
             .map(
@@ -262,7 +266,7 @@ public class SegmentManagerTest {
                           segmentManager.dropSegment(segment);
                           return (Void) null;
                         }))
-            .collect(Collectors.toList());
+            .collect(toList());
 
     for (Future<Boolean> eachFuture : loadFutures) {
       Assert.assertTrue(eachFuture.get());
@@ -298,7 +302,7 @@ public class SegmentManagerTest {
                         () ->
                             segmentManager.loadSegment(
                                 segment, false, SegmentLazyLoadFailCallback.NOOP)))
-            .collect(Collectors.toList());
+            .collect(toList());
 
     int numSucceededFutures = 0;
     int numFailedFutures = 0;
@@ -336,7 +340,7 @@ public class SegmentManagerTest {
                           segmentManager.dropSegment(segment);
                           return (Void) null;
                         }))
-            .collect(Collectors.toList());
+            .collect(toList());
 
     for (Future<Void> future : futures) {
       future.get();
@@ -388,14 +392,12 @@ public class SegmentManagerTest {
       throws SegmentLoadingException {
     final Map<String, Long> expectedDataSourceSizes =
         expectedExistingSegments.stream()
-            .collect(Collectors.toMap(DataSegment::getDataSource, DataSegment::getSize, Long::sum));
+            .collect(toMap(DataSegment::getDataSource, DataSegment::getSize, Long::sum));
     final Map<String, Long> expectedDataSourceCounts =
         expectedExistingSegments.stream()
-            .collect(Collectors.toMap(DataSegment::getDataSource, segment -> 1L, Long::sum));
+            .collect(toMap(DataSegment::getDataSource, segment -> 1L, Long::sum));
     final Set<String> expectedDataSourceNames =
-        expectedExistingSegments.stream()
-            .map(DataSegment::getDataSource)
-            .collect(Collectors.toSet());
+        expectedExistingSegments.stream().map(DataSegment::getDataSource).collect(toSet());
     final Map<String, VersionedIntervalTimeline<String, ReferenceCountingSegment>>
         expectedTimelines = new HashMap<>();
     for (DataSegment segment : expectedExistingSegments) {
diff --git a/server/src/test/java/org/apache/druid/server/SegmentManagerThreadSafetyTest.java b/server/src/test/java/org/apache/druid/server/SegmentManagerThreadSafetyTest.java
index 802f388339..6c1122334d 100644
--- a/server/src/test/java/org/apache/druid/server/SegmentManagerThreadSafetyTest.java
+++ b/server/src/test/java/org/apache/druid/server/SegmentManagerThreadSafetyTest.java
@@ -19,6 +19,13 @@
 
 package org.apache.druid.server;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
 import com.fasterxml.jackson.databind.InjectableValues.Std;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.jsontype.NamedType;
@@ -27,14 +34,12 @@ import com.google.common.collect.ImmutableMap;
 import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Future;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import javax.annotation.Nullable;
 import org.apache.druid.jackson.DefaultObjectMapper;
@@ -69,7 +74,6 @@ import org.junit.Before;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
-import org.mockito.Mockito;
 
 public class SegmentManagerThreadSafetyTest {
   private static final int NUM_THREAD = 4;
@@ -104,8 +108,7 @@ public class SegmentManagerThreadSafetyTest {
             new SegmentLoaderConfig() {
               @Override
               public List<StorageLocationConfig> getLocations() {
-                return Collections.singletonList(
-                    new StorageLocationConfig(segmentCacheDir, null, null));
+                return singletonList(new StorageLocationConfig(segmentCacheDir, null, null));
               }
             },
             objectMapper);
@@ -132,7 +135,7 @@ public class SegmentManagerThreadSafetyTest {
                         () ->
                             segmentManager.loadSegment(
                                 segment, false, SegmentLazyLoadFailCallback.NOOP)))
-            .collect(Collectors.toList());
+            .collect(toList());
     for (Future future : futures) {
       future.get();
     }
@@ -166,7 +169,7 @@ public class SegmentManagerThreadSafetyTest {
                             }
                           }
                         }))
-            .collect(Collectors.toList());
+            .collect(toList());
     for (Future future : futures) {
       future.get();
     }
@@ -181,9 +184,9 @@ public class SegmentManagerThreadSafetyTest {
             "dataSource",
             Intervals.of(interval),
             "version",
-            Collections.emptyMap(),
-            Collections.emptyList(),
-            Collections.emptyList(),
+            emptyMap(),
+            emptyList(),
+            emptyList(),
             new NumberedShardSpec(0, 0),
             9,
             100);
@@ -225,7 +228,7 @@ public class SegmentManagerThreadSafetyTest {
         boolean lazy,
         SegmentLazyLoadFailCallback SegmentLazyLoadFailCallback) {
       return new Segment() {
-        StorageAdapter storageAdapter = Mockito.mock(StorageAdapter.class);
+        StorageAdapter storageAdapter = mock(StorageAdapter.class);
 
         @Override
         public SegmentId getId() {
@@ -245,7 +248,7 @@ public class SegmentManagerThreadSafetyTest {
 
         @Override
         public StorageAdapter asStorageAdapter() {
-          Mockito.when(storageAdapter.getNumRows()).thenReturn(1);
+          when(storageAdapter.getNumRows()).thenReturn(1);
           return storageAdapter;
         }
 
diff --git a/server/src/test/java/org/apache/druid/server/StatusResourceTest.java b/server/src/test/java/org/apache/druid/server/StatusResourceTest.java
index 7de5167d01..bf3eb48f54 100644
--- a/server/src/test/java/org/apache/druid/server/StatusResourceTest.java
+++ b/server/src/test/java/org/apache/druid/server/StatusResourceTest.java
@@ -19,17 +19,18 @@
 
 package org.apache.druid.server;
 
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toSet;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Injector;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.guice.PropertiesModule;
 import org.apache.druid.guice.StartupInjectorBuilder;
 import org.apache.druid.initialization.DruidModule;
@@ -76,15 +77,11 @@ public class StatusResourceTest {
 
   private void testHiddenPropertiesWithPropertyFileName(String fileName) throws Exception {
     Injector injector =
-        new StartupInjectorBuilder()
-            .add(new PropertiesModule(Collections.singletonList(fileName)))
-            .build();
+        new StartupInjectorBuilder().add(new PropertiesModule(singletonList(fileName))).build();
     Map<String, String> returnedProperties =
         injector.getInstance(StatusResource.class).getProperties();
     Set<String> lowerCasePropertyNames =
-        returnedProperties.keySet().stream()
-            .map(StringUtils::toLowerCase)
-            .collect(Collectors.toSet());
+        returnedProperties.keySet().stream().map(StringUtils::toLowerCase).collect(toSet());
 
     Assert.assertTrue(
         "The list of unfiltered Properties is not > the list of filtered Properties?!?",
diff --git a/server/src/test/java/org/apache/druid/server/TestClusterQuerySegmentWalker.java b/server/src/test/java/org/apache/druid/server/TestClusterQuerySegmentWalker.java
index b7e85fa5e4..44ef74e6e5 100644
--- a/server/src/test/java/org/apache/druid/server/TestClusterQuerySegmentWalker.java
+++ b/server/src/test/java/org/apache/druid/server/TestClusterQuerySegmentWalker.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server;
 
-import com.google.common.base.Preconditions;
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Collections.emptyList;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
@@ -217,7 +218,7 @@ public class TestClusterQuerySegmentWalker implements QuerySegmentWalker {
         timelines.get(dataSource);
 
     if (timeline == null) {
-      return Collections.emptyList();
+      return emptyList();
     } else {
       final List<WindowedSegment> retVal = new ArrayList<>();
 
@@ -238,7 +239,7 @@ public class TestClusterQuerySegmentWalker implements QuerySegmentWalker {
         timelines.get(dataSource);
 
     if (timeline == null) {
-      return Collections.emptyList();
+      return emptyList();
     } else {
       final List<WindowedSegment> retVal = new ArrayList<>();
 
@@ -259,7 +260,7 @@ public class TestClusterQuerySegmentWalker implements QuerySegmentWalker {
     public WindowedSegment(ReferenceCountingSegment segment, Interval interval) {
       this.segment = segment;
       this.interval = interval;
-      Preconditions.checkArgument(segment.getId().getInterval().contains(interval));
+      checkArgument(segment.getId().getInterval().contains(interval));
     }
 
     public ReferenceCountingSegment getSegment() {
diff --git a/server/src/test/java/org/apache/druid/server/WebserverTestUtils.java b/server/src/test/java/org/apache/druid/server/WebserverTestUtils.java
index d4b86d4e08..6541a5373a 100644
--- a/server/src/test/java/org/apache/druid/server/WebserverTestUtils.java
+++ b/server/src/test/java/org/apache/druid/server/WebserverTestUtils.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server;
 
+import static java.util.Collections.emptyMap;
+
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Binder;
 import com.google.inject.Injector;
@@ -34,7 +36,6 @@ import com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory;
 import com.sun.jersey.spi.inject.SingletonTypeInjectableProvider;
 import java.io.IOException;
 import java.net.URI;
-import java.util.Collections;
 import java.util.Map;
 import java.util.concurrent.ThreadLocalRandom;
 import java.util.function.Consumer;
@@ -117,7 +118,7 @@ public class WebserverTestUtils {
     static HttpServletRequest createMockRequest() {
       HttpServletRequest mockRequest = EasyMock.createNiceMock(HttpServletRequest.class);
       AuthenticationResult authenticationResult =
-          new AuthenticationResult("druid", "druid", null, Collections.emptyMap());
+          new AuthenticationResult("druid", "druid", null, emptyMap());
 
       EasyMock.expect(mockRequest.getAttribute(AuthConfig.DRUID_AUTHORIZATION_CHECKED))
           .andReturn(null)
diff --git a/server/src/test/java/org/apache/druid/server/audit/SQLAuditManagerTest.java b/server/src/test/java/org/apache/druid/server/audit/SQLAuditManagerTest.java
index 6b77db10f2..013b081dc3 100644
--- a/server/src/test/java/org/apache/druid/server/audit/SQLAuditManagerTest.java
+++ b/server/src/test/java/org/apache/druid/server/audit/SQLAuditManagerTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server.audit;
 
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.io.IOException;
@@ -43,8 +46,6 @@ import org.junit.Before;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.runner.RunWith;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 import org.skife.jdbi.v2.Handle;
 import org.skife.jdbi.v2.tweak.HandleCallback;
@@ -405,7 +406,7 @@ public class SQLAuditManagerTest {
 
   @Test(timeout = 60_000L)
   public void testCreateAuditEntryWithSkipNullConfigTrue() {
-    ConfigSerde<Map<String, String>> mockConfigSerde = Mockito.mock(ConfigSerde.class);
+    ConfigSerde<Map<String, String>> mockConfigSerde = mock(ConfigSerde.class);
     SQLAuditManager auditManagerWithSkipNull =
         new SQLAuditManager(
             connector,
@@ -429,8 +430,7 @@ public class SQLAuditManagerTest {
 
     auditManagerWithSkipNull.doAudit(
         entry1Key, entry1Type, entry1AuditInfo, entryPayload1WithNull, mockConfigSerde);
-    Mockito.verify(mockConfigSerde)
-        .serializeToString(ArgumentMatchers.eq(entryPayload1WithNull), ArgumentMatchers.eq(true));
+    verify(mockConfigSerde).serializeToString(entryPayload1WithNull, true);
   }
 
   @After
diff --git a/server/src/test/java/org/apache/druid/server/coordination/ChangeRequestHttpSyncerTest.java b/server/src/test/java/org/apache/druid/server/coordination/ChangeRequestHttpSyncerTest.java
index cf494160b4..10b4653283 100644
--- a/server/src/test/java/org/apache/druid/server/coordination/ChangeRequestHttpSyncerTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordination/ChangeRequestHttpSyncerTest.java
@@ -129,7 +129,7 @@ public class ChangeRequestHttpSyncerTest {
 
     syncer.start();
 
-    while (httpClient.results.size() != 0) {
+    while (!httpClient.results.isEmpty()) {
       Thread.sleep(100);
     }
 
diff --git a/server/src/test/java/org/apache/druid/server/coordination/SegmentLoadDropHandlerCacheTest.java b/server/src/test/java/org/apache/druid/server/coordination/SegmentLoadDropHandlerCacheTest.java
index 972d544c7c..da7eb0ae01 100644
--- a/server/src/test/java/org/apache/druid/server/coordination/SegmentLoadDropHandlerCacheTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordination/SegmentLoadDropHandlerCacheTest.java
@@ -19,7 +19,14 @@
 
 package org.apache.druid.server.coordination;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.singletonList;
+import static java.util.Comparator.comparing;
 import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.reset;
+import static org.mockito.Mockito.verify;
 
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
@@ -29,11 +36,8 @@ import com.google.common.collect.ImmutableMap;
 import com.google.common.io.Files;
 import java.io.File;
 import java.io.IOException;
-import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
 import java.util.List;
 import java.util.concurrent.ThreadLocalRandom;
 import org.apache.druid.guice.ServerTypeConfig;
@@ -63,7 +67,6 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
 import org.mockito.ArgumentCaptor;
-import org.mockito.Mockito;
 
 /** This class includes tests that cover the storage location layer as well. */
 public class SegmentLoadDropHandlerCacheTest {
@@ -80,8 +83,7 @@ public class SegmentLoadDropHandlerCacheTest {
     storageLoc = new TestStorageLocation(temporaryFolder);
     SegmentLoaderConfig config =
         new SegmentLoaderConfig()
-            .withLocations(
-                Collections.singletonList(storageLoc.toStorageLocationConfig(MAX_SIZE, null)))
+            .withLocations(singletonList(storageLoc.toStorageLocationConfig(MAX_SIZE, null)))
             .withInfoDir(storageLoc.getInfoDir());
     objectMapper = TestHelper.makeJsonMapper();
     objectMapper.registerSubtypes(TestLoadSpec.class);
@@ -90,13 +92,13 @@ public class SegmentLoadDropHandlerCacheTest {
     SegmentManager segmentManager =
         new SegmentManager(
             new SegmentLocalCacheLoader(cacheManager, TestIndex.INDEX_IO, objectMapper));
-    segmentAnnouncer = Mockito.mock(DataSegmentAnnouncer.class);
+    segmentAnnouncer = mock(DataSegmentAnnouncer.class);
     loadDropHandler =
         new SegmentLoadDropHandler(
             objectMapper,
             config,
             segmentAnnouncer,
-            Mockito.mock(DataSegmentServerAnnouncer.class),
+            mock(DataSegmentServerAnnouncer.class),
             segmentManager,
             cacheManager,
             new ServerTypeConfig(ServerType.HISTORICAL));
@@ -125,23 +127,23 @@ public class SegmentLoadDropHandlerCacheTest {
 
     // Verify the expected announcements
     ArgumentCaptor<Iterable<DataSegment>> argCaptor = ArgumentCaptor.forClass(Iterable.class);
-    Mockito.verify(segmentAnnouncer).announceSegments(argCaptor.capture());
+    verify(segmentAnnouncer).announceSegments(argCaptor.capture());
     List<DataSegment> announcedSegments = new ArrayList<>();
     argCaptor.getValue().forEach(announcedSegments::add);
-    announcedSegments.sort(Comparator.comparing(DataSegment::getVersion));
+    announcedSegments.sort(comparing(DataSegment::getVersion));
     Assert.assertEquals(expectedSegments, announcedSegments);
 
     // make sure adding segments beyond allowed size fails
-    Mockito.reset(segmentAnnouncer);
+    reset(segmentAnnouncer);
     DataSegment newSegment = makeSegment("test", "new-segment");
     loadDropHandler.addSegment(newSegment, null);
-    Mockito.verify(segmentAnnouncer, Mockito.never()).announceSegment(any());
-    Mockito.verify(segmentAnnouncer, Mockito.never()).announceSegments(any());
+    verify(segmentAnnouncer, never()).announceSegment(any());
+    verify(segmentAnnouncer, never()).announceSegments(any());
 
     // clearing some segment should allow for new segments
     loadDropHandler.removeSegment(expectedSegments.get(0), null, false);
     loadDropHandler.addSegment(newSegment, null);
-    Mockito.verify(segmentAnnouncer).announceSegment(newSegment);
+    verify(segmentAnnouncer).announceSegment(newSegment);
   }
 
   private DataSegment makeSegment(String dataSource, String name) {
@@ -186,8 +188,7 @@ public class SegmentLoadDropHandlerCacheTest {
         byte[] bytes = new byte[size];
         ThreadLocalRandom.current().nextBytes(bytes);
         Files.write(bytes, segmentFile);
-        Files.write(
-            "{\"type\":\"testSegmentFactory\"}".getBytes(StandardCharsets.UTF_8), factoryJson);
+        Files.write("{\"type\":\"testSegmentFactory\"}".getBytes(UTF_8), factoryJson);
       } catch (IOException e) {
         throw new SegmentLoadingException(
             e, "Failed to write data in directory %s", destDir.getAbsolutePath());
@@ -202,7 +203,7 @@ public class SegmentLoadDropHandlerCacheTest {
     @Override
     public Segment factorize(
         DataSegment segment, File parentDir, boolean lazy, SegmentLazyLoadFailCallback loadFailed) {
-      return Mockito.mock(Segment.class);
+      return mock(Segment.class);
     }
   }
 }
diff --git a/server/src/test/java/org/apache/druid/server/coordination/SegmentLoadDropHandlerTest.java b/server/src/test/java/org/apache/druid/server/coordination/SegmentLoadDropHandlerTest.java
index 918d8e38f7..d826daaf2f 100644
--- a/server/src/test/java/org/apache/druid/server/coordination/SegmentLoadDropHandlerTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordination/SegmentLoadDropHandlerTest.java
@@ -19,6 +19,15 @@
 
 package org.apache.druid.server.coordination;
 
+import static java.util.Collections.singletonList;
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyBoolean;
+import static org.mockito.Mockito.doNothing;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
@@ -28,7 +37,6 @@ import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -66,8 +74,6 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.rules.TemporaryFolder;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
 
 /** */
 public class SegmentLoadDropHandlerTest {
@@ -108,7 +114,7 @@ public class SegmentLoadDropHandlerTest {
       throw new RuntimeException(e);
     }
 
-    locations = Collections.singletonList(testStorageLocation.toStorageLocationConfig());
+    locations = singletonList(testStorageLocation.toStorageLocationConfig());
 
     scheduledRunnable = new ArrayList<>();
 
@@ -242,7 +248,7 @@ public class SegmentLoadDropHandlerTest {
             jsonMapper,
             segmentLoaderConfig,
             announcer,
-            Mockito.mock(DataSegmentServerAnnouncer.class),
+            mock(DataSegmentServerAnnouncer.class),
             segmentManager,
             segmentCacheManager,
             scheduledExecutorFactory.create(5, "SegmentLoadDropHandlerTest-[%d]"),
@@ -402,7 +408,7 @@ public class SegmentLoadDropHandlerTest {
               }
             },
             announcer,
-            Mockito.mock(DataSegmentServerAnnouncer.class),
+            mock(DataSegmentServerAnnouncer.class),
             segmentManager,
             segmentCacheManager,
             new ServerTypeConfig(ServerType.HISTORICAL));
@@ -480,11 +486,10 @@ public class SegmentLoadDropHandlerTest {
   @Test(timeout = 60_000L)
   public void testProcessBatchDuplicateLoadRequestsWhenFirstRequestFailsSecondRequestShouldSucceed()
       throws Exception {
-    final SegmentManager segmentManager = Mockito.mock(SegmentManager.class);
-    Mockito.when(
-            segmentManager.loadSegment(
-                ArgumentMatchers.any(), ArgumentMatchers.anyBoolean(),
-                ArgumentMatchers.any(), ArgumentMatchers.any()))
+    final SegmentManager segmentManager = mock(SegmentManager.class);
+    when(segmentManager.loadSegment(
+            any(), anyBoolean(),
+            any(), any()))
         .thenThrow(new RuntimeException("segment loading failure test"))
         .thenReturn(true);
     final SegmentLoadDropHandler segmentLoadDropHandler =
@@ -492,7 +497,7 @@ public class SegmentLoadDropHandlerTest {
             jsonMapper,
             segmentLoaderConfig,
             announcer,
-            Mockito.mock(DataSegmentServerAnnouncer.class),
+            mock(DataSegmentServerAnnouncer.class),
             segmentManager,
             segmentCacheManager,
             scheduledExecutorFactory.create(5, "SegmentLoadDropHandlerTest-[%d]"),
@@ -525,21 +530,15 @@ public class SegmentLoadDropHandlerTest {
 
   @Test(timeout = 60_000L)
   public void testProcessBatchLoadDropLoadSequenceForSameSegment() throws Exception {
-    final SegmentManager segmentManager = Mockito.mock(SegmentManager.class);
-    Mockito.when(
-            segmentManager.loadSegment(
-                ArgumentMatchers.any(),
-                ArgumentMatchers.anyBoolean(),
-                ArgumentMatchers.any(),
-                ArgumentMatchers.any()))
-        .thenReturn(true);
-    Mockito.doNothing().when(segmentManager).dropSegment(ArgumentMatchers.any());
+    final SegmentManager segmentManager = mock(SegmentManager.class);
+    when(segmentManager.loadSegment(any(), anyBoolean(), any(), any())).thenReturn(true);
+    doNothing().when(segmentManager).dropSegment(any());
     final SegmentLoadDropHandler segmentLoadDropHandler =
         new SegmentLoadDropHandler(
             jsonMapper,
             noAnnouncerSegmentLoaderConfig,
             announcer,
-            Mockito.mock(DataSegmentServerAnnouncer.class),
+            mock(DataSegmentServerAnnouncer.class),
             segmentManager,
             segmentCacheManager,
             scheduledExecutorFactory.create(5, "SegmentLoadDropHandlerTest-[%d]"),
@@ -572,13 +571,8 @@ public class SegmentLoadDropHandlerTest {
     scheduledRunnable.clear();
 
     // check invocations after a load-drop sequence
-    Mockito.verify(segmentManager, Mockito.times(1))
-        .loadSegment(
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyBoolean(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
-    Mockito.verify(segmentManager, Mockito.times(1)).dropSegment(ArgumentMatchers.any());
+    verify(segmentManager).loadSegment(any(), anyBoolean(), any(), any());
+    verify(segmentManager).dropSegment(any());
 
     // try to reload the segment - this should be a no-op since it might be the case that this is
     // the first load client
@@ -590,13 +584,8 @@ public class SegmentLoadDropHandlerTest {
     Assert.assertEquals(STATE.SUCCESS, result.get(0).getStatus().getState());
 
     // check invocations - should stay the same
-    Mockito.verify(segmentManager, Mockito.times(1))
-        .loadSegment(
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyBoolean(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
-    Mockito.verify(segmentManager, Mockito.times(1)).dropSegment(ArgumentMatchers.any());
+    verify(segmentManager).loadSegment(any(), anyBoolean(), any(), any());
+    verify(segmentManager).dropSegment(any());
 
     // try to reload the segment - this time the loader will know that is a fresh request to load
     // so, the segment manager will be asked to load
@@ -610,13 +599,8 @@ public class SegmentLoadDropHandlerTest {
     scheduledRunnable.clear();
 
     // check invocations - the load segment counter should bump up
-    Mockito.verify(segmentManager, Mockito.times(2))
-        .loadSegment(
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyBoolean(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
-    Mockito.verify(segmentManager, Mockito.times(1)).dropSegment(ArgumentMatchers.any());
+    verify(segmentManager, times(2)).loadSegment(any(), anyBoolean(), any(), any());
+    verify(segmentManager).dropSegment(any());
 
     segmentLoadDropHandler.stop();
   }
diff --git a/server/src/test/java/org/apache/druid/server/coordination/ServerManagerTest.java b/server/src/test/java/org/apache/druid/server/coordination/ServerManagerTest.java
index ffd810a676..fa09310f4f 100644
--- a/server/src/test/java/org/apache/druid/server/coordination/ServerManagerTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordination/ServerManagerTest.java
@@ -19,15 +19,17 @@
 
 package org.apache.druid.server.coordination;
 
+import static com.google.common.base.Functions.identity;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.google.common.base.Function;
-import com.google.common.base.Functions;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -237,7 +239,7 @@ public class ServerManagerTest {
             Granularities.DAY,
             "testTombstone",
             Intervals.of("P1d/2011-04-01"),
-            Collections.emptyList() // tombstone returns no data
+            emptyList() // tombstone returns no data
             );
     waitForTestVerificationAndCleanup(future);
   }
@@ -445,7 +447,7 @@ public class ServerManagerTest {
     final QueryRunner<Result<SearchResultValue>> queryRunner =
         serverManager.getQueryRunnerForIntervals(
             searchQuery("unknown_datasource", interval, Granularities.ALL),
-            Collections.singletonList(interval));
+            singletonList(interval));
     Assert.assertSame(NoopQueryRunner.class, queryRunner.getClass());
   }
 
@@ -456,7 +458,7 @@ public class ServerManagerTest {
     final SearchQuery query =
         searchQueryWithQueryDataSource("unknown_datasource", interval, Granularities.ALL);
     final List<SegmentDescriptor> unknownSegments =
-        Collections.singletonList(new SegmentDescriptor(interval, "unknown_version", 0));
+        singletonList(new SegmentDescriptor(interval, "unknown_version", 0));
     final QueryRunner<Result<SearchResultValue>> queryRunner =
         serverManager.getQueryRunnerForSegments(query, unknownSegments);
     final ResponseContext responseContext = DefaultResponseContext.createEmpty();
@@ -472,7 +474,7 @@ public class ServerManagerTest {
     final Interval interval = Intervals.of("0000-01-01/P1D");
     final SearchQuery query = searchQuery("unknown_datasource", interval, Granularities.ALL);
     final List<SegmentDescriptor> unknownSegments =
-        Collections.singletonList(new SegmentDescriptor(interval, "unknown_version", 0));
+        singletonList(new SegmentDescriptor(interval, "unknown_version", 0));
     final QueryRunner<Result<SearchResultValue>> queryRunner =
         serverManager.getQueryRunnerForSegments(query, unknownSegments);
     final ResponseContext responseContext = DefaultResponseContext.createEmpty();
@@ -488,7 +490,7 @@ public class ServerManagerTest {
     final Interval interval = Intervals.of("P1d/2011-04-01");
     final SearchQuery query = searchQuery("test", interval, Granularities.ALL);
     final List<SegmentDescriptor> unknownSegments =
-        Collections.singletonList(new SegmentDescriptor(interval, "unknown_version", 0));
+        singletonList(new SegmentDescriptor(interval, "unknown_version", 0));
     final QueryRunner<Result<SearchResultValue>> queryRunner =
         serverManager.getQueryRunnerForSegments(query, unknownSegments);
     final ResponseContext responseContext = DefaultResponseContext.createEmpty();
@@ -506,7 +508,7 @@ public class ServerManagerTest {
     final int unknownPartitionId = 1000;
     final SearchQuery query = searchQuery("test", interval, Granularities.ALL);
     final List<SegmentDescriptor> unknownSegments =
-        Collections.singletonList(new SegmentDescriptor(interval, "1", unknownPartitionId));
+        singletonList(new SegmentDescriptor(interval, "1", unknownPartitionId));
     final QueryRunner<Result<SearchResultValue>> queryRunner =
         serverManager.getQueryRunnerForSegments(query, unknownSegments);
     final ResponseContext responseContext = DefaultResponseContext.createEmpty();
@@ -553,7 +555,7 @@ public class ServerManagerTest {
   public void testGetQueryRunnerForSegmentsForUnknownQueryThrowingException() {
     final Interval interval = Intervals.of("P1d/2011-04-01");
     final List<SegmentDescriptor> descriptors =
-        Collections.singletonList(new SegmentDescriptor(interval, "1", 0));
+        singletonList(new SegmentDescriptor(interval, "1", 0));
     expectedException.expect(QueryUnsupportedException.class);
     expectedException.expectMessage("Unknown query type");
     serverManager.getQueryRunnerForSegments(
@@ -610,7 +612,7 @@ public class ServerManagerTest {
   private SearchQuery searchQuery(String datasource, Interval interval, Granularity granularity) {
     return Druids.newSearchQueryBuilder()
         .dataSource(datasource)
-        .intervals(Collections.singletonList(interval))
+        .intervals(singletonList(interval))
         .granularity(granularity)
         .limit(10000)
         .query("wow")
@@ -631,7 +633,7 @@ public class ServerManagerTest {
                     .intervals(new MultipleSpecificSegmentSpec(descriptors))
                     .granularity(Granularities.ALL)
                     .build()))
-        .intervals(Collections.singletonList(interval))
+        .intervals(singletonList(interval))
         .granularity(granularity)
         .limit(10000)
         .query("wow")
@@ -644,7 +646,7 @@ public class ServerManagerTest {
       Interval interval,
       List<Pair<String, Interval>> expected) {
     final Iterator<Pair<String, Interval>> expectedIter = expected.iterator();
-    final List<Interval> intervals = Collections.singletonList(interval);
+    final List<Interval> intervals = singletonList(interval);
     final SearchQuery query = searchQuery(dataSource, interval, granularity);
     final QueryRunner<Result<SearchResultValue>> runner =
         serverManager.getQueryRunnerForIntervals(query, intervals);
@@ -794,7 +796,7 @@ public class ServerManagerTest {
 
     @Override
     public Function<T, T> makePreComputeManipulatorFn(QueryType query, MetricManipulationFn fn) {
-      return Functions.identity();
+      return identity();
     }
 
     @Override
@@ -901,86 +903,83 @@ public class ServerManagerTest {
     }
 
     private StorageAdapter makeFakeStorageAdapter(Interval interval, int cardinality) {
-      StorageAdapter adapter =
-          new StorageAdapter() {
-            @Override
-            public Interval getInterval() {
-              return interval;
-            }
-
-            @Override
-            public int getDimensionCardinality(String column) {
-              return cardinality;
-            }
+      return new StorageAdapter() {
+        @Override
+        public Interval getInterval() {
+          return interval;
+        }
 
-            @Override
-            public DateTime getMinTime() {
-              return interval.getStart();
-            }
+        @Override
+        public int getDimensionCardinality(String column) {
+          return cardinality;
+        }
 
-            @Override
-            public DateTime getMaxTime() {
-              return interval.getEnd();
-            }
+        @Override
+        public DateTime getMinTime() {
+          return interval.getStart();
+        }
 
-            // stubs below this line not important for tests
+        @Override
+        public DateTime getMaxTime() {
+          return interval.getEnd();
+        }
 
-            @Override
-            public Indexed<String> getAvailableDimensions() {
-              return null;
-            }
+        // stubs below this line not important for tests
 
-            @Override
-            public Iterable<String> getAvailableMetrics() {
-              return null;
-            }
+        @Override
+        public Indexed<String> getAvailableDimensions() {
+          return null;
+        }
 
-            @Nullable
-            @Override
-            public Comparable getMinValue(String column) {
-              return null;
-            }
+        @Override
+        public Iterable<String> getAvailableMetrics() {
+          return null;
+        }
 
-            @Nullable
-            @Override
-            public Comparable getMaxValue(String column) {
-              return null;
-            }
+        @Nullable
+        @Override
+        public Comparable getMinValue(String column) {
+          return null;
+        }
 
-            @Nullable
-            @Override
-            public ColumnCapabilities getColumnCapabilities(String column) {
-              return null;
-            }
+        @Nullable
+        @Override
+        public Comparable getMaxValue(String column) {
+          return null;
+        }
 
-            @Override
-            public int getNumRows() {
-              return 0;
-            }
+        @Nullable
+        @Override
+        public ColumnCapabilities getColumnCapabilities(String column) {
+          return null;
+        }
 
-            @Override
-            public DateTime getMaxIngestedEventTime() {
-              return null;
-            }
+        @Override
+        public int getNumRows() {
+          return 0;
+        }
 
-            @Override
-            public Metadata getMetadata() {
-              return null;
-            }
+        @Override
+        public DateTime getMaxIngestedEventTime() {
+          return null;
+        }
 
-            @Override
-            public Sequence<Cursor> makeCursors(
-                @Nullable Filter filter,
-                Interval interval,
-                VirtualColumns virtualColumns,
-                Granularity gran,
-                boolean descending,
-                @Nullable QueryMetrics<?> queryMetrics) {
-              return null;
-            }
-          };
+        @Override
+        public Metadata getMetadata() {
+          return null;
+        }
 
-      return adapter;
+        @Override
+        public Sequence<Cursor> makeCursors(
+            @Nullable Filter filter,
+            Interval interval,
+            VirtualColumns virtualColumns,
+            Granularity gran,
+            boolean descending,
+            @Nullable QueryMetrics<?> queryMetrics) {
+          return null;
+        }
+      };
     }
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/coordination/TestStorageLocation.java b/server/src/test/java/org/apache/druid/server/coordination/TestStorageLocation.java
index bf68d81e31..6890e8c389 100644
--- a/server/src/test/java/org/apache/druid/server/coordination/TestStorageLocation.java
+++ b/server/src/test/java/org/apache/druid/server/coordination/TestStorageLocation.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.server.coordination;
 
+import static java.util.stream.Collectors.toSet;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.io.File;
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.java.util.common.FileUtils;
 import org.apache.druid.java.util.common.logger.Logger;
 import org.apache.druid.segment.TestHelper;
@@ -92,7 +93,7 @@ public class TestStorageLocation {
                     throw new RuntimeException(e);
                   }
                 })
-            .collect(Collectors.toSet());
+            .collect(toSet());
     Assert.assertEquals(expectedSegments, segmentsInFiles);
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/coordination/ZkCoordinatorTest.java b/server/src/test/java/org/apache/druid/server/coordination/ZkCoordinatorTest.java
index 170e2068d1..436a54b6fd 100644
--- a/server/src/test/java/org/apache/druid/server/coordination/ZkCoordinatorTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordination/ZkCoordinatorTest.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.server.coordination;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableMap;
 import java.io.File;
 import java.io.IOException;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ScheduledExecutorService;
@@ -83,7 +84,7 @@ public class ZkCoordinatorTest extends CuratorTestBase {
       throw new RuntimeException(e);
     }
 
-    locations = Collections.singletonList(new StorageLocationConfig(infoDir, 100L, 100d));
+    locations = singletonList(new StorageLocationConfig(infoDir, 100L, 100d));
 
     setupServerAndCurator();
     curator.start();
diff --git a/server/src/test/java/org/apache/druid/server/coordination/coordination/BatchDataSegmentAnnouncerTest.java b/server/src/test/java/org/apache/druid/server/coordination/coordination/BatchDataSegmentAnnouncerTest.java
index 341fdcb363..17f9d21f17 100644
--- a/server/src/test/java/org/apache/druid/server/coordination/coordination/BatchDataSegmentAnnouncerTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordination/coordination/BatchDataSegmentAnnouncerTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.coordination.coordination;
 
+import static java.util.Collections.singletonMap;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Joiner;
@@ -28,7 +30,6 @@ import com.google.common.collect.Iterables;
 import com.google.common.collect.Sets;
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
@@ -439,7 +440,7 @@ public class BatchDataSegmentAnnouncerTest {
         .loadSpec(ImmutableMap.of("type", "local"))
         .size(0);
     if (isTombstone) {
-      builder.loadSpec(Collections.singletonMap("type", DataSegment.TOMBSTONE_LOADSPEC_TYPE));
+      builder.loadSpec(singletonMap("type", DataSegment.TOMBSTONE_LOADSPEC_TYPE));
     }
 
     return builder.build();
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/BalanceSegmentsProfiler.java b/server/src/test/java/org/apache/druid/server/coordinator/BalanceSegmentsProfiler.java
index 0eec6af99d..6de58068bc 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/BalanceSegmentsProfiler.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/BalanceSegmentsProfiler.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyList;
+
 import com.google.common.base.Stopwatch;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -112,7 +113,7 @@ public class BalanceSegmentsProfiler {
       if (i == 0) {
         ImmutableDruidServerTests.expectSegments(server, segments);
       } else {
-        ImmutableDruidServerTests.expectSegments(server, Collections.emptyList());
+        ImmutableDruidServerTests.expectSegments(server, emptyList());
       }
       EasyMock.expect(server.getSegment(EasyMock.anyObject())).andReturn(null).anyTimes();
       EasyMock.replay(server);
@@ -167,7 +168,7 @@ public class BalanceSegmentsProfiler {
     EasyMock.expect(druidServer2.getTier()).andReturn("normal").anyTimes();
     EasyMock.expect(druidServer2.getCurrSize()).andReturn(0L).atLeastOnce();
     EasyMock.expect(druidServer2.getMaxSize()).andReturn(100L).atLeastOnce();
-    ImmutableDruidServerTests.expectSegments(druidServer2, Collections.emptyList());
+    ImmutableDruidServerTests.expectSegments(druidServer2, emptyList());
     EasyMock.expect(druidServer2.getSegment(EasyMock.anyObject())).andReturn(null).anyTimes();
     EasyMock.replay(druidServer2);
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/BalanceSegmentsTest.java b/server/src/test/java/org/apache/druid/server/coordinator/BalanceSegmentsTest.java
index b9a035ae1f..4c31aa489d 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/BalanceSegmentsTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/BalanceSegmentsTest.java
@@ -19,19 +19,22 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singleton;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toMap;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.util.concurrent.ListeningExecutorService;
 import com.google.common.util.concurrent.MoreExecutors;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import org.apache.druid.client.ImmutableDruidServer;
 import org.apache.druid.client.ImmutableDruidServerTests;
@@ -163,7 +166,7 @@ public class BalanceSegmentsTest {
     balancerStrategy =
         new CostBalancerStrategyFactory().createBalancerStrategy(balancerStrategyExecutor);
 
-    broadcastDatasources = Collections.singleton("datasourceBroadcast");
+    broadcastDatasources = singleton("datasourceBroadcast");
   }
 
   @After
@@ -179,7 +182,7 @@ public class BalanceSegmentsTest {
   @Test
   public void testMoveToEmptyServerBalancer() {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, segments);
-    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer3);
     EasyMock.replay(druidServer4);
@@ -216,7 +219,7 @@ public class BalanceSegmentsTest {
   public void testMoveDecommissioningMaxPercentOfMaxSegmentsToMove() {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, Arrays.asList(segment1, segment2));
     mockDruidServer(druidServer2, "2", "normal", 30L, 100L, Arrays.asList(segment3, segment4));
-    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer4);
 
@@ -292,7 +295,7 @@ public class BalanceSegmentsTest {
   public void testMoveDecommissioningMaxPercentOfMaxSegmentsToMoveWithNoDecommissioning() {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, Arrays.asList(segment1, segment2));
     mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Arrays.asList(segment3, segment4));
-    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer4);
 
@@ -338,7 +341,7 @@ public class BalanceSegmentsTest {
   @Test
   public void testMoveToDecommissioningServer() {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, segments);
-    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer3);
     EasyMock.replay(druidServer4);
@@ -377,7 +380,7 @@ public class BalanceSegmentsTest {
   @Test
   public void testMoveFromDecommissioningServer() {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, segments);
-    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer3);
     EasyMock.replay(druidServer4);
@@ -416,7 +419,7 @@ public class BalanceSegmentsTest {
   @Test
   public void testMoveMaxLoadQueueServerBalancer() {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, segments);
-    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer3);
     EasyMock.replay(druidServer4);
@@ -455,7 +458,7 @@ public class BalanceSegmentsTest {
   @Test
   public void testMoveSameSegmentTwice() {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, segments);
-    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer3);
     EasyMock.replay(druidServer4);
@@ -483,7 +486,7 @@ public class BalanceSegmentsTest {
   public void testRun1() {
     // Mock some servers of different usages
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, segments);
-    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer3);
     EasyMock.replay(druidServer4);
@@ -504,9 +507,9 @@ public class BalanceSegmentsTest {
   public void testRun2() {
     // Mock some servers of different usages
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, segments);
-    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Collections.emptyList());
-    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, Collections.emptyList());
-    mockDruidServer(druidServer4, "4", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, emptyList());
+    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, emptyList());
+    mockDruidServer(druidServer4, "4", "normal", 0L, 100L, emptyList());
 
     // Mock stuff that the coordinator needs
     mockCoordinator(coordinator);
@@ -526,7 +529,7 @@ public class BalanceSegmentsTest {
   public void testThatDynamicConfigIsHonoredWhenPickingSegmentToMove() {
     mockDruidServer(druidServer1, "1", "normal", 50L, 100L, Arrays.asList(segment1, segment2));
     mockDruidServer(druidServer2, "2", "normal", 30L, 100L, Arrays.asList(segment3, segment4));
-    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer4);
 
@@ -574,9 +577,9 @@ public class BalanceSegmentsTest {
   @Test
   public void testUseBatchedSegmentSampler() {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, segments);
-    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, Collections.emptyList());
-    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, Collections.emptyList());
-    mockDruidServer(druidServer4, "4", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer2, "2", "normal", 0L, 100L, emptyList());
+    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, emptyList());
+    mockDruidServer(druidServer4, "4", "normal", 0L, 100L, emptyList());
 
     mockCoordinator(coordinator);
 
@@ -597,7 +600,7 @@ public class BalanceSegmentsTest {
   private DruidCoordinatorRuntimeParams.Builder defaultRuntimeParamsBuilder(
       List<ImmutableDruidServer> druidServers, List<LoadQueuePeon> peons) {
     return defaultRuntimeParamsBuilder(
-        druidServers, peons, druidServers.stream().map(s -> false).collect(Collectors.toList()));
+        druidServers, peons, druidServers.stream().map(s -> false).collect(toList()));
   }
 
   private DruidCoordinatorRuntimeParams.Builder defaultRuntimeParamsBuilder(
@@ -619,7 +622,7 @@ public class BalanceSegmentsTest {
         .withLoadManagementPeons(
             IntStream.range(0, peons.size())
                 .boxed()
-                .collect(Collectors.toMap(i -> String.valueOf(i + 1), peons::get)))
+                .collect(toMap(i -> String.valueOf(i + 1), peons::get)))
         .withUsedSegmentsInTest(segments)
         .withDynamicConfigs(
             CoordinatorDynamicConfig.builder().withMaxSegmentsToMove(MAX_SEGMENTS_TO_MOVE).build())
@@ -708,7 +711,7 @@ public class BalanceSegmentsTest {
       int percent) {
     mockDruidServer(druidServer1, "1", "normal", 30L, 100L, Arrays.asList(segment1, segment3));
     mockDruidServer(druidServer2, "2", "normal", 30L, 100L, Arrays.asList(segment2, segment3));
-    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, Collections.emptyList());
+    mockDruidServer(druidServer3, "3", "normal", 0L, 100L, emptyList());
 
     EasyMock.replay(druidServer4);
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/CostBalancerStrategyBenchmark.java b/server/src/test/java/org/apache/druid/server/coordinator/CostBalancerStrategyBenchmark.java
index f1780174b3..522b1b1512 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/CostBalancerStrategyBenchmark.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/CostBalancerStrategyBenchmark.java
@@ -72,8 +72,8 @@ public class CostBalancerStrategyBenchmark extends AbstractBenchmark {
     serverHolderList = null;
   }
 
-  @Test
   @BenchmarkOptions(warmupRounds = 10, benchmarkRounds = 1000)
+  @Test
   public void testBenchmark() {
     DataSegment segment = CostBalancerStrategyTest.getSegment(1000, "testds", interval1);
     selected = strategy.findNewSegmentHomeReplicator(segment, serverHolderList);
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/CostBalancerStrategyTest.java b/server/src/test/java/org/apache/druid/server/coordinator/CostBalancerStrategyTest.java
index 7a2d5a8b5c..a1cb92c017 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/CostBalancerStrategyTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/CostBalancerStrategyTest.java
@@ -19,15 +19,16 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyMap;
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.util.concurrent.MoreExecutors;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import org.apache.druid.client.ImmutableDruidDataSource;
 import org.apache.druid.client.ImmutableDruidServer;
@@ -61,9 +62,11 @@ public class CostBalancerStrategyTest {
       LoadQueuePeonTester fromPeon = new LoadQueuePeonTester();
 
       List<DataSegment> segments =
-          IntStream.range(0, maxSegments).mapToObj(j -> getSegment(j)).collect(Collectors.toList());
+          IntStream.range(0, maxSegments)
+              .mapToObj(CostBalancerStrategyTest::getSegment)
+              .collect(toList());
       ImmutableDruidDataSource dataSource =
-          new ImmutableDruidDataSource("DUMMY", Collections.emptyMap(), segments);
+          new ImmutableDruidDataSource("DUMMY", emptyMap(), segments);
 
       String serverName = "DruidServer_Name_" + i;
       ServerHolder serverHolder =
@@ -111,18 +114,16 @@ public class CostBalancerStrategyTest {
 
   public static DataSegment getSegment(int index, String dataSource, Interval interval) {
     // Not using EasyMock as it hampers the performance of multithreads.
-    DataSegment segment =
-        new DataSegment(
-            dataSource,
-            interval,
-            String.valueOf(index),
-            new ConcurrentHashMap<>(),
-            new ArrayList<>(),
-            new ArrayList<>(),
-            null,
-            0,
-            index * 100L);
-    return segment;
+    return new DataSegment(
+        dataSource,
+        interval,
+        String.valueOf(index),
+        new ConcurrentHashMap<>(),
+        new ArrayList<>(),
+        new ArrayList<>(),
+        null,
+        0,
+        index * 100L);
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/CreateDataSegments.java b/server/src/test/java/org/apache/druid/server/coordinator/CreateDataSegments.java
index b21472fdae..1d9027dac9 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/CreateDataSegments.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/CreateDataSegments.java
@@ -19,8 +19,11 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableList;
+
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.java.util.common.granularity.Granularity;
@@ -82,7 +85,7 @@ public class CreateDataSegments {
       nextStart = granularity.increment(nextStart);
     }
 
-    return Collections.unmodifiableList(segments);
+    return unmodifiableList(segments);
   }
 
   /** Simple implementation of DataSegment with a unique integer id to make debugging easier. */
@@ -99,9 +102,9 @@ public class CreateDataSegments {
           datasource,
           interval,
           "1",
-          Collections.emptyMap(),
-          Collections.emptyList(),
-          Collections.emptyList(),
+          emptyMap(),
+          emptyList(),
+          emptyList(),
           shardSpec,
           IndexIO.CURRENT_VERSION_ID,
           size);
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java b/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java
index 0b1e424b59..1c314b57c2 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/CuratorDruidCoordinatorTest.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.server.coordinator;
 
+import static com.google.common.base.Predicates.alwaysTrue;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Predicates;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
@@ -292,7 +293,7 @@ public class CuratorDruidCoordinatorTest extends CuratorTestBase {
         .getListenable()
         .addListener(
             (CuratorFramework curatorFramework, PathChildrenCacheEvent event) -> {
-              if (event.getType().equals(PathChildrenCacheEvent.Type.INITIALIZED)) {
+              if (event.getType() == PathChildrenCacheEvent.Type.INITIALIZED) {
                 srcCountdown.countDown();
               } else if (CuratorUtils.isChildAdded(event)) {
                 // Simulate source server dropping segment
@@ -306,7 +307,7 @@ public class CuratorDruidCoordinatorTest extends CuratorTestBase {
         .getListenable()
         .addListener(
             (CuratorFramework curatorFramework, PathChildrenCacheEvent event) -> {
-              if (event.getType().equals(PathChildrenCacheEvent.Type.INITIALIZED)) {
+              if (event.getType() == PathChildrenCacheEvent.Type.INITIALIZED) {
                 destCountdown.countDown();
               } else if (CuratorUtils.isChildAdded(event)) {
                 // Simulate destination server loading segment
@@ -411,8 +412,7 @@ public class CuratorDruidCoordinatorTest extends CuratorTestBase {
 
   private void setupView() throws Exception {
     baseView =
-        new BatchServerInventoryView(
-            zkPathsConfig, curator, jsonMapper, Predicates.alwaysTrue(), "test") {
+        new BatchServerInventoryView(zkPathsConfig, curator, jsonMapper, alwaysTrue(), "test") {
           @Override
           public void registerSegmentCallback(Executor exec, final SegmentCallback callback) {
             super.registerSegmentCallback(
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/DataSourceCompactionConfigAuditEntryTest.java b/server/src/test/java/org/apache/druid/server/coordinator/DataSourceCompactionConfigAuditEntryTest.java
index 791cd28863..361374984b 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/DataSourceCompactionConfigAuditEntryTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/DataSourceCompactionConfigAuditEntryTest.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.server.coordinator;
 
+import static org.mockito.Mockito.when;
+
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -37,12 +38,10 @@ public class DataSourceCompactionConfigAuditEntryTest {
 
   @Before
   public void setUp() {
-    Mockito.when(coordinatorCompactionConfig.getCompactionTaskSlotRatio())
+    when(coordinatorCompactionConfig.getCompactionTaskSlotRatio())
         .thenReturn(COMPACTION_TASK_SLOT_RATIO);
-    Mockito.when(coordinatorCompactionConfig.getMaxCompactionTaskSlots())
-        .thenReturn(MAX_COMPACTION_SLOTS);
-    Mockito.when(coordinatorCompactionConfig.isUseAutoScaleSlots())
-        .thenReturn(USE_AUTO_SCALE_SLOTS);
+    when(coordinatorCompactionConfig.getMaxCompactionTaskSlots()).thenReturn(MAX_COMPACTION_SLOTS);
+    when(coordinatorCompactionConfig.isUseAutoScaleSlots()).thenReturn(USE_AUTO_SCALE_SLOTS);
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/DataSourceCompactionConfigHistoryTest.java b/server/src/test/java/org/apache/druid/server/coordinator/DataSourceCompactionConfigHistoryTest.java
index ac123fb12f..7e68acfc34 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/DataSourceCompactionConfigHistoryTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/DataSourceCompactionConfigHistoryTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server.coordinator;
 
+import static org.mockito.Answers.RETURNS_DEEP_STUBS;
+import static org.mockito.Answers.RETURNS_MOCKS;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableList;
 import org.apache.druid.audit.AuditInfo;
 import org.apache.druid.java.util.common.DateTimes;
@@ -27,9 +31,7 @@ import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
-import org.mockito.Answers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -46,37 +48,35 @@ public class DataSourceCompactionConfigHistoryTest {
 
   @Mock private CoordinatorCompactionConfig compactionConfig;
 
-  @Mock(answer = Answers.RETURNS_MOCKS)
+  @Mock(answer = RETURNS_MOCKS)
   private DataSourceCompactionConfig configForDataSource;
 
-  @Mock(answer = Answers.RETURNS_MOCKS)
+  @Mock(answer = RETURNS_MOCKS)
   private DataSourceCompactionConfig configForDataSourceWithChange;
 
-  @Mock(answer = Answers.RETURNS_MOCKS)
+  @Mock(answer = RETURNS_MOCKS)
   private DataSourceCompactionConfig configForDataSource2;
 
-  @Mock(answer = Answers.RETURNS_DEEP_STUBS)
+  @Mock(answer = RETURNS_DEEP_STUBS)
   private AuditInfo auditInfo;
 
-  @Mock(answer = Answers.RETURNS_DEEP_STUBS)
+  @Mock(answer = RETURNS_DEEP_STUBS)
   private AuditInfo auditInfo2;
 
-  @Mock(answer = Answers.RETURNS_DEEP_STUBS)
+  @Mock(answer = RETURNS_DEEP_STUBS)
   private AuditInfo auditInfo3;
 
   private DataSourceCompactionConfigHistory target;
 
   @Before
   public void setUp() {
-    Mockito.when(compactionConfig.getCompactionTaskSlotRatio())
-        .thenReturn(COMPACTION_TASK_SLOT_RATIO);
-    Mockito.when(compactionConfig.getMaxCompactionTaskSlots())
-        .thenReturn(MAX_COMPACTION_TASK_SLOTS);
-    Mockito.when(compactionConfig.isUseAutoScaleSlots()).thenReturn(USE_AUTO_SCALE_SLOTS);
-    Mockito.when(configForDataSource.getDataSource()).thenReturn(DATASOURCE);
-    Mockito.when(configForDataSourceWithChange.getDataSource()).thenReturn(DATASOURCE);
-    Mockito.when(configForDataSource2.getDataSource()).thenReturn(DATASOURCE_2);
-    Mockito.when(compactionConfig.getCompactionConfigs())
+    when(compactionConfig.getCompactionTaskSlotRatio()).thenReturn(COMPACTION_TASK_SLOT_RATIO);
+    when(compactionConfig.getMaxCompactionTaskSlots()).thenReturn(MAX_COMPACTION_TASK_SLOTS);
+    when(compactionConfig.isUseAutoScaleSlots()).thenReturn(USE_AUTO_SCALE_SLOTS);
+    when(configForDataSource.getDataSource()).thenReturn(DATASOURCE);
+    when(configForDataSourceWithChange.getDataSource()).thenReturn(DATASOURCE);
+    when(configForDataSource2.getDataSource()).thenReturn(DATASOURCE_2);
+    when(compactionConfig.getCompactionConfigs())
         .thenReturn(ImmutableList.of(configForDataSource, configForDataSource2));
     target = new DataSourceCompactionConfigHistory(DATASOURCE);
   }
@@ -94,7 +94,7 @@ public class DataSourceCompactionConfigHistoryTest {
   @Test
   public void testAddAndDeleteCompactionConfigShouldAddBothToHistory() {
     target.add(compactionConfig, auditInfo, AUDIT_TIME);
-    Mockito.when(compactionConfig.getCompactionConfigs())
+    when(compactionConfig.getCompactionConfigs())
         .thenReturn(ImmutableList.of(configForDataSource2));
     target.add(compactionConfig, auditInfo2, AUDIT_TIME_2);
     Assert.assertEquals(2, target.getHistory().size());
@@ -111,8 +111,7 @@ public class DataSourceCompactionConfigHistoryTest {
   @Test
   public void testAddAndDeleteAnotherCompactionConfigShouldNotAddToHistory() {
     target.add(compactionConfig, auditInfo, AUDIT_TIME);
-    Mockito.when(compactionConfig.getCompactionConfigs())
-        .thenReturn(ImmutableList.of(configForDataSource));
+    when(compactionConfig.getCompactionConfigs()).thenReturn(ImmutableList.of(configForDataSource));
     target.add(compactionConfig, auditInfo2, AUDIT_TIME_2);
     Assert.assertEquals(1, target.getHistory().size());
     DataSourceCompactionConfigAuditEntry auditEntry = target.getHistory().get(0);
@@ -124,10 +123,10 @@ public class DataSourceCompactionConfigHistoryTest {
   @Test
   public void testAddDeletedAddCompactionConfigShouldAddAllToHistory() {
     target.add(compactionConfig, auditInfo, AUDIT_TIME);
-    Mockito.when(compactionConfig.getCompactionConfigs())
+    when(compactionConfig.getCompactionConfigs())
         .thenReturn(ImmutableList.of(configForDataSource2));
     target.add(compactionConfig, auditInfo2, AUDIT_TIME_2);
-    Mockito.when(compactionConfig.getCompactionConfigs())
+    when(compactionConfig.getCompactionConfigs())
         .thenReturn(ImmutableList.of(configForDataSourceWithChange, configForDataSource2));
     target.add(compactionConfig, auditInfo3, AUDIT_TIME_3);
     Assert.assertEquals(3, target.getHistory().size());
@@ -144,7 +143,7 @@ public class DataSourceCompactionConfigHistoryTest {
   @Test
   public void testAddAndChangeCompactionConfigShouldAddBothToHistory() {
     target.add(compactionConfig, auditInfo, AUDIT_TIME);
-    Mockito.when(compactionConfig.getCompactionConfigs())
+    when(compactionConfig.getCompactionConfigs())
         .thenReturn(ImmutableList.of(configForDataSourceWithChange));
     target.add(compactionConfig, auditInfo2, AUDIT_TIME_2);
     Assert.assertEquals(2, target.getHistory().size());
@@ -162,7 +161,7 @@ public class DataSourceCompactionConfigHistoryTest {
   public void testAddAndChangeGlobalSettingsShouldAddTwice() {
     target.add(compactionConfig, auditInfo, AUDIT_TIME);
     int newMaxTaskSlots = MAX_COMPACTION_TASK_SLOTS - 1;
-    Mockito.when(compactionConfig.getMaxCompactionTaskSlots()).thenReturn(newMaxTaskSlots);
+    when(compactionConfig.getMaxCompactionTaskSlots()).thenReturn(newMaxTaskSlots);
     target.add(compactionConfig, auditInfo2, AUDIT_TIME_2);
     Assert.assertEquals(2, target.getHistory().size());
     DataSourceCompactionConfigAuditEntry auditEntry = target.getHistory().get(0);
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/DiskNormalizedCostBalancerStrategyTest.java b/server/src/test/java/org/apache/druid/server/coordinator/DiskNormalizedCostBalancerStrategyTest.java
index f530ae4204..53c589a397 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/DiskNormalizedCostBalancerStrategyTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/DiskNormalizedCostBalancerStrategyTest.java
@@ -19,13 +19,14 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyMap;
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.util.concurrent.MoreExecutors;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.ConcurrentHashMap;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import org.apache.druid.client.ImmutableDruidDataSource;
 import org.apache.druid.client.ImmutableDruidServer;
@@ -56,9 +57,11 @@ public class DiskNormalizedCostBalancerStrategyTest {
       LoadQueuePeonTester fromPeon = new LoadQueuePeonTester();
 
       List<DataSegment> segments =
-          IntStream.range(0, maxSegments).mapToObj(j -> getSegment(j)).collect(Collectors.toList());
+          IntStream.range(0, maxSegments)
+              .mapToObj(DiskNormalizedCostBalancerStrategyTest::getSegment)
+              .collect(toList());
       ImmutableDruidDataSource dataSource =
-          new ImmutableDruidDataSource("DUMMY", Collections.emptyMap(), segments);
+          new ImmutableDruidDataSource("DUMMY", emptyMap(), segments);
 
       serverHolderList.add(
           new ServerHolder(
@@ -110,18 +113,16 @@ public class DiskNormalizedCostBalancerStrategyTest {
 
   public static DataSegment getSegment(int index, String dataSource, Interval interval) {
     // Not using EasyMock as it hampers the performance of multithreads.
-    DataSegment segment =
-        new DataSegment(
-            dataSource,
-            interval,
-            String.valueOf(index),
-            new ConcurrentHashMap<>(),
-            new ArrayList<>(),
-            new ArrayList<>(),
-            null,
-            0,
-            index * 100L);
-    return segment;
+    return new DataSegment(
+        dataSource,
+        interval,
+        String.valueOf(index),
+        new ConcurrentHashMap<>(),
+        new ArrayList<>(),
+        new ArrayList<>(),
+        null,
+        0,
+        index * 100L);
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/DruidClusterBuilder.java b/server/src/test/java/org/apache/druid/server/coordinator/DruidClusterBuilder.java
index b15e219a28..51483c4eb1 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/DruidClusterBuilder.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/DruidClusterBuilder.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.coordinator;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -48,9 +50,10 @@ public final class DruidClusterBuilder {
   }
 
   public DruidClusterBuilder addTier(String tierName, ServerHolder... historicals) {
-    if (this.historicals.putIfAbsent(tierName, Arrays.asList(historicals)) != null) {
-      throw new IllegalArgumentException("Duplicate tier: " + tierName);
-    }
+    checkArgument(
+        this.historicals.putIfAbsent(tierName, Arrays.asList(historicals)) == null,
+        "Duplicate tier: %s",
+        tierName);
     return this;
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/DruidClusterTest.java b/server/src/test/java/org/apache/druid/server/coordinator/DruidClusterTest.java
index eaa6e6561c..d6ef64a425 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/DruidClusterTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/DruidClusterTest.java
@@ -19,15 +19,17 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableSet;
 import java.util.Set;
-import java.util.stream.Collectors;
 import org.apache.druid.client.ImmutableDruidDataSource;
 import org.apache.druid.client.ImmutableDruidServer;
 import org.apache.druid.java.util.common.Intervals;
@@ -65,12 +67,8 @@ public class DruidClusterTest {
 
   private static final Map<String, ImmutableDruidDataSource> DATA_SOURCES =
       ImmutableMap.of(
-          "src1",
-              new ImmutableDruidDataSource(
-                  "src1", Collections.emptyMap(), Collections.singletonList(SEGMENTS.get(0))),
-          "src2",
-              new ImmutableDruidDataSource(
-                  "src2", Collections.emptyMap(), Collections.singletonList(SEGMENTS.get(0))));
+          "src1", new ImmutableDruidDataSource("src1", emptyMap(), singletonList(SEGMENTS.get(0))),
+          "src2", new ImmutableDruidDataSource("src2", emptyMap(), singletonList(SEGMENTS.get(0))));
 
   private static final ServerHolder NEW_REALTIME =
       new ServerHolder(
@@ -151,7 +149,7 @@ public class DruidClusterTest {
         allServers.containsAll(
             cluster.getHistoricals().values().stream()
                 .flatMap(Collection::stream)
-                .collect(Collectors.toList())));
+                .collect(toList())));
 
     Assert.assertEquals(expectedHistoricals, cluster.getHistoricals());
     Assert.assertEquals(expectedRealtimes, cluster.getRealtimes());
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java b/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java
index a235d5b1e7..6c4ca2125e 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/DruidCoordinatorTest.java
@@ -19,6 +19,11 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singleton;
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
@@ -26,7 +31,6 @@ import com.google.common.collect.ImmutableSet;
 import com.google.common.util.concurrent.ListeningExecutorService;
 import it.unimi.dsi.fastutil.objects.Object2IntMap;
 import it.unimi.dsi.fastutil.objects.Object2LongMap;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
@@ -243,9 +247,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
     EasyMock.replay(scheduledExecutorFactory);
     EasyMock.replay(metadataRuleManager);
     ImmutableDruidDataSource dataSource = EasyMock.createMock(ImmutableDruidDataSource.class);
-    EasyMock.expect(dataSource.getSegments())
-        .andReturn(Collections.singletonList(segment))
-        .anyTimes();
+    EasyMock.expect(dataSource.getSegments()).andReturn(singletonList(segment)).anyTimes();
     EasyMock.replay(dataSource);
     EasyMock.expect(druidServer.toImmutableDruidServer())
         .andReturn(
@@ -327,7 +329,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
     EasyMock.replay(metadataRuleManager);
 
     // Setup SegmentsMetadataManager
-    DruidDataSource[] dataSources = {new DruidDataSource(dataSource, Collections.emptyMap())};
+    DruidDataSource[] dataSources = {new DruidDataSource(dataSource, emptyMap())};
     final DataSegment dataSegment =
         new DataSegment(
             dataSource, Intervals.of("2010-01-01/P1D"), "v1", null, null, null, null, 0x9, 0);
@@ -468,7 +470,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
     pathChildrenCache.start();
     pathChildrenCacheCold.start();
 
-    DruidDataSource[] druidDataSources = {new DruidDataSource(dataSource, Collections.emptyMap())};
+    DruidDataSource[] druidDataSources = {new DruidDataSource(dataSource, emptyMap())};
     dataSegments.values().forEach(druidDataSources[0]::addSegment);
 
     setupSegmentsMetadataMock(druidDataSources[0]);
@@ -653,7 +655,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
     pathChildrenCacheBroker2.start();
     pathChildrenCachePeon.start();
 
-    DruidDataSource[] druidDataSources = {new DruidDataSource(dataSource, Collections.emptyMap())};
+    DruidDataSource[] druidDataSources = {new DruidDataSource(dataSource, emptyMap())};
     dataSegments.values().forEach(druidDataSources[0]::addSegment);
 
     setupSegmentsMetadataMock(druidDataSources[0]);
@@ -788,7 +790,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
             null,
             null);
 
-    DruidCoordinator.DutiesRunnable duty = c.new DutiesRunnable(Collections.emptyList(), 0, "TEST");
+    DruidCoordinator.DutiesRunnable duty = c.new DutiesRunnable(emptyList(), 0, "TEST");
     // before initialization
     Assert.assertEquals(0, c.getCachedBalancerThreadNumber());
     Assert.assertNull(c.getBalancerExec());
@@ -845,9 +847,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
     // Since CompactSegments is not enabled in Custom Duty Group, then CompactSegments must be
     // created in IndexingServiceDuties
     List<CoordinatorDuty> indexingDuties = coordinator.makeIndexingServiceDuties();
-    Assert.assertTrue(
-        indexingDuties.stream()
-            .anyMatch(coordinatorDuty -> coordinatorDuty instanceof CompactSegments));
+    Assert.assertTrue(indexingDuties.stream().anyMatch(CompactSegments.class::isInstance));
 
     // CompactSegments should not exist in Custom Duty Group
     List<CompactSegments> compactSegmentsDutyFromCustomGroups =
@@ -895,9 +895,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
     // Since CompactSegments is not enabled in Custom Duty Group, then CompactSegments must be
     // created in IndexingServiceDuties
     List<CoordinatorDuty> indexingDuties = coordinator.makeIndexingServiceDuties();
-    Assert.assertTrue(
-        indexingDuties.stream()
-            .anyMatch(coordinatorDuty -> coordinatorDuty instanceof CompactSegments));
+    Assert.assertTrue(indexingDuties.stream().anyMatch(CompactSegments.class::isInstance));
 
     // CompactSegments should not exist in Custom Duty Group
     List<CompactSegments> compactSegmentsDutyFromCustomGroups =
@@ -954,9 +952,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
     // Since CompactSegments is enabled in Custom Duty Group, then CompactSegments must not be
     // created in IndexingServiceDuties
     List<CoordinatorDuty> indexingDuties = coordinator.makeIndexingServiceDuties();
-    Assert.assertTrue(
-        indexingDuties.stream()
-            .noneMatch(coordinatorDuty -> coordinatorDuty instanceof CompactSegments));
+    Assert.assertTrue(indexingDuties.stream().noneMatch(CompactSegments.class::isInstance));
 
     // CompactSegments should exist in Custom Duty Group
     List<CompactSegments> compactSegmentsDutyFromCustomGroups =
@@ -1000,7 +996,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
     EasyMock.expect(segmentsMetadataManager.isPollingDatabasePeriodically())
         .andReturn(true)
         .anyTimes();
-    DruidDataSource dataSource = new DruidDataSource("dataSource1", Collections.emptyMap());
+    DruidDataSource dataSource = new DruidDataSource("dataSource1", emptyMap());
     DataSegment dataSegment =
         new DataSegment(
             "dataSource1", Intervals.of("2010-01-01/P1D"), "v1", null, null, null, null, 0x9, 0);
@@ -1142,7 +1138,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
         .andReturn(dataSource.getSegments())
         .anyTimes();
     EasyMock.expect(segmentsMetadataManager.getImmutableDataSourcesWithAllUsedSegments())
-        .andReturn(Collections.singleton(dataSource.toImmutableDruidDataSource()))
+        .andReturn(singleton(dataSource.toImmutableDruidDataSource()))
         .anyTimes();
     DataSourcesSnapshot dataSourcesSnapshot =
         new DataSourcesSnapshot(
@@ -1151,7 +1147,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
         .andReturn(dataSourcesSnapshot)
         .anyTimes();
     EasyMock.expect(segmentsMetadataManager.retrieveAllDataSourceNames())
-        .andReturn(Collections.singleton(dataSource.getName()))
+        .andReturn(singleton(dataSource.getName()))
         .anyTimes();
     EasyMock.replay(segmentsMetadataManager);
 
@@ -1159,7 +1155,7 @@ public class DruidCoordinatorTest extends CuratorTestBase {
         .andReturn(dataSource.getSegments())
         .anyTimes();
     EasyMock.expect(this.dataSourcesSnapshot.getDataSourcesWithAllUsedSegments())
-        .andReturn(Collections.singleton(dataSource.toImmutableDruidDataSource()))
+        .andReturn(singleton(dataSource.toImmutableDruidDataSource()))
         .anyTimes();
     EasyMock.replay(this.dataSourcesSnapshot);
   }
@@ -1169,8 +1165,8 @@ public class DruidCoordinatorTest extends CuratorTestBase {
       Map<String, DataSegment> dataSegments, PathChildrenCacheEvent event) {
     return dataSegments.entrySet().stream()
         .filter(x -> event.getData().getPath().contains(x.getKey()))
-        .map(Map.Entry::getValue)
         .findFirst()
+        .map(Map.Entry::getValue)
         .orElse(null);
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTest.java b/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTest.java
index fda7d6aec8..8075554c98 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/LoadQueuePeonTest.java
@@ -104,8 +104,7 @@ public class LoadQueuePeonTest extends CuratorTestBase {
             new Function<String, DataSegment>() {
               @Override
               public DataSegment apply(String intervalStr) {
-                DataSegment dataSegment = dataSegmentWithInterval(intervalStr);
-                return dataSegment;
+                return dataSegmentWithInterval(intervalStr);
               }
             });
 
@@ -160,7 +159,7 @@ public class LoadQueuePeonTest extends CuratorTestBase {
                 "2014-10-30T00:00:00Z/P1D",
                 "2014-10-28T00:00:00Z/P1D",
                 "2014-10-27T00:00:00Z/P1D"),
-            intervalStr -> dataSegmentWithInterval(intervalStr));
+            this::dataSegmentWithInterval);
 
     final DataSegmentChangeHandler handler =
         new DataSegmentChangeHandler() {
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/ReservoirSegmentSamplerTest.java b/server/src/test/java/org/apache/druid/server/coordinator/ReservoirSegmentSamplerTest.java
index 839d703f54..c431ee97da 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/ReservoirSegmentSamplerTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/ReservoirSegmentSamplerTest.java
@@ -19,8 +19,10 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptySet;
+import static java.util.Collections.singleton;
+
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -63,8 +65,7 @@ public class ReservoirSegmentSamplerTest {
       // due to the pseudo-randomness of this method, we may not select a segment every single time
       // no matter what.
       segmentCountMap.compute(
-          ReservoirSegmentSampler.getRandomBalancerSegmentHolders(
-                  servers, Collections.emptySet(), 1)
+          ReservoirSegmentSampler.getRandomBalancerSegmentHolders(servers, emptySet(), 1)
               .get(0)
               .getSegment(),
           (segment, count) -> count == null ? 1 : count + 1);
@@ -96,7 +97,7 @@ public class ReservoirSegmentSamplerTest {
     for (int i = 0; i < iterations; i++) {
       segmentCountMap.compute(
           ReservoirSegmentSampler.getRandomBalancerSegmentHolder(
-                  servers, Collections.emptySet(), percentOfSegmentsToConsider)
+                  servers, emptySet(), percentOfSegmentsToConsider)
               .getSegment(),
           (segment, count) -> count == null ? 1 : count + 1);
     }
@@ -121,7 +122,7 @@ public class ReservoirSegmentSamplerTest {
     // Try to pick all the segments on the servers
     List<BalancerSegmentHolder> pickedSegments =
         ReservoirSegmentSampler.getRandomBalancerSegmentHolders(
-            Arrays.asList(historical, broker), Collections.emptySet(), 10);
+            Arrays.asList(historical, broker), emptySet(), 10);
 
     // Verify that only the segments on the historical are picked
     Assert.assertEquals(2, pickedSegments.size());
@@ -149,7 +150,7 @@ public class ReservoirSegmentSamplerTest {
     // Try to pick all the segments on the servers
     List<BalancerSegmentHolder> pickedSegments =
         ReservoirSegmentSampler.getRandomBalancerSegmentHolders(
-            servers, Collections.singleton(broadcastDatasource), 10);
+            servers, singleton(broadcastDatasource), 10);
 
     // Verify that none of the broadcast segments are picked
     Assert.assertEquals(2, pickedSegments.size());
@@ -200,8 +201,7 @@ public class ReservoirSegmentSamplerTest {
 
     int numIterations = 1;
     for (; numIterations < 10000; ++numIterations) {
-      ReservoirSegmentSampler.getRandomBalancerSegmentHolders(
-              servers, Collections.emptySet(), sampleSize)
+      ReservoirSegmentSampler.getRandomBalancerSegmentHolders(servers, emptySet(), sampleSize)
           .forEach(holder -> pickedSegments.add(holder.getSegment()));
 
       if (pickedSegments.size() >= numSegments) {
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/RoundRobinServerSelectorTest.java b/server/src/test/java/org/apache/druid/server/coordinator/RoundRobinServerSelectorTest.java
index a5b547cdda..8915cc9f8c 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/RoundRobinServerSelectorTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/RoundRobinServerSelectorTest.java
@@ -19,7 +19,9 @@
 
 package org.apache.druid.server.coordinator;
 
-import java.util.Collections;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+
 import java.util.Iterator;
 import org.apache.druid.client.DruidServer;
 import org.apache.druid.java.util.common.Intervals;
@@ -38,9 +40,9 @@ public class RoundRobinServerSelectorTest {
           "wiki",
           Intervals.of("2022-01-01/2022-01-02"),
           "1",
-          Collections.emptyMap(),
-          Collections.emptyList(),
-          Collections.emptyList(),
+          emptyMap(),
+          emptyList(),
+          emptyList(),
           new NumberedShardSpec(1, 10),
           IndexIO.CURRENT_VERSION_ID,
           100);
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/RunRulesTest.java b/server/src/test/java/org/apache/druid/server/coordinator/RunRulesTest.java
index aef87ef6d7..71b667e5cb 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/RunRulesTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/RunRulesTest.java
@@ -19,12 +19,13 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.singletonList;
+
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.util.concurrent.ListeningExecutorService;
 import com.google.common.util.concurrent.MoreExecutors;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -108,7 +109,7 @@ public class RunRulesTest {
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new IntervalLoadRule(
                     Intervals.of("2012-01-01T00:00:00.000Z/2012-01-02T00:00:00.000Z"),
                     ImmutableMap.of("normal", 2))))
@@ -180,7 +181,7 @@ public class RunRulesTest {
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new IntervalLoadRule(
                     Intervals.of("2012-01-01T00:00:00.000Z/2012-01-02T00:00:00.000Z"),
                     ImmutableMap.of("hot", 2, "normal", 2))))
@@ -549,7 +550,7 @@ public class RunRulesTest {
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new IntervalLoadRule(
                     Intervals.of("2012-01-02T00:00:00.000Z/2012-01-03T00:00:00.000Z"),
                     ImmutableMap.of("normal", 1))))
@@ -834,7 +835,7 @@ public class RunRulesTest {
     mockCoordinator();
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new IntervalLoadRule(
                     Intervals.of("2012-01-01T00:00:00.000Z/2012-01-01T01:00:00.000Z"),
                     ImmutableMap.of("normal", 0))))
@@ -910,7 +911,7 @@ public class RunRulesTest {
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new IntervalLoadRule(
                     Intervals.of("2012-01-01T00:00:00.000Z/2013-01-01T00:00:00.000Z"),
                     ImmutableMap.of("hot", 2))))
@@ -1000,7 +1001,7 @@ public class RunRulesTest {
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new IntervalLoadRule(
                     Intervals.of("2012-01-01T00:00:00.000Z/2013-01-01T00:00:00.000Z"),
                     ImmutableMap.of("hot", 1, DruidServer.DEFAULT_TIER, 1))))
@@ -1061,7 +1062,7 @@ public class RunRulesTest {
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new IntervalLoadRule(
                     Intervals.of("2012-01-01T00:00:00.000Z/2013-01-02T00:00:00.000Z"),
                     ImmutableMap.of("normal", 1))))
@@ -1079,7 +1080,7 @@ public class RunRulesTest {
             NoneShardSpec.instance(),
             1,
             0);
-    List<DataSegment> longerUsedSegments = Lists.newArrayList(usedSegments);
+    List<DataSegment> longerUsedSegments = new ArrayList<>(usedSegments);
     longerUsedSegments.add(overFlowSegment);
 
     DruidServer server1 =
@@ -1163,9 +1164,7 @@ public class RunRulesTest {
     mockEmptyPeon();
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
-        .andReturn(
-            Collections.singletonList(
-                new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, 1))))
+        .andReturn(singletonList(new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, 1))))
         .atLeastOnce();
     EasyMock.replay(databaseRuleManager);
 
@@ -1227,9 +1226,7 @@ public class RunRulesTest {
     mockEmptyPeon();
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
-        .andReturn(
-            Collections.singletonList(
-                new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, 3))))
+        .andReturn(singletonList(new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, 3))))
         .atLeastOnce();
     EasyMock.replay(databaseRuleManager);
 
@@ -1278,8 +1275,7 @@ public class RunRulesTest {
     RandomBalancerStrategy balancerStrategy = new RandomBalancerStrategy();
 
     DruidCoordinatorRuntimeParams params =
-        makeCoordinatorRuntimeParams(
-                druidCluster, balancerStrategy, Collections.singletonList(dataSegment))
+        makeCoordinatorRuntimeParams(druidCluster, balancerStrategy, singletonList(dataSegment))
             .withDynamicConfigs(CoordinatorDynamicConfig.builder().withMaxSegmentsToMove(5).build())
             .build();
 
@@ -1305,9 +1301,7 @@ public class RunRulesTest {
     mockEmptyPeon();
 
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
-        .andReturn(
-            Collections.singletonList(
-                new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, 1))))
+        .andReturn(singletonList(new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, 1))))
         .atLeastOnce();
     EasyMock.replay(databaseRuleManager);
 
@@ -1343,8 +1337,7 @@ public class RunRulesTest {
     RandomBalancerStrategy balancerStrategy = new RandomBalancerStrategy();
 
     DruidCoordinatorRuntimeParams params =
-        makeCoordinatorRuntimeParams(
-                druidCluster, balancerStrategy, Collections.singletonList(dataSegment))
+        makeCoordinatorRuntimeParams(druidCluster, balancerStrategy, singletonList(dataSegment))
             .withDynamicConfigs(CoordinatorDynamicConfig.builder().withMaxSegmentsToMove(5).build())
             .build();
 
@@ -1368,7 +1361,7 @@ public class RunRulesTest {
     int numReplicants = 1;
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, numReplicants))))
         .atLeastOnce();
     EasyMock.replay(databaseRuleManager);
@@ -1405,8 +1398,7 @@ public class RunRulesTest {
     RandomBalancerStrategy balancerStrategy = new RandomBalancerStrategy();
 
     DruidCoordinatorRuntimeParams params =
-        makeCoordinatorRuntimeParams(
-                druidCluster, balancerStrategy, Collections.singletonList(dataSegment))
+        makeCoordinatorRuntimeParams(druidCluster, balancerStrategy, singletonList(dataSegment))
             .withDynamicConfigs(CoordinatorDynamicConfig.builder().withMaxSegmentsToMove(5).build())
             .build();
 
@@ -1433,7 +1425,7 @@ public class RunRulesTest {
     int numReplicants = 1;
     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.anyObject()))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, numReplicants))))
         .atLeastOnce();
     EasyMock.replay(databaseRuleManager);
@@ -1472,8 +1464,7 @@ public class RunRulesTest {
     CostBalancerStrategy balancerStrategy = new CostBalancerStrategy(exec);
 
     DruidCoordinatorRuntimeParams params =
-        makeCoordinatorRuntimeParams(
-                druidCluster, balancerStrategy, Collections.singletonList(dataSegment))
+        makeCoordinatorRuntimeParams(druidCluster, balancerStrategy, singletonList(dataSegment))
             .withDynamicConfigs(CoordinatorDynamicConfig.builder().withMaxSegmentsToMove(5).build())
             .build();
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/ServerHolderTest.java b/server/src/test/java/org/apache/druid/server/coordinator/ServerHolderTest.java
index 3a1d7112bd..957bcc8eeb 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/ServerHolderTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/ServerHolderTest.java
@@ -19,9 +19,11 @@
 
 package org.apache.druid.server.coordinator;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonList;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import org.apache.druid.client.ImmutableDruidDataSource;
@@ -60,12 +62,8 @@ public class ServerHolderTest {
 
   private static final Map<String, ImmutableDruidDataSource> DATA_SOURCES =
       ImmutableMap.of(
-          "src1",
-              new ImmutableDruidDataSource(
-                  "src1", Collections.emptyMap(), Collections.singletonList(SEGMENTS.get(0))),
-          "src2",
-              new ImmutableDruidDataSource(
-                  "src2", Collections.emptyMap(), Collections.singletonList(SEGMENTS.get(1))));
+          "src1", new ImmutableDruidDataSource("src1", emptyMap(), singletonList(SEGMENTS.get(0))),
+          "src2", new ImmutableDruidDataSource("src2", emptyMap(), singletonList(SEGMENTS.get(1))));
 
   @Test
   public void testCompareTo() {
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/cost/CachingCostBalancerStrategyTest.java b/server/src/test/java/org/apache/druid/server/coordinator/cost/CachingCostBalancerStrategyTest.java
index d983386067..d5adf3d72d 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/cost/CachingCostBalancerStrategyTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/cost/CachingCostBalancerStrategyTest.java
@@ -19,14 +19,17 @@
 
 package org.apache.druid.server.coordinator.cost;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptyMap;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toSet;
+
 import com.google.common.util.concurrent.ListeningExecutorService;
 import com.google.common.util.concurrent.MoreExecutors;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Random;
 import java.util.concurrent.TimeUnit;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import org.apache.druid.client.DruidServer;
 import org.apache.druid.java.util.common.DateTimes;
@@ -71,7 +74,7 @@ public class CachingCostBalancerStrategyTest {
                         NUMBER_OF_SEGMENTS_ON_SERVER,
                         random,
                         referenceTime))
-            .collect(Collectors.toList());
+            .collect(toList());
 
     segmentQueries = createDataSegments(NUMBER_OF_QUERIES, random, referenceTime);
     executorService = MoreExecutors.listeningDecorator(Execs.singleThreaded(""));
@@ -136,7 +139,7 @@ public class CachingCostBalancerStrategyTest {
     return new ArrayList<>(
         IntStream.range(0, numberOfSegments)
             .mapToObj(i -> createRandomSegment(random, referenceTime))
-            .collect(Collectors.toSet()));
+            .collect(toSet()));
   }
 
   private DataSegment createRandomSegment(Random random, DateTime referenceTime) {
@@ -145,9 +148,9 @@ public class CachingCostBalancerStrategyTest {
         String.valueOf(random.nextInt(50)),
         new Interval(referenceTime.plusHours(timeShift), referenceTime.plusHours(timeShift + 1)),
         "version",
-        Collections.emptyMap(),
-        Collections.emptyList(),
-        Collections.emptyList(),
+        emptyMap(),
+        emptyList(),
+        emptyList(),
         null,
         0,
         100);
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/cost/SegmentsCostCacheTest.java b/server/src/test/java/org/apache/druid/server/coordinator/cost/SegmentsCostCacheTest.java
index 1b952e055b..bcf9b9966e 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/cost/SegmentsCostCacheTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/cost/SegmentsCostCacheTest.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.coordinator.cost;
 
+import static java.util.UUID.randomUUID;
+
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Random;
-import java.util.UUID;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.TimeUnit;
 import org.apache.druid.java.util.common.DateTimes;
@@ -163,7 +164,7 @@ public class SegmentsCostCacheTest {
     return new DataSegment(
         dataSource,
         interval,
-        UUID.randomUUID().toString(),
+        randomUUID().toString(),
         new ConcurrentHashMap<>(),
         new ArrayList<>(),
         new ArrayList<>(),
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java
index 9981933514..7487ac50f1 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java
@@ -19,27 +19,35 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.emptyList;
+import static java.util.Collections.emptySet;
+import static java.util.stream.Collectors.toList;
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyInt;
+import static org.mockito.ArgumentMatchers.anyString;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.InjectableValues;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import java.io.IOException;
 import java.net.URL;
-import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.function.BiFunction;
 import java.util.function.BooleanSupplier;
 import java.util.function.Supplier;
-import java.util.stream.Collectors;
 import junitparams.converters.Nullable;
 import org.apache.commons.lang3.mutable.MutableInt;
 import org.apache.druid.client.DataSourcesSnapshot;
@@ -123,14 +131,12 @@ import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 import org.mockito.ArgumentCaptor;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
 
 @RunWith(Parameterized.class)
 public class CompactSegmentsTest {
   private static final ObjectMapper JSON_MAPPER = new DefaultObjectMapper();
   private static final DruidCoordinatorConfig COORDINATOR_CONFIG =
-      Mockito.mock(DruidCoordinatorConfig.class);
+      mock(DruidCoordinatorConfig.class);
   private static final String DATA_SOURCE_PREFIX = "dataSource_";
   private static final int PARTITION_PER_TIME_INTERVAL = 4;
   // Each dataSource starts with 440 byte, 44 segments, and 11 intervals needing compaction
@@ -212,7 +218,7 @@ public class CompactSegmentsTest {
     dataSources =
         DataSourcesSnapshot.fromUsedSegments(allSegments, ImmutableMap.of())
             .getUsedSegmentsTimelinesPerDataSource();
-    Mockito.when(COORDINATOR_CONFIG.getCompactionSkipLockedIntervals()).thenReturn(true);
+    when(COORDINATOR_CONFIG.getCompactionSkipLockedIntervals()).thenReturn(true);
   }
 
   private DataSegment createSegment(
@@ -715,7 +721,7 @@ public class CompactSegmentsTest {
   @Test
   public void testCompactWithoutGranularitySpec() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -757,18 +763,18 @@ public class CompactSegmentsTest {
     ArgumentCaptor<List<DataSegment>> segmentsCaptor = ArgumentCaptor.forClass(List.class);
     ArgumentCaptor<ClientCompactionTaskGranularitySpec> granularitySpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskGranularitySpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
+            anyString(),
             segmentsCaptor.capture(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
+            anyInt(),
+            any(),
             granularitySpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any(),
+            any());
     // Only the same amount of segments as the original PARTITION_PER_TIME_INTERVAL since segment
     // granulartity is the same
     Assert.assertEquals(PARTITION_PER_TIME_INTERVAL, segmentsCaptor.getValue().size());
@@ -780,7 +786,7 @@ public class CompactSegmentsTest {
   @Test
   public void testCompactWithNotNullIOConfig() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -820,25 +826,25 @@ public class CompactSegmentsTest {
             null));
     doCompactSegments(compactSegments, compactionConfigs);
     ArgumentCaptor<Boolean> dropExistingCapture = ArgumentCaptor.forClass(Boolean.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
+            anyString(),
+            any(),
+            anyInt(),
+            any(),
+            any(),
+            any(),
+            any(),
+            any(),
             dropExistingCapture.capture(),
-            ArgumentMatchers.any());
+            any());
     Assert.assertEquals(true, dropExistingCapture.getValue());
   }
 
   @Test
   public void testCompactWithNullIOConfig() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -878,25 +884,25 @@ public class CompactSegmentsTest {
             null));
     doCompactSegments(compactSegments, compactionConfigs);
     ArgumentCaptor<Boolean> dropExistingCapture = ArgumentCaptor.forClass(Boolean.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
+            anyString(),
+            any(),
+            anyInt(),
+            any(),
+            any(),
+            any(),
+            any(),
+            any(),
             dropExistingCapture.capture(),
-            ArgumentMatchers.any());
+            any());
     Assert.assertNull(dropExistingCapture.getValue());
   }
 
   @Test
   public void testCompactWithGranularitySpec() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -938,18 +944,18 @@ public class CompactSegmentsTest {
     ArgumentCaptor<List<DataSegment>> segmentsCaptor = ArgumentCaptor.forClass(List.class);
     ArgumentCaptor<ClientCompactionTaskGranularitySpec> granularitySpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskGranularitySpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
+            anyString(),
             segmentsCaptor.capture(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
+            anyInt(),
+            any(),
             granularitySpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any(),
+            any());
     // All segments is compact at the same time since we changed the segment granularity to YEAR and
     // all segment
     // are within the same year
@@ -965,7 +971,7 @@ public class CompactSegmentsTest {
   @Test
   public void testCompactWithDimensionSpec() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1007,18 +1013,18 @@ public class CompactSegmentsTest {
     doCompactSegments(compactSegments, compactionConfigs);
     ArgumentCaptor<ClientCompactionTaskDimensionsSpec> dimensionsSpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskDimensionsSpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
+            anyString(),
+            any(),
+            anyInt(),
+            any(),
+            any(),
             dimensionsSpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any());
     ClientCompactionTaskDimensionsSpec actual = dimensionsSpecArgumentCaptor.getValue();
     Assert.assertNotNull(actual);
     Assert.assertEquals(
@@ -1028,7 +1034,7 @@ public class CompactSegmentsTest {
   @Test
   public void testCompactWithoutDimensionSpec() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1069,18 +1075,18 @@ public class CompactSegmentsTest {
     doCompactSegments(compactSegments, compactionConfigs);
     ArgumentCaptor<ClientCompactionTaskDimensionsSpec> dimensionsSpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskDimensionsSpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
+            anyString(),
+            any(),
+            anyInt(),
+            any(),
+            any(),
             dimensionsSpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any());
     ClientCompactionTaskDimensionsSpec actual = dimensionsSpecArgumentCaptor.getValue();
     Assert.assertNull(actual);
   }
@@ -1088,7 +1094,7 @@ public class CompactSegmentsTest {
   @Test
   public void testCompactWithRollupInGranularitySpec() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1130,18 +1136,18 @@ public class CompactSegmentsTest {
     ArgumentCaptor<List<DataSegment>> segmentsCaptor = ArgumentCaptor.forClass(List.class);
     ArgumentCaptor<ClientCompactionTaskGranularitySpec> granularitySpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskGranularitySpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
+            anyString(),
             segmentsCaptor.capture(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
+            anyInt(),
+            any(),
             granularitySpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any(),
+            any());
     Assert.assertEquals(
         datasourceToSegments.get(dataSource).size(), segmentsCaptor.getValue().size());
     ClientCompactionTaskGranularitySpec actual = granularitySpecArgumentCaptor.getValue();
@@ -1156,7 +1162,7 @@ public class CompactSegmentsTest {
     final String dataSource = DATA_SOURCE_PREFIX + 0;
     final String conflictTaskId = "taskIdDummy";
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     TaskStatusPlus runningConflictCompactionTask =
         new TaskStatusPlus(
             conflictTaskId,
@@ -1186,9 +1192,9 @@ public class CompactSegmentsTest {
                 null,
                 null,
                 null));
-    Mockito.when(mockIndexingServiceClient.getActiveTasks())
+    when(mockIndexingServiceClient.getActiveTasks())
         .thenReturn(ImmutableList.of(runningConflictCompactionTask));
-    Mockito.when(mockIndexingServiceClient.getTaskPayload(ArgumentMatchers.eq(conflictTaskId)))
+    when(mockIndexingServiceClient.getTaskPayload(conflictTaskId))
         .thenReturn(runningConflictCompactionTaskPayload);
 
     final CompactSegments compactSegments =
@@ -1229,7 +1235,7 @@ public class CompactSegmentsTest {
             null));
     doCompactSegments(compactSegments, compactionConfigs);
     // Verify that conflict task was canceled
-    Mockito.verify(mockIndexingServiceClient).cancelTask(conflictTaskId);
+    verify(mockIndexingServiceClient).cancelTask(conflictTaskId);
     // The active conflict task has interval of 2000/2099
     // Make sure that we do not skip interval of conflict task.
     // Since we cancel the task and will have to compact those intervals with the new
@@ -1237,18 +1243,18 @@ public class CompactSegmentsTest {
     ArgumentCaptor<List<DataSegment>> segmentsCaptor = ArgumentCaptor.forClass(List.class);
     ArgumentCaptor<ClientCompactionTaskGranularitySpec> granularitySpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskGranularitySpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
+            anyString(),
             segmentsCaptor.capture(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
+            anyInt(),
+            any(),
             granularitySpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any(),
+            any());
     // All segments is compact at the same time since we changed the segment granularity to YEAR and
     // all segment
     // are within the same year
@@ -1324,7 +1330,7 @@ public class CompactSegmentsTest {
   public void testCompactWithTransformSpec() {
     NullHandling.initializeForTests();
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1365,18 +1371,18 @@ public class CompactSegmentsTest {
     doCompactSegments(compactSegments, compactionConfigs);
     ArgumentCaptor<ClientCompactionTaskTransformSpec> transformSpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskTransformSpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
+            anyString(),
+            any(),
+            anyInt(),
+            any(),
+            any(),
+            any(),
+            any(),
             transformSpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any());
     ClientCompactionTaskTransformSpec actual = transformSpecArgumentCaptor.getValue();
     Assert.assertNotNull(actual);
     Assert.assertEquals(new SelectorDimFilter("dim1", "foo", null), actual.getFilter());
@@ -1385,7 +1391,7 @@ public class CompactSegmentsTest {
   @Test
   public void testCompactWithoutCustomSpecs() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1428,18 +1434,18 @@ public class CompactSegmentsTest {
         ArgumentCaptor.forClass(ClientCompactionTaskTransformSpec.class);
     ArgumentCaptor<AggregatorFactory[]> metricsSpecArgumentCaptor =
         ArgumentCaptor.forClass(AggregatorFactory[].class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
+            anyString(),
+            any(),
+            anyInt(),
+            any(),
+            any(),
+            any(),
             metricsSpecArgumentCaptor.capture(),
             transformSpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any());
     ClientCompactionTaskTransformSpec actualTransformSpec = transformSpecArgumentCaptor.getValue();
     Assert.assertNull(actualTransformSpec);
     AggregatorFactory[] actualMetricsSpec = metricsSpecArgumentCaptor.getValue();
@@ -1452,7 +1458,7 @@ public class CompactSegmentsTest {
     AggregatorFactory[] aggregatorFactories =
         new AggregatorFactory[] {new CountAggregatorFactory("cnt")};
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1493,18 +1499,18 @@ public class CompactSegmentsTest {
     doCompactSegments(compactSegments, compactionConfigs);
     ArgumentCaptor<AggregatorFactory[]> metricsSpecArgumentCaptor =
         ArgumentCaptor.forClass(AggregatorFactory[].class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
+            anyString(),
+            any(),
+            anyInt(),
+            any(),
+            any(),
+            any(),
             metricsSpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any());
     AggregatorFactory[] actual = metricsSpecArgumentCaptor.getValue();
     Assert.assertNotNull(actual);
     Assert.assertArrayEquals(aggregatorFactories, actual);
@@ -1512,7 +1518,7 @@ public class CompactSegmentsTest {
 
   @Test
   public void testRunWithLockedIntervalsNoSkip() {
-    Mockito.when(COORDINATOR_CONFIG.getCompactionSkipLockedIntervals()).thenReturn(false);
+    when(COORDINATOR_CONFIG.getCompactionSkipLockedIntervals()).thenReturn(false);
 
     final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);
     leaderClient.start();
@@ -1600,7 +1606,7 @@ public class CompactSegmentsTest {
             .getUsedSegmentsTimelinesPerDataSource();
 
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1641,18 +1647,18 @@ public class CompactSegmentsTest {
     ArgumentCaptor<List<DataSegment>> segmentsCaptor = ArgumentCaptor.forClass(List.class);
     ArgumentCaptor<ClientCompactionTaskGranularitySpec> granularitySpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskGranularitySpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
+            anyString(),
             segmentsCaptor.capture(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
+            anyInt(),
+            any(),
             granularitySpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any(),
+            any());
     Assert.assertEquals(2, segmentsCaptor.getValue().size());
     ClientCompactionTaskGranularitySpec actual = granularitySpecArgumentCaptor.getValue();
     Assert.assertNotNull(actual);
@@ -1692,7 +1698,7 @@ public class CompactSegmentsTest {
             .getUsedSegmentsTimelinesPerDataSource();
 
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1733,18 +1739,18 @@ public class CompactSegmentsTest {
     ArgumentCaptor<List<DataSegment>> segmentsCaptor = ArgumentCaptor.forClass(List.class);
     ArgumentCaptor<ClientCompactionTaskGranularitySpec> granularitySpecArgumentCaptor =
         ArgumentCaptor.forClass(ClientCompactionTaskGranularitySpec.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
+            anyString(),
             segmentsCaptor.capture(),
-            ArgumentMatchers.anyInt(),
-            ArgumentMatchers.any(),
+            anyInt(),
+            any(),
             granularitySpecArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any(),
+            any());
     Assert.assertEquals(2, segmentsCaptor.getValue().size());
     ClientCompactionTaskGranularitySpec actual = granularitySpecArgumentCaptor.getValue();
     Assert.assertNotNull(actual);
@@ -1756,7 +1762,7 @@ public class CompactSegmentsTest {
   @Test
   public void testCompactWithMetricsSpecShouldSetPreserveExistingMetricsTrue() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1798,18 +1804,18 @@ public class CompactSegmentsTest {
     ArgumentCaptor<ClientCompactionTaskQueryTuningConfig>
         clientCompactionTaskQueryTuningConfigArgumentCaptor =
             ArgumentCaptor.forClass(ClientCompactionTaskQueryTuningConfig.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
+            anyString(),
+            any(),
+            anyInt(),
             clientCompactionTaskQueryTuningConfigArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any(),
+            any(),
+            any());
     Assert.assertNotNull(clientCompactionTaskQueryTuningConfigArgumentCaptor.getValue());
     Assert.assertNotNull(
         clientCompactionTaskQueryTuningConfigArgumentCaptor.getValue().getAppendableIndexSpec());
@@ -1824,7 +1830,7 @@ public class CompactSegmentsTest {
   @Test
   public void testCompactWithoutMetricsSpecShouldSetPreserveExistingMetricsFalse() {
     final HttpIndexingServiceClient mockIndexingServiceClient =
-        Mockito.mock(HttpIndexingServiceClient.class);
+        mock(HttpIndexingServiceClient.class);
     final CompactSegments compactSegments =
         new CompactSegments(COORDINATOR_CONFIG, SEARCH_POLICY, mockIndexingServiceClient);
     final List<DataSourceCompactionConfig> compactionConfigs = new ArrayList<>();
@@ -1866,18 +1872,18 @@ public class CompactSegmentsTest {
     ArgumentCaptor<ClientCompactionTaskQueryTuningConfig>
         clientCompactionTaskQueryTuningConfigArgumentCaptor =
             ArgumentCaptor.forClass(ClientCompactionTaskQueryTuningConfig.class);
-    Mockito.verify(mockIndexingServiceClient)
+    verify(mockIndexingServiceClient)
         .compactSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.anyInt(),
+            anyString(),
+            any(),
+            anyInt(),
             clientCompactionTaskQueryTuningConfigArgumentCaptor.capture(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any(),
-            ArgumentMatchers.any());
+            any(),
+            any(),
+            any(),
+            any(),
+            any(),
+            any());
     Assert.assertNotNull(clientCompactionTaskQueryTuningConfigArgumentCaptor.getValue());
     Assert.assertNotNull(
         clientCompactionTaskQueryTuningConfigArgumentCaptor.getValue().getAppendableIndexSpec());
@@ -2206,8 +2212,7 @@ public class CompactSegmentsTest {
       } else if (urlString.contains("/druid/indexer/v1/waitingTasks")
           || urlString.contains("/druid/indexer/v1/pendingTasks")
           || urlString.contains("/druid/indexer/v1/runningTasks")) {
-        return createStringFullResponseHolder(
-            jsonMapper.writeValueAsString(Collections.emptyList()));
+        return createStringFullResponseHolder(jsonMapper.writeValueAsString(emptyList()));
       } else if (urlString.contains(("/druid/indexer/v1/lockedIntervals"))) {
         return handleLockedIntervals();
       } else {
@@ -2218,8 +2223,7 @@ public class CompactSegmentsTest {
     private StringFullResponseHolder createStringFullResponseHolder(String content) {
       final HttpResponse httpResponse =
           new DefaultHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK);
-      final StringFullResponseHolder holder =
-          new StringFullResponseHolder(httpResponse, StandardCharsets.UTF_8);
+      final StringFullResponseHolder holder = new StringFullResponseHolder(httpResponse, UTF_8);
       holder.addChunk(content);
       return holder;
     }
@@ -2232,8 +2236,8 @@ public class CompactSegmentsTest {
             new IndexingWorkerInfo(
                 new IndexingWorker("http", "host", "8091", 1, "version"),
                 0,
-                Collections.emptySet(),
-                Collections.emptyList(),
+                emptySet(),
+                emptyList(),
                 DateTimes.EPOCH,
                 null));
       }
@@ -2262,7 +2266,7 @@ public class CompactSegmentsTest {
           timeline.lookup(intervalToCompact).stream()
               .flatMap(holder -> Streams.sequentialStreamFrom(holder.getObject()))
               .map(PartitionChunk::getObject)
-              .collect(Collectors.toList());
+              .collect(toList());
       compactSegments(timeline, segments, compactionTaskQuery);
       return createStringFullResponseHolder(
           jsonMapper.writeValueAsString(ImmutableMap.of("task", taskQuery.getId())));
@@ -2276,7 +2280,7 @@ public class CompactSegmentsTest {
         VersionedIntervalTimeline<String, DataSegment> timeline,
         List<DataSegment> segments,
         ClientCompactionTaskQuery clientCompactionTaskQuery) {
-      Preconditions.checkArgument(segments.size() > 1);
+      checkArgument(segments.size() > 1);
       DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;
       for (DataSegment segment : segments) {
         if (segment.getInterval().getStart().compareTo(minStart) < 0) {
@@ -2392,50 +2396,49 @@ public class CompactSegmentsTest {
     @Test
     public void testIsParallelModeNullPartitionsSpecReturnFalse() {
       ClientCompactionTaskQueryTuningConfig tuningConfig =
-          Mockito.mock(ClientCompactionTaskQueryTuningConfig.class);
-      Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(null);
+          mock(ClientCompactionTaskQueryTuningConfig.class);
+      when(tuningConfig.getPartitionsSpec()).thenReturn(null);
       Assert.assertFalse(CompactSegments.isParallelMode(tuningConfig));
     }
 
     @Test
     public void testIsParallelModeNonRangePartitionVaryingMaxNumConcurrentSubTasks() {
       ClientCompactionTaskQueryTuningConfig tuningConfig =
-          Mockito.mock(ClientCompactionTaskQueryTuningConfig.class);
-      Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(Mockito.mock(PartitionsSpec.class));
+          mock(ClientCompactionTaskQueryTuningConfig.class);
+      when(tuningConfig.getPartitionsSpec()).thenReturn(mock(PartitionsSpec.class));
 
-      Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(null);
+      when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(null);
       Assert.assertFalse(CompactSegments.isParallelMode(tuningConfig));
 
-      Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
+      when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
       Assert.assertFalse(CompactSegments.isParallelMode(tuningConfig));
 
-      Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
+      when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
       Assert.assertTrue(CompactSegments.isParallelMode(tuningConfig));
     }
 
     @Test
     public void testIsParallelModeRangePartitionVaryingMaxNumConcurrentSubTasks() {
       ClientCompactionTaskQueryTuningConfig tuningConfig =
-          Mockito.mock(ClientCompactionTaskQueryTuningConfig.class);
-      Mockito.when(tuningConfig.getPartitionsSpec())
-          .thenReturn(Mockito.mock(SingleDimensionPartitionsSpec.class));
+          mock(ClientCompactionTaskQueryTuningConfig.class);
+      when(tuningConfig.getPartitionsSpec()).thenReturn(mock(SingleDimensionPartitionsSpec.class));
 
-      Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(null);
+      when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(null);
       Assert.assertFalse(CompactSegments.isParallelMode(tuningConfig));
 
-      Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
+      when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
       Assert.assertTrue(CompactSegments.isParallelMode(tuningConfig));
 
-      Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
+      when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
       Assert.assertTrue(CompactSegments.isParallelMode(tuningConfig));
     }
 
     @Test
     public void testFindMaxNumTaskSlotsUsedByOneCompactionTaskWhenIsParallelMode() {
       ClientCompactionTaskQueryTuningConfig tuningConfig =
-          Mockito.mock(ClientCompactionTaskQueryTuningConfig.class);
-      Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(Mockito.mock(PartitionsSpec.class));
-      Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
+          mock(ClientCompactionTaskQueryTuningConfig.class);
+      when(tuningConfig.getPartitionsSpec()).thenReturn(mock(PartitionsSpec.class));
+      when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(2);
       Assert.assertEquals(
           3, CompactSegments.findMaxNumTaskSlotsUsedByOneCompactionTask(tuningConfig));
     }
@@ -2443,9 +2446,9 @@ public class CompactSegmentsTest {
     @Test
     public void testFindMaxNumTaskSlotsUsedByOneCompactionTaskWhenIsSequentialMode() {
       ClientCompactionTaskQueryTuningConfig tuningConfig =
-          Mockito.mock(ClientCompactionTaskQueryTuningConfig.class);
-      Mockito.when(tuningConfig.getPartitionsSpec()).thenReturn(Mockito.mock(PartitionsSpec.class));
-      Mockito.when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
+          mock(ClientCompactionTaskQueryTuningConfig.class);
+      when(tuningConfig.getPartitionsSpec()).thenReturn(mock(PartitionsSpec.class));
+      when(tuningConfig.getMaxNumConcurrentSubTasks()).thenReturn(1);
       Assert.assertEquals(
           1, CompactSegments.findMaxNumTaskSlotsUsedByOneCompactionTask(tuningConfig));
     }
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetricsTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetricsTest.java
index 8d98f8c8dd..7dee1621b0 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetricsTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetricsTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static org.mockito.Mockito.atLeastOnce;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableMap;
 import it.unimi.dsi.fastutil.objects.Object2IntMaps;
 import java.util.List;
@@ -35,7 +39,6 @@ import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.mockito.ArgumentCaptor;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -51,21 +54,20 @@ public class EmitClusterStatsAndMetricsTest {
   public void testRunOnlyEmitStatsForHistoricalDuties() {
     ArgumentCaptor<ServiceEventBuilder> argumentCaptor =
         ArgumentCaptor.forClass(ServiceEventBuilder.class);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getCoordinatorStats())
-        .thenReturn(mockCoordinatorStats);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getDruidCluster()).thenReturn(mockDruidCluster);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getDatabaseRuleManager())
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getCoordinatorStats()).thenReturn(mockCoordinatorStats);
+    when(mockDruidCoordinatorRuntimeParams.getDruidCluster()).thenReturn(mockDruidCluster);
+    when(mockDruidCoordinatorRuntimeParams.getDatabaseRuleManager())
         .thenReturn(mockMetadataRuleManager);
-    Mockito.when(mockDruidCoordinator.computeNumsUnavailableUsedSegmentsPerDataSource())
+    when(mockDruidCoordinator.computeNumsUnavailableUsedSegmentsPerDataSource())
         .thenReturn(Object2IntMaps.emptyMap());
-    Mockito.when(mockDruidCoordinator.computeUnderReplicationCountsPerDataSourcePerTier())
+    when(mockDruidCoordinator.computeUnderReplicationCountsPerDataSourcePerTier())
         .thenReturn(ImmutableMap.of());
     CoordinatorDuty duty =
         new EmitClusterStatsAndMetrics(
             mockDruidCoordinator, DruidCoordinator.HISTORICAL_MANAGEMENT_DUTIES_DUTY_GROUP, false);
     duty.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verify(mockServiceEmitter, Mockito.atLeastOnce()).emit(argumentCaptor.capture());
+    verify(mockServiceEmitter, atLeastOnce()).emit(argumentCaptor.capture());
     List<ServiceEventBuilder> emittedEvents = argumentCaptor.getAllValues();
     boolean foundCompactMetric = false;
     boolean foundHistoricalDutyMetric = false;
@@ -90,13 +92,12 @@ public class EmitClusterStatsAndMetricsTest {
     String groupName = "blah";
     ArgumentCaptor<ServiceEventBuilder> argumentCaptor =
         ArgumentCaptor.forClass(ServiceEventBuilder.class);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getCoordinatorStats())
-        .thenReturn(mockCoordinatorStats);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getDruidCluster()).thenReturn(mockDruidCluster);
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getCoordinatorStats()).thenReturn(mockCoordinatorStats);
+    when(mockDruidCoordinatorRuntimeParams.getDruidCluster()).thenReturn(mockDruidCluster);
     CoordinatorDuty duty = new EmitClusterStatsAndMetrics(mockDruidCoordinator, groupName, true);
     duty.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verify(mockServiceEmitter, Mockito.atLeastOnce()).emit(argumentCaptor.capture());
+    verify(mockServiceEmitter, atLeastOnce()).emit(argumentCaptor.capture());
     List<ServiceEventBuilder> emittedEvents = argumentCaptor.getAllValues();
     boolean foundCompactMetric = false;
     boolean foundHistoricalDutyMetric = false;
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillAuditLogTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillAuditLogTest.java
index f64980a4c0..ad1aabf59d 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillAuditLogTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillAuditLogTest.java
@@ -19,6 +19,12 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyLong;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyNoInteractions;
+import static org.mockito.Mockito.when;
+
 import org.apache.druid.audit.AuditManager;
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
 import org.apache.druid.java.util.emitter.service.ServiceEventBuilder;
@@ -29,9 +35,7 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -58,12 +62,12 @@ public class KillAuditLogTest {
             .build();
     killAuditLog = new KillAuditLog(mockAuditManager, druidCoordinatorConfig);
     killAuditLog.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verifyNoInteractions(mockAuditManager);
+    verifyNoInteractions(mockAuditManager);
   }
 
   @Test
   public void testRunNotSkipIfLastRunMoreThanPeriod() {
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
     TestDruidCoordinatorConfig druidCoordinatorConfig =
         new TestDruidCoordinatorConfig.Builder()
             .withMetadataStoreManagementPeriod(new Duration("PT5s"))
@@ -74,8 +78,8 @@ public class KillAuditLogTest {
             .build();
     killAuditLog = new KillAuditLog(mockAuditManager, druidCoordinatorConfig);
     killAuditLog.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verify(mockAuditManager).removeAuditLogsOlderThan(ArgumentMatchers.anyLong());
-    Mockito.verify(mockServiceEmitter).emit(ArgumentMatchers.any(ServiceEventBuilder.class));
+    verify(mockAuditManager).removeAuditLogsOlderThan(anyLong());
+    verify(mockServiceEmitter).emit(any(ServiceEventBuilder.class));
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillCompactionConfigTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillCompactionConfigTest.java
index da7fd3a343..f857c8d030 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillCompactionConfigTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillCompactionConfigTest.java
@@ -19,6 +19,15 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyString;
+import static org.mockito.ArgumentMatchers.eq;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyNoInteractions;
+import static org.mockito.Mockito.verifyNoMoreInteractions;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
@@ -44,9 +53,7 @@ import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
 import org.mockito.ArgumentCaptor;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.invocation.InvocationOnMock;
 import org.mockito.junit.MockitoJUnitRunner;
 import org.mockito.stubbing.Answer;
@@ -71,7 +78,7 @@ public class KillCompactionConfigTest {
 
   @Before
   public void setup() {
-    Mockito.when(mockConnectorConfig.getConfigTable()).thenReturn("druid_config");
+    when(mockConnectorConfig.getConfigTable()).thenReturn("druid_config");
   }
 
   @Test
@@ -91,9 +98,9 @@ public class KillCompactionConfigTest {
             mockConnector,
             mockConnectorConfig);
     killCompactionConfig.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verifyNoInteractions(mockSqlSegmentsMetadataManager);
-    Mockito.verifyNoInteractions(mockJacksonConfigManager);
-    Mockito.verifyNoInteractions(mockServiceEmitter);
+    verifyNoInteractions(mockSqlSegmentsMetadataManager);
+    verifyNoInteractions(mockJacksonConfigManager);
+    verifyNoInteractions(mockServiceEmitter);
   }
 
   @Test
@@ -119,20 +126,13 @@ public class KillCompactionConfigTest {
 
   @Test
   public void testRunDoNothingIfCurrentConfigIsEmpty() {
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
     // Set current compaction config to an empty compaction config
-    Mockito.when(
-            mockConnector.lookup(
-                ArgumentMatchers.anyString(),
-                ArgumentMatchers.eq("name"),
-                ArgumentMatchers.eq("payload"),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY)))
+    when(mockConnector.lookup(
+            anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY)))
         .thenReturn(null);
-    Mockito.when(
-            mockJacksonConfigManager.convertByteToConfig(
-                ArgumentMatchers.eq(null),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.empty())))
+    when(mockJacksonConfigManager.convertByteToConfig(
+            null, CoordinatorCompactionConfig.class, CoordinatorCompactionConfig.empty()))
         .thenReturn(CoordinatorCompactionConfig.empty());
 
     TestDruidCoordinatorConfig druidCoordinatorConfig =
@@ -150,27 +150,21 @@ public class KillCompactionConfigTest {
             mockConnector,
             mockConnectorConfig);
     killCompactionConfig.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verifyNoInteractions(mockSqlSegmentsMetadataManager);
+    verifyNoInteractions(mockSqlSegmentsMetadataManager);
     final ArgumentCaptor<ServiceEventBuilder> emittedEventCaptor =
         ArgumentCaptor.forClass(ServiceEventBuilder.class);
-    Mockito.verify(mockServiceEmitter).emit(emittedEventCaptor.capture());
+    verify(mockServiceEmitter).emit(emittedEventCaptor.capture());
     Assert.assertEquals(
         KillCompactionConfig.COUNT_METRIC,
         emittedEventCaptor.getValue().build(ImmutableMap.of()).toMap().get("metric"));
     Assert.assertEquals(
         0, emittedEventCaptor.getValue().build(ImmutableMap.of()).toMap().get("value"));
-    Mockito.verify(mockJacksonConfigManager)
+    verify(mockJacksonConfigManager)
         .convertByteToConfig(
-            ArgumentMatchers.eq(null),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.empty()));
-    Mockito.verify(mockConnector)
-        .lookup(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.eq("name"),
-            ArgumentMatchers.eq("payload"),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY));
-    Mockito.verifyNoMoreInteractions(mockJacksonConfigManager);
+            null, CoordinatorCompactionConfig.class, CoordinatorCompactionConfig.empty());
+    verify(mockConnector)
+        .lookup(anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY));
+    verifyNoMoreInteractions(mockJacksonConfigManager);
   }
 
   @Test
@@ -210,31 +204,25 @@ public class KillCompactionConfigTest {
         CoordinatorCompactionConfig.from(
             ImmutableList.of(inactiveDatasourceConfig, activeDatasourceConfig));
     byte[] originalCurrentConfigBytes = {1, 2, 3};
-    Mockito.when(
-            mockConnector.lookup(
-                ArgumentMatchers.anyString(),
-                ArgumentMatchers.eq("name"),
-                ArgumentMatchers.eq("payload"),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY)))
+    when(mockConnector.lookup(
+            anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY)))
         .thenReturn(originalCurrentConfigBytes);
-    Mockito.when(
-            mockJacksonConfigManager.convertByteToConfig(
-                ArgumentMatchers.eq(originalCurrentConfigBytes),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.empty())))
+    when(mockJacksonConfigManager.convertByteToConfig(
+            originalCurrentConfigBytes,
+            CoordinatorCompactionConfig.class,
+            CoordinatorCompactionConfig.empty()))
         .thenReturn(originalCurrentConfig);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
-    Mockito.when(mockSqlSegmentsMetadataManager.retrieveAllDataSourceNames())
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockSqlSegmentsMetadataManager.retrieveAllDataSourceNames())
         .thenReturn(ImmutableSet.of(activeDatasourceName));
     final ArgumentCaptor<byte[]> oldConfigCaptor = ArgumentCaptor.forClass(byte[].class);
     final ArgumentCaptor<CoordinatorCompactionConfig> newConfigCaptor =
         ArgumentCaptor.forClass(CoordinatorCompactionConfig.class);
-    Mockito.when(
-            mockJacksonConfigManager.set(
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                oldConfigCaptor.capture(),
-                newConfigCaptor.capture(),
-                ArgumentMatchers.any()))
+    when(mockJacksonConfigManager.set(
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            oldConfigCaptor.capture(),
+            newConfigCaptor.capture(),
+            any()))
         .thenReturn(ConfigManager.SetResult.ok());
 
     TestDruidCoordinatorConfig druidCoordinatorConfig =
@@ -264,7 +252,7 @@ public class KillCompactionConfigTest {
         activeDatasourceConfig, newConfigCaptor.getValue().getCompactionConfigs().get(0));
     final ArgumentCaptor<ServiceEventBuilder> emittedEventCaptor =
         ArgumentCaptor.forClass(ServiceEventBuilder.class);
-    Mockito.verify(mockServiceEmitter).emit(emittedEventCaptor.capture());
+    verify(mockServiceEmitter).emit(emittedEventCaptor.capture());
     Assert.assertEquals(
         KillCompactionConfig.COUNT_METRIC,
         emittedEventCaptor.getValue().build(ImmutableMap.of()).toMap().get("metric"));
@@ -272,26 +260,22 @@ public class KillCompactionConfigTest {
     Assert.assertEquals(
         1, emittedEventCaptor.getValue().build(ImmutableMap.of()).toMap().get("value"));
 
-    Mockito.verify(mockJacksonConfigManager)
+    verify(mockJacksonConfigManager)
         .convertByteToConfig(
-            ArgumentMatchers.eq(originalCurrentConfigBytes),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.empty()));
-    Mockito.verify(mockConnector)
-        .lookup(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.eq("name"),
-            ArgumentMatchers.eq("payload"),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY));
-    Mockito.verify(mockJacksonConfigManager)
+            originalCurrentConfigBytes,
+            CoordinatorCompactionConfig.class,
+            CoordinatorCompactionConfig.empty());
+    verify(mockConnector)
+        .lookup(anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY));
+    verify(mockJacksonConfigManager)
         .set(
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-            ArgumentMatchers.any(byte[].class),
-            ArgumentMatchers.any(CoordinatorCompactionConfig.class),
-            ArgumentMatchers.any());
-    Mockito.verifyNoMoreInteractions(mockJacksonConfigManager);
-    Mockito.verify(mockSqlSegmentsMetadataManager).retrieveAllDataSourceNames();
-    Mockito.verifyNoMoreInteractions(mockSqlSegmentsMetadataManager);
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            any(byte[].class),
+            any(CoordinatorCompactionConfig.class),
+            any());
+    verifyNoMoreInteractions(mockJacksonConfigManager);
+    verify(mockSqlSegmentsMetadataManager).retrieveAllDataSourceNames();
+    verifyNoMoreInteractions(mockSqlSegmentsMetadataManager);
   }
 
   @Test
@@ -315,28 +299,21 @@ public class KillCompactionConfigTest {
     CoordinatorCompactionConfig originalCurrentConfig =
         CoordinatorCompactionConfig.from(ImmutableList.of(inactiveDatasourceConfig));
     byte[] originalCurrentConfigBytes = {1, 2, 3};
-    Mockito.when(
-            mockConnector.lookup(
-                ArgumentMatchers.anyString(),
-                ArgumentMatchers.eq("name"),
-                ArgumentMatchers.eq("payload"),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY)))
+    when(mockConnector.lookup(
+            anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY)))
         .thenReturn(originalCurrentConfigBytes);
-    Mockito.when(
-            mockJacksonConfigManager.convertByteToConfig(
-                ArgumentMatchers.eq(originalCurrentConfigBytes),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.empty())))
+    when(mockJacksonConfigManager.convertByteToConfig(
+            originalCurrentConfigBytes,
+            CoordinatorCompactionConfig.class,
+            CoordinatorCompactionConfig.empty()))
         .thenReturn(originalCurrentConfig);
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
-    Mockito.when(mockSqlSegmentsMetadataManager.retrieveAllDataSourceNames())
-        .thenReturn(ImmutableSet.of());
-    Mockito.when(
-            mockJacksonConfigManager.set(
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                ArgumentMatchers.any(byte[].class),
-                ArgumentMatchers.any(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.any()))
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockSqlSegmentsMetadataManager.retrieveAllDataSourceNames()).thenReturn(ImmutableSet.of());
+    when(mockJacksonConfigManager.set(
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            any(byte[].class),
+            any(CoordinatorCompactionConfig.class),
+            any()))
         .thenAnswer(
             new Answer() {
               private int count = 0;
@@ -372,7 +349,7 @@ public class KillCompactionConfigTest {
     // Verify and Assert
     final ArgumentCaptor<ServiceEventBuilder> emittedEventCaptor =
         ArgumentCaptor.forClass(ServiceEventBuilder.class);
-    Mockito.verify(mockServiceEmitter).emit(emittedEventCaptor.capture());
+    verify(mockServiceEmitter).emit(emittedEventCaptor.capture());
     Assert.assertEquals(
         KillCompactionConfig.COUNT_METRIC,
         emittedEventCaptor.getValue().build(ImmutableMap.of()).toMap().get("metric"));
@@ -382,29 +359,25 @@ public class KillCompactionConfigTest {
 
     // Should call convertByteToConfig and lookup (to refresh current compaction config) four times
     // due to RetryableException when failed
-    Mockito.verify(mockJacksonConfigManager, Mockito.times(4))
+    verify(mockJacksonConfigManager, times(4))
         .convertByteToConfig(
-            ArgumentMatchers.eq(originalCurrentConfigBytes),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.empty()));
-    Mockito.verify(mockConnector, Mockito.times(4))
-        .lookup(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.eq("name"),
-            ArgumentMatchers.eq("payload"),
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY));
+            originalCurrentConfigBytes,
+            CoordinatorCompactionConfig.class,
+            CoordinatorCompactionConfig.empty());
+    verify(mockConnector, times(4))
+        .lookup(anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY));
 
     // Should call set (to try set new updated compaction config) four times due to
     // RetryableException when failed
-    Mockito.verify(mockJacksonConfigManager, Mockito.times(4))
+    verify(mockJacksonConfigManager, times(4))
         .set(
-            ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-            ArgumentMatchers.any(byte[].class),
-            ArgumentMatchers.any(CoordinatorCompactionConfig.class),
-            ArgumentMatchers.any());
-    Mockito.verifyNoMoreInteractions(mockJacksonConfigManager);
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            any(byte[].class),
+            any(CoordinatorCompactionConfig.class),
+            any());
+    verifyNoMoreInteractions(mockJacksonConfigManager);
     // Should call retrieveAllDataSourceNames four times due to RetryableException when failed
-    Mockito.verify(mockSqlSegmentsMetadataManager, Mockito.times(4)).retrieveAllDataSourceNames();
-    Mockito.verifyNoMoreInteractions(mockSqlSegmentsMetadataManager);
+    verify(mockSqlSegmentsMetadataManager, times(4)).retrieveAllDataSourceNames();
+    verifyNoMoreInteractions(mockSqlSegmentsMetadataManager);
   }
 }
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillDatasourceMetadataTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillDatasourceMetadataTest.java
index 8741fc4a9a..6a222799d2 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillDatasourceMetadataTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillDatasourceMetadataTest.java
@@ -19,6 +19,14 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyLong;
+import static org.mockito.ArgumentMatchers.anySet;
+import static org.mockito.ArgumentMatchers.eq;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyNoInteractions;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableSet;
 import org.apache.druid.indexing.overlord.IndexerMetadataStorageCoordinator;
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
@@ -32,9 +40,7 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -69,13 +75,13 @@ public class KillDatasourceMetadataTest {
             mockIndexerMetadataStorageCoordinator,
             mockMetadataSupervisorManager);
     killDatasourceMetadata.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verifyNoInteractions(mockIndexerMetadataStorageCoordinator);
-    Mockito.verifyNoInteractions(mockMetadataSupervisorManager);
+    verifyNoInteractions(mockIndexerMetadataStorageCoordinator);
+    verifyNoInteractions(mockMetadataSupervisorManager);
   }
 
   @Test
   public void testRunNotSkipIfLastRunMoreThanPeriod() {
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
 
     TestDruidCoordinatorConfig druidCoordinatorConfig =
         new TestDruidCoordinatorConfig.Builder()
@@ -91,9 +97,9 @@ public class KillDatasourceMetadataTest {
             mockIndexerMetadataStorageCoordinator,
             mockMetadataSupervisorManager);
     killDatasourceMetadata.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verify(mockIndexerMetadataStorageCoordinator)
-        .removeDataSourceMetadataOlderThan(ArgumentMatchers.anyLong(), ArgumentMatchers.anySet());
-    Mockito.verify(mockServiceEmitter).emit(ArgumentMatchers.any(ServiceEventBuilder.class));
+    verify(mockIndexerMetadataStorageCoordinator)
+        .removeDataSourceMetadataOlderThan(anyLong(), anySet());
+    verify(mockServiceEmitter).emit(any(ServiceEventBuilder.class));
   }
 
   @Test
@@ -137,7 +143,7 @@ public class KillDatasourceMetadataTest {
 
   @Test
   public void testRunWithEmptyFilterExcludedDatasource() {
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
 
     TestDruidCoordinatorConfig druidCoordinatorConfig =
         new TestDruidCoordinatorConfig.Builder()
@@ -153,9 +159,8 @@ public class KillDatasourceMetadataTest {
             mockIndexerMetadataStorageCoordinator,
             mockMetadataSupervisorManager);
     killDatasourceMetadata.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verify(mockIndexerMetadataStorageCoordinator)
-        .removeDataSourceMetadataOlderThan(
-            ArgumentMatchers.anyLong(), ArgumentMatchers.eq(ImmutableSet.of()));
-    Mockito.verify(mockServiceEmitter).emit(ArgumentMatchers.any(ServiceEventBuilder.class));
+    verify(mockIndexerMetadataStorageCoordinator)
+        .removeDataSourceMetadataOlderThan(anyLong(), eq(ImmutableSet.of()));
+    verify(mockServiceEmitter).emit(any(ServiceEventBuilder.class));
   }
 }
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillRulesTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillRulesTest.java
index bddeb169a0..d9f0262eb2 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillRulesTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillRulesTest.java
@@ -19,6 +19,12 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyLong;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyNoInteractions;
+import static org.mockito.Mockito.when;
+
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
 import org.apache.druid.java.util.emitter.service.ServiceEventBuilder;
 import org.apache.druid.metadata.MetadataRuleManager;
@@ -30,9 +36,7 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -49,8 +53,7 @@ public class KillRulesTest {
 
   @Before
   public void setup() {
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getDatabaseRuleManager())
-        .thenReturn(mockRuleManager);
+    when(mockDruidCoordinatorRuntimeParams.getDatabaseRuleManager()).thenReturn(mockRuleManager);
   }
 
   @Test
@@ -65,12 +68,12 @@ public class KillRulesTest {
             .build();
     killRules = new KillRules(druidCoordinatorConfig);
     killRules.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verifyNoInteractions(mockRuleManager);
+    verifyNoInteractions(mockRuleManager);
   }
 
   @Test
   public void testRunNotSkipIfLastRunMoreThanPeriod() {
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
     TestDruidCoordinatorConfig druidCoordinatorConfig =
         new TestDruidCoordinatorConfig.Builder()
             .withMetadataStoreManagementPeriod(new Duration("PT5S"))
@@ -81,9 +84,8 @@ public class KillRulesTest {
             .build();
     killRules = new KillRules(druidCoordinatorConfig);
     killRules.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verify(mockRuleManager)
-        .removeRulesForEmptyDatasourcesOlderThan(ArgumentMatchers.anyLong());
-    Mockito.verify(mockServiceEmitter).emit(ArgumentMatchers.any(ServiceEventBuilder.class));
+    verify(mockRuleManager).removeRulesForEmptyDatasourcesOlderThan(anyLong());
+    verify(mockServiceEmitter).emit(any(ServiceEventBuilder.class));
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillSupervisorsCustomDutyTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillSupervisorsCustomDutyTest.java
index 90d8ff189d..98ebf60499 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillSupervisorsCustomDutyTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillSupervisorsCustomDutyTest.java
@@ -19,6 +19,11 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyLong;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
 import org.apache.druid.java.util.emitter.service.ServiceEventBuilder;
 import org.apache.druid.metadata.MetadataSupervisorManager;
@@ -29,9 +34,7 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -72,12 +75,11 @@ public class KillSupervisorsCustomDutyTest {
 
   @Test
   public void testRun() {
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
     killSupervisors =
         new KillSupervisorsCustomDuty(new Duration("PT1S"), mockMetadataSupervisorManager);
     killSupervisors.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verify(mockMetadataSupervisorManager)
-        .removeTerminatedSupervisorsOlderThan(ArgumentMatchers.anyLong());
-    Mockito.verify(mockServiceEmitter).emit(ArgumentMatchers.any(ServiceEventBuilder.class));
+    verify(mockMetadataSupervisorManager).removeTerminatedSupervisorsOlderThan(anyLong());
+    verify(mockServiceEmitter).emit(any(ServiceEventBuilder.class));
   }
 }
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillSupervisorsTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillSupervisorsTest.java
index dc512e403e..f081db47da 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillSupervisorsTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillSupervisorsTest.java
@@ -19,6 +19,12 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyLong;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyNoInteractions;
+import static org.mockito.Mockito.when;
+
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
 import org.apache.druid.java.util.emitter.service.ServiceEventBuilder;
 import org.apache.druid.metadata.MetadataSupervisorManager;
@@ -29,9 +35,7 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
 import org.junit.runner.RunWith;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -58,12 +62,12 @@ public class KillSupervisorsTest {
             .build();
     killSupervisors = new KillSupervisors(druidCoordinatorConfig, mockMetadataSupervisorManager);
     killSupervisors.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verifyNoInteractions(mockMetadataSupervisorManager);
+    verifyNoInteractions(mockMetadataSupervisorManager);
   }
 
   @Test
   public void testRunNotSkipIfLastRunMoreThanPeriod() {
-    Mockito.when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
+    when(mockDruidCoordinatorRuntimeParams.getEmitter()).thenReturn(mockServiceEmitter);
     TestDruidCoordinatorConfig druidCoordinatorConfig =
         new TestDruidCoordinatorConfig.Builder()
             .withMetadataStoreManagementPeriod(new Duration("PT5S"))
@@ -74,9 +78,8 @@ public class KillSupervisorsTest {
             .build();
     killSupervisors = new KillSupervisors(druidCoordinatorConfig, mockMetadataSupervisorManager);
     killSupervisors.run(mockDruidCoordinatorRuntimeParams);
-    Mockito.verify(mockMetadataSupervisorManager)
-        .removeTerminatedSupervisorsOlderThan(ArgumentMatchers.anyLong());
-    Mockito.verify(mockServiceEmitter).emit(ArgumentMatchers.any(ServiceEventBuilder.class));
+    verify(mockMetadataSupervisorManager).removeTerminatedSupervisorsOlderThan(anyLong());
+    verify(mockServiceEmitter).emit(any(ServiceEventBuilder.class));
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillUnusedSegmentsTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillUnusedSegmentsTest.java
index 7532a0e520..0db91ea1ee 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/KillUnusedSegmentsTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/KillUnusedSegmentsTest.java
@@ -19,15 +19,22 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static java.util.Collections.singleton;
+import static java.util.stream.Collectors.toList;
+import static org.mockito.Answers.RETURNS_DEEP_STUBS;
 import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyInt;
 import static org.mockito.ArgumentMatchers.anyString;
+import static org.mockito.ArgumentMatchers.eq;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
 
 import com.google.common.collect.ImmutableList;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
-import java.util.stream.Collectors;
 import org.apache.druid.client.indexing.IndexingServiceClient;
 import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.metadata.SegmentsMetadataManager;
@@ -43,10 +50,7 @@ import org.joda.time.Period;
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
-import org.mockito.Answers;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 /** */
@@ -60,7 +64,7 @@ public class KillUnusedSegmentsTest {
   @Mock private SegmentsMetadataManager segmentsMetadataManager;
   @Mock private IndexingServiceClient indexingServiceClient;
 
-  @Mock(answer = Answers.RETURNS_DEEP_STUBS)
+  @Mock(answer = RETURNS_DEEP_STUBS)
   private DruidCoordinatorConfig config;
 
   @Mock private DruidCoordinatorRuntimeParams params;
@@ -77,13 +81,13 @@ public class KillUnusedSegmentsTest {
 
   @Before
   public void setup() {
-    Mockito.doReturn(coordinatorDynamicConfig).when(params).getCoordinatorDynamicConfig();
-    Mockito.doReturn(COORDINATOR_KILL_PERIOD).when(config).getCoordinatorKillPeriod();
-    Mockito.doReturn(DURATION_TO_RETAIN).when(config).getCoordinatorKillDurationToRetain();
-    Mockito.doReturn(INDEXING_PERIOD).when(config).getCoordinatorIndexingPeriod();
-    Mockito.doReturn(MAX_SEGMENTS_TO_KILL).when(config).getCoordinatorKillMaxSegments();
+    doReturn(coordinatorDynamicConfig).when(params).getCoordinatorDynamicConfig();
+    doReturn(COORDINATOR_KILL_PERIOD).when(config).getCoordinatorKillPeriod();
+    doReturn(DURATION_TO_RETAIN).when(config).getCoordinatorKillDurationToRetain();
+    doReturn(INDEXING_PERIOD).when(config).getCoordinatorIndexingPeriod();
+    doReturn(MAX_SEGMENTS_TO_KILL).when(config).getCoordinatorKillMaxSegments();
 
-    Mockito.doReturn(Collections.singleton("DS1"))
+    doReturn(singleton("DS1"))
         .when(coordinatorDynamicConfig)
         .getSpecificDataSourcesToKillUnusedSegmentsIn();
 
@@ -105,9 +109,7 @@ public class KillUnusedSegmentsTest {
             nextDaySegment,
             nextMonthSegment);
 
-    Mockito.when(
-            segmentsMetadataManager.getUnusedSegmentIntervals(
-                ArgumentMatchers.anyString(), ArgumentMatchers.any(), ArgumentMatchers.anyInt()))
+    when(segmentsMetadataManager.getUnusedSegmentIntervals(anyString(), any(), anyInt()))
         .thenAnswer(
             invocation -> {
               DateTime maxEndTime = invocation.getArgument(1);
@@ -116,7 +118,7 @@ public class KillUnusedSegmentsTest {
                   unusedSegments.stream()
                       .map(DataSegment::getInterval)
                       .filter(i -> i.getEnd().getMillis() <= maxEndMillis)
-                      .collect(Collectors.toList());
+                      .collect(toList());
 
               int limit = invocation.getArgument(2);
               return unusedIntervals.size() <= limit
@@ -129,24 +131,23 @@ public class KillUnusedSegmentsTest {
 
   @Test
   public void testRunWithNoIntervalShouldNotKillAnySegments() {
-    Mockito.doReturn(null)
+    doReturn(null)
         .when(segmentsMetadataManager)
-        .getUnusedSegmentIntervals(
-            ArgumentMatchers.anyString(), ArgumentMatchers.any(), ArgumentMatchers.anyInt());
+        .getUnusedSegmentIntervals(anyString(), any(), anyInt());
 
     target.run(params);
-    Mockito.verify(indexingServiceClient, Mockito.never())
+    verify(indexingServiceClient, never())
         .killUnusedSegments(anyString(), anyString(), any(Interval.class));
   }
 
   @Test
   public void testRunWithSpecificDatasourceAndNoIntervalShouldNotKillAnySegments() {
-    Mockito.doReturn(Duration.standardDays(400)).when(config).getCoordinatorKillDurationToRetain();
+    doReturn(Duration.standardDays(400)).when(config).getCoordinatorKillDurationToRetain();
     target = new KillUnusedSegments(segmentsMetadataManager, indexingServiceClient, config);
 
     // No unused segment is older than the retention period
     target.run(params);
-    Mockito.verify(indexingServiceClient, Mockito.never())
+    verify(indexingServiceClient, never())
         .killUnusedSegments(anyString(), anyString(), any(Interval.class));
   }
 
@@ -161,9 +162,7 @@ public class KillUnusedSegmentsTest {
   @Test
   public void testNegativeDurationToRetain() {
     // Duration to retain = -1 day, reinit target for config to take effect
-    Mockito.doReturn(DURATION_TO_RETAIN.negated())
-        .when(config)
-        .getCoordinatorKillDurationToRetain();
+    doReturn(DURATION_TO_RETAIN.negated()).when(config).getCoordinatorKillDurationToRetain();
     target = new KillUnusedSegments(segmentsMetadataManager, indexingServiceClient, config);
 
     // Segments upto 1 day in the future are killed
@@ -175,7 +174,7 @@ public class KillUnusedSegmentsTest {
 
   @Test
   public void testIgnoreDurationToRetain() {
-    Mockito.doReturn(true).when(config).getCoordinatorKillIgnoreDurationToRetain();
+    doReturn(true).when(config).getCoordinatorKillIgnoreDurationToRetain();
     target = new KillUnusedSegments(segmentsMetadataManager, indexingServiceClient, config);
 
     // All future and past unused segments are killed
@@ -187,7 +186,7 @@ public class KillUnusedSegmentsTest {
 
   @Test
   public void testMaxSegmentsToKill() {
-    Mockito.doReturn(1).when(config).getCoordinatorKillMaxSegments();
+    doReturn(1).when(config).getCoordinatorKillMaxSegments();
     target = new KillUnusedSegments(segmentsMetadataManager, indexingServiceClient, config);
 
     // Only 1 unused segment is killed
@@ -196,11 +195,8 @@ public class KillUnusedSegmentsTest {
 
   private void runAndVerifyKillInterval(Interval expectedKillInterval) {
     target.run(params);
-    Mockito.verify(indexingServiceClient, Mockito.times(1))
-        .killUnusedSegments(
-            ArgumentMatchers.anyString(),
-            ArgumentMatchers.eq("DS1"),
-            ArgumentMatchers.eq(expectedKillInterval));
+    verify(indexingServiceClient)
+        .killUnusedSegments(anyString(), eq("DS1"), eq(expectedKillInterval));
   }
 
   private DataSegment createSegmentWithEnd(DateTime endTime) {
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/MarkAsUnusedOvershadowedSegmentsTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/MarkAsUnusedOvershadowedSegmentsTest.java
index e131b35eef..7354d8e65d 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/MarkAsUnusedOvershadowedSegmentsTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/MarkAsUnusedOvershadowedSegmentsTest.java
@@ -77,8 +77,8 @@ public class MarkAsUnusedOvershadowedSegmentsTest {
           .size(0)
           .build();
 
+  @Parameters({"broker", "historical"})
   @Test
-  @Parameters({"historical", "broker"})
   public void testRun(String serverTypeString) {
     ServerType serverType = ServerType.fromString(serverTypeString);
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstPolicyTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstPolicyTest.java
index 7d3ac210c7..600ab81680 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstPolicyTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstPolicyTest.java
@@ -19,23 +19,26 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static java.util.Collections.emptyMap;
+import static java.util.Comparator.comparing;
+import static java.util.Comparator.naturalOrder;
+import static java.util.stream.Collectors.toList;
+import static org.assertj.core.api.Assertions.assertThat;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.InjectableValues;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.TimeZone;
-import java.util.stream.Collectors;
 import org.apache.druid.client.indexing.ClientCompactionTaskQueryTuningConfig;
 import org.apache.druid.common.config.NullHandling;
 import org.apache.druid.data.input.impl.DimensionsSpec;
@@ -67,7 +70,6 @@ import org.apache.druid.timeline.Partitions;
 import org.apache.druid.timeline.SegmentTimeline;
 import org.apache.druid.timeline.partition.NumberedShardSpec;
 import org.apache.druid.timeline.partition.ShardSpec;
-import org.assertj.core.api.Assertions;
 import org.joda.time.DateTimeZone;
 import org.joda.time.Interval;
 import org.joda.time.Period;
@@ -94,7 +96,7 @@ public class NewestSegmentFirstPolicyTest {
                         Intervals.of("2017-11-16T20:00:00/2017-11-17T04:00:00"), segmentPeriod),
                     new SegmentGenerateSpec(
                         Intervals.of("2017-11-14T00:00:00/2017-11-16T07:00:00"), segmentPeriod))),
-            Collections.emptyMap());
+            emptyMap());
 
     assertCompactSegmentIntervals(
         iterator,
@@ -117,7 +119,7 @@ public class NewestSegmentFirstPolicyTest {
                         Intervals.of("2017-11-16T20:00:00/2017-11-17T04:00:00"), segmentPeriod),
                     new SegmentGenerateSpec(
                         Intervals.of("2017-11-14T00:00:00/2017-11-16T07:00:00"), segmentPeriod))),
-            Collections.emptyMap());
+            emptyMap());
 
     assertCompactSegmentIntervals(
         iterator,
@@ -148,7 +150,7 @@ public class NewestSegmentFirstPolicyTest {
                     // larger gap than SegmentCompactionUtil.LOOKUP_PERIOD (1 day)
                     new SegmentGenerateSpec(
                         Intervals.of("2017-11-14T00:00:00/2017-11-15T07:00:00"), segmentPeriod))),
-            Collections.emptyMap());
+            emptyMap());
 
     assertCompactSegmentIntervals(
         iterator,
@@ -188,7 +190,7 @@ public class NewestSegmentFirstPolicyTest {
                         new Period("PT1H"),
                         200,
                         DEFAULT_NUM_SEGMENTS_PER_SHARD))),
-            Collections.emptyMap());
+            emptyMap());
 
     Interval lastInterval = null;
     while (iterator.hasNext()) {
@@ -238,7 +240,7 @@ public class NewestSegmentFirstPolicyTest {
                         new Period("PT1H"),
                         375,
                         80))),
-            Collections.emptyMap());
+            emptyMap());
 
     Interval lastInterval = null;
     while (iterator.hasNext()) {
@@ -278,7 +280,7 @@ public class NewestSegmentFirstPolicyTest {
                         Intervals.of("2017-11-16T20:00:00/2017-11-17T04:00:00"), segmentPeriod),
                     new SegmentGenerateSpec(
                         Intervals.of("2017-11-14T00:00:00/2017-11-16T07:00:00"), segmentPeriod))),
-            Collections.emptyMap());
+            emptyMap());
 
     assertCompactSegmentIntervals(
         iterator,
@@ -314,21 +316,21 @@ public class NewestSegmentFirstPolicyTest {
                 DATA_SOURCE,
                 createCompactionConfig(inputSegmentSizeBytes, new Period("P0D"), null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     final List<DataSegment> expectedSegmentsToCompact =
         new ArrayList<>(
             timeline.findNonOvershadowedObjectsInInterval(
                 Intervals.of("2017-12-03/2017-12-04"), Partitions.ONLY_COMPLETE));
-    expectedSegmentsToCompact.sort(Comparator.naturalOrder());
+    expectedSegmentsToCompact.sort(naturalOrder());
 
     final List<DataSegment> expectedSegmentsToCompact2 =
         new ArrayList<>(
             timeline.findNonOvershadowedObjectsInInterval(
                 Intervals.of("2017-12-01/2017-12-02"), Partitions.ONLY_COMPLETE));
-    expectedSegmentsToCompact2.sort(Comparator.naturalOrder());
+    expectedSegmentsToCompact2.sort(naturalOrder());
 
-    Assertions.assertThat(iterator)
+    assertThat(iterator)
         .toIterable()
         .containsExactly(expectedSegmentsToCompact, expectedSegmentsToCompact2);
   }
@@ -347,7 +349,7 @@ public class NewestSegmentFirstPolicyTest {
         policy.reset(
             ImmutableMap.of(DATA_SOURCE, createCompactionConfig(40000, new Period("P1D"), null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     Assert.assertFalse(iterator.hasNext());
   }
@@ -366,7 +368,7 @@ public class NewestSegmentFirstPolicyTest {
         policy.reset(
             ImmutableMap.of(DATA_SOURCE, createCompactionConfig(40000, new Period("P1D"), null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     Assert.assertFalse(iterator.hasNext());
   }
@@ -389,7 +391,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P1D"),
                     new UserCompactionTaskGranularityConfig(Granularities.DAY, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     // We should only get segments in Oct
     final List<DataSegment> expectedSegmentsToCompact =
@@ -427,7 +429,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P1D"),
                     new UserCompactionTaskGranularityConfig(Granularities.MONTH, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     // We should only get segments in Oct
     final List<DataSegment> expectedSegmentsToCompact =
@@ -461,7 +463,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P1D"),
                     new UserCompactionTaskGranularityConfig(Granularities.MINUTE, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     // We should only get segments in Oct
     final List<DataSegment> expectedSegmentsToCompact =
@@ -566,7 +568,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.MONTH, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     // We should get all segments in timeline back since skip offset is P0D.
     // However, we only need to iterator 3 times (once for each month) since the new configured
@@ -618,7 +620,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.MONTH, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get the segment of "2020-01-28/2020-02-03" back twice when the iterator returns for
     // Jan and when the
     // iterator returns for Feb.
@@ -663,7 +665,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.MINUTE, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     final List<DataSegment> expectedSegmentsToCompact =
         new ArrayList<>(
@@ -701,7 +703,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.MONTH, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     // We should get all segments in timeline back since skip offset is P0D.
     Assert.assertTrue(iterator.hasNext());
@@ -749,7 +751,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.DAY, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     Assert.assertFalse(iterator.hasNext());
   }
 
@@ -799,7 +801,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.DAY, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     Assert.assertFalse(iterator.hasNext());
   }
 
@@ -837,7 +839,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.YEAR, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get all segments in timeline back since skip offset is P0D.
     Assert.assertTrue(iterator.hasNext());
     List<DataSegment> expectedSegmentsToCompact =
@@ -896,7 +898,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.YEAR, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get all segments in timeline back since skip offset is P0D.
     Assert.assertTrue(iterator.hasNext());
     List<DataSegment> expectedSegmentsToCompact =
@@ -944,7 +946,7 @@ public class NewestSegmentFirstPolicyTest {
                         null,
                         null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get all segments in timeline back since skip offset is P0D.
     Assert.assertTrue(iterator.hasNext());
     List<DataSegment> expectedSegmentsToCompact =
@@ -992,7 +994,7 @@ public class NewestSegmentFirstPolicyTest {
                         null,
                         null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get all segments in timeline back since skip offset is P0D.
     Assert.assertTrue(iterator.hasNext());
     List<DataSegment> expectedSegmentsToCompact =
@@ -1060,7 +1062,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(null, null, true))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get interval 2017-10-01T00:00:00/2017-10-02T00:00:00 and interval
     // 2017-10-03T00:00:00/2017-10-04T00:00:00.
     Assert.assertTrue(iterator.hasNext());
@@ -1136,7 +1138,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(null, Granularities.MINUTE, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get interval 2017-10-01T00:00:00/2017-10-02T00:00:00 and interval
     // 2017-10-03T00:00:00/2017-10-04T00:00:00.
     Assert.assertTrue(iterator.hasNext());
@@ -1224,7 +1226,7 @@ public class NewestSegmentFirstPolicyTest {
                     null,
                     null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get interval 2017-10-01T00:00:00/2017-10-02T00:00:00, interval
     // 2017-10-04T00:00:00/2017-10-05T00:00:00, and interval
     // 2017-10-03T00:00:00/2017-10-04T00:00:00.
@@ -1265,7 +1267,7 @@ public class NewestSegmentFirstPolicyTest {
                     null,
                     null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // No more
     Assert.assertFalse(iterator.hasNext());
   }
@@ -1353,7 +1355,7 @@ public class NewestSegmentFirstPolicyTest {
                         new SelectorDimFilter("dim1", "bar", null)),
                     null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get interval 2017-10-01T00:00:00/2017-10-02T00:00:00, interval
     // 2017-10-04T00:00:00/2017-10-05T00:00:00, and interval
     // 2017-10-03T00:00:00/2017-10-04T00:00:00.
@@ -1394,7 +1396,7 @@ public class NewestSegmentFirstPolicyTest {
                     new UserCompactionTaskTransformConfig(null),
                     null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // No more
     Assert.assertFalse(iterator.hasNext());
   }
@@ -1486,7 +1488,7 @@ public class NewestSegmentFirstPolicyTest {
                       new CountAggregatorFactory("cnt"), new LongSumAggregatorFactory("val", "val")
                     })),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get interval 2017-10-01T00:00:00/2017-10-02T00:00:00, interval
     // 2017-10-04T00:00:00/2017-10-05T00:00:00, and interval
     // 2017-10-03T00:00:00/2017-10-04T00:00:00.
@@ -1521,7 +1523,7 @@ public class NewestSegmentFirstPolicyTest {
                 DATA_SOURCE,
                 createCompactionConfig(130000, new Period("P0D"), null, null, null, null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // No more
     Assert.assertFalse(iterator.hasNext());
   }
@@ -1551,7 +1553,7 @@ public class NewestSegmentFirstPolicyTest {
                     new Period("P0D"),
                     new UserCompactionTaskGranularityConfig(Granularities.HOUR, null, null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
 
     // We should get all segments in timeline back since skip offset is P0D.
     // Although the first iteration only covers the last hour of 2017-10-01
@@ -1604,7 +1606,7 @@ public class NewestSegmentFirstPolicyTest {
                         null,
                         null))),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     // We should get all segments in timeline back since indexSpec changed
     Assert.assertTrue(iterator.hasNext());
     List<DataSegment> expectedSegmentsToCompact =
@@ -1664,7 +1666,7 @@ public class NewestSegmentFirstPolicyTest {
                         null),
                     null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     Assert.assertFalse(iterator.hasNext());
 
     iterator =
@@ -1699,7 +1701,7 @@ public class NewestSegmentFirstPolicyTest {
                         null),
                     null)),
             ImmutableMap.of(DATA_SOURCE, timeline),
-            Collections.emptyMap());
+            emptyMap());
     Assert.assertFalse(iterator.hasNext());
   }
 
@@ -1722,7 +1724,7 @@ public class NewestSegmentFirstPolicyTest {
                             new NumberedShardSpec(0, 0),
                             0,
                             100)))),
-            Collections.emptyMap());
+            emptyMap());
 
     Assert.assertFalse(iterator.hasNext());
   }
@@ -1751,7 +1753,7 @@ public class NewestSegmentFirstPolicyTest {
                             new NumberedShardSpec(0, 0),
                             0,
                             100)))),
-            Collections.emptyMap());
+            emptyMap());
 
     Assert.assertFalse(iterator.hasNext());
   }
@@ -1780,7 +1782,7 @@ public class NewestSegmentFirstPolicyTest {
                             new NumberedShardSpec(0, 0),
                             0,
                             100)))),
-            Collections.emptyMap());
+            emptyMap());
 
     Assert.assertFalse(iterator.hasNext());
   }
@@ -1815,8 +1817,7 @@ public class NewestSegmentFirstPolicyTest {
       expectedIntervals.sort(Comparators.intervalsByStartThenEnd());
 
       Assert.assertEquals(
-          expectedIntervals,
-          segments.stream().map(DataSegment::getInterval).collect(Collectors.toList()));
+          expectedIntervals, segments.stream().map(DataSegment::getInterval).collect(toList()));
 
       if (expectedSegmentIntervalStart.equals(from)) {
         break;
@@ -1836,8 +1837,7 @@ public class NewestSegmentFirstPolicyTest {
 
     final List<SegmentGenerateSpec> orderedSpecs = Arrays.asList(specs);
     orderedSpecs.sort(
-        Comparator.comparing(
-            s -> s.totalInterval, Comparators.intervalsByStartThenEnd().reversed()));
+        comparing(s -> s.totalInterval, Comparators.intervalsByStartThenEnd().reversed()));
 
     for (SegmentGenerateSpec spec : orderedSpecs) {
       Interval remainingInterval = spec.totalInterval;
@@ -1961,7 +1961,7 @@ public class NewestSegmentFirstPolicyTest {
         int numSegmentsPerShard,
         String version,
         CompactionState lastCompactionState) {
-      Preconditions.checkArgument(numSegmentsPerShard >= 1);
+      checkArgument(numSegmentsPerShard >= 1);
       this.totalInterval = totalInterval;
       this.segmentPeriod = segmentPeriod;
       this.segmentSize = segmentSize;
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/duty/UnloadUnusedSegmentsTest.java b/server/src/test/java/org/apache/druid/server/coordinator/duty/UnloadUnusedSegmentsTest.java
index 158a875119..c73f34477b 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/duty/UnloadUnusedSegmentsTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/duty/UnloadUnusedSegmentsTest.java
@@ -19,11 +19,14 @@
 
 package org.apache.druid.server.coordinator.duty;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singleton;
+import static java.util.Collections.singletonList;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableSet;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Set;
@@ -151,17 +154,13 @@ public class UnloadUnusedSegmentsTest {
     brokerPeon = new LoadQueuePeonTester();
     indexerPeon = new LoadQueuePeonTester();
 
-    dataSource1 =
-        new ImmutableDruidDataSource(
-            "datasource1", Collections.emptyMap(), Collections.singleton(segment1));
-    dataSource2 =
-        new ImmutableDruidDataSource(
-            "datasource2", Collections.emptyMap(), Collections.singleton(segment2));
+    dataSource1 = new ImmutableDruidDataSource("datasource1", emptyMap(), singleton(segment1));
+    dataSource2 = new ImmutableDruidDataSource("datasource2", emptyMap(), singleton(segment2));
 
-    broadcastDatasourceNames = Collections.singleton("broadcastDatasource");
+    broadcastDatasourceNames = singleton("broadcastDatasource");
     broadcastDatasource =
         new ImmutableDruidDataSource(
-            "broadcastDatasource", Collections.emptyMap(), Collections.singleton(broadcastSegment));
+            "broadcastDatasource", emptyMap(), singleton(broadcastSegment));
 
     dataSources = ImmutableList.of(dataSource1, dataSource2, broadcastDatasource);
 
@@ -169,8 +168,7 @@ public class UnloadUnusedSegmentsTest {
     // unpublished segments,
     // while also having a broadcast segment loaded.
     dataSource2ForRealtime =
-        new ImmutableDruidDataSource(
-            "datasource2", Collections.emptyMap(), Collections.singleton(realtimeOnlySegment));
+        new ImmutableDruidDataSource("datasource2", emptyMap(), singleton(realtimeOnlySegment));
     dataSourcesForRealtime = ImmutableList.of(dataSource2ForRealtime, broadcastDatasource);
   }
 
@@ -306,18 +304,18 @@ public class UnloadUnusedSegmentsTest {
   private static void mockRuleManager(MetadataRuleManager metadataRuleManager) {
     EasyMock.expect(metadataRuleManager.getRulesWithDefault("datasource1"))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, 1, "tier2", 1))))
         .anyTimes();
 
     EasyMock.expect(metadataRuleManager.getRulesWithDefault("datasource2"))
         .andReturn(
-            Collections.singletonList(
+            singletonList(
                 new ForeverLoadRule(ImmutableMap.of(DruidServer.DEFAULT_TIER, 1, "tier2", 1))))
         .anyTimes();
 
     EasyMock.expect(metadataRuleManager.getRulesWithDefault("broadcastDatasource"))
-        .andReturn(Collections.singletonList(new ForeverBroadcastDistributionRule()))
+        .andReturn(singletonList(new ForeverBroadcastDistributionRule()))
         .anyTimes();
 
     EasyMock.replay(metadataRuleManager);
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleSerdeTest.java b/server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleSerdeTest.java
index 5a2933f4b2..f71329c2e7 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleSerdeTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleSerdeTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server.coordinator.rules;
 
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.Lists;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import org.apache.druid.jackson.DefaultObjectMapper;
 import org.apache.druid.java.util.common.Intervals;
@@ -53,7 +54,7 @@ public class BroadcastDistributionRuleSerdeTest {
 
   @Test
   public void testSerde() throws IOException {
-    final List<Rule> rules = Collections.singletonList(testRule);
+    final List<Rule> rules = singletonList(testRule);
     final String json = MAPPER.writeValueAsString(rules);
     final List<Rule> fromJson = MAPPER.readValue(json, new TypeReference<List<Rule>>() {});
     Assert.assertEquals(rules, fromJson);
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBaseTest.java b/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBaseTest.java
index d704955a57..9310d98e71 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBaseTest.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBaseTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.coordinator.simulate;
 
+import static com.google.common.base.Preconditions.checkArgument;
+
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -155,9 +157,9 @@ public abstract class CoordinatorSimulationBaseTest
 
   /** Creates a map containing dimension key-values to filter out metric events. */
   static Map<String, Object> filter(String... dimensionValues) {
-    if (dimensionValues.length < 2 || dimensionValues.length % 2 == 1) {
-      throw new IllegalArgumentException("Dimension key-values must be specified in pairs.");
-    }
+    checkArgument(
+        dimensionValues.length >= 2 && dimensionValues.length % 2 != 1,
+        "Dimension key-values must be specified in pairs.");
 
     final Map<String, Object> filters = new HashMap<>();
     for (int i = 0; i < dimensionValues.length; ) {
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBuilder.java b/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBuilder.java
index 190fcc08c5..6dc14699a5 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBuilder.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/simulate/CoordinatorSimulationBuilder.java
@@ -19,12 +19,14 @@
 
 package org.apache.druid.server.coordinator.simulate;
 
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkState;
+import static java.util.Collections.emptySet;
+
 import com.fasterxml.jackson.databind.InjectableValues;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -153,7 +155,7 @@ public class CoordinatorSimulationBuilder {
   }
 
   public CoordinatorSimulation build() {
-    Preconditions.checkArgument(
+    checkArgument(
         servers != null && !servers.isEmpty(), "Cannot run simulation for an empty cluster");
 
     // Prepare the environment
@@ -183,9 +185,9 @@ public class CoordinatorSimulationBuilder {
             env.loadQueueTaskMaster,
             new ServiceAnnouncer.Noop(),
             null,
-            Collections.emptySet(),
+            emptySet(),
             null,
-            new CoordinatorCustomDutyGroups(Collections.emptySet()),
+            new CoordinatorCustomDutyGroups(emptySet()),
             createBalancerStrategy(env),
             env.lookupCoordinatorManager,
             env.leaderSelector,
@@ -280,7 +282,7 @@ public class CoordinatorSimulationBuilder {
     @Override
     public void syncInventoryView() {
       verifySimulationRunning();
-      Preconditions.checkState(
+      checkState(
           !env.autoSyncInventory,
           "Cannot invoke syncInventoryView as simulation is running in auto-sync mode.");
       env.coordinatorInventoryView.sync(env.inventory);
@@ -299,7 +301,7 @@ public class CoordinatorSimulationBuilder {
     @Override
     public void loadQueuedSegments() {
       verifySimulationRunning();
-      Preconditions.checkState(
+      checkState(
           !env.loadImmediately,
           "Cannot invoke loadQueuedSegments as simulation is running in immediate loading mode.");
 
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestMetadataRuleManager.java b/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestMetadataRuleManager.java
index 4112e08499..38c26684f7 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestMetadataRuleManager.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestMetadataRuleManager.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.server.coordinator.simulate;
 
+import static java.util.Collections.singletonList;
+
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -35,7 +36,7 @@ public class TestMetadataRuleManager implements MetadataRuleManager {
   private static final String DEFAULT_DATASOURCE = "_default";
 
   public TestMetadataRuleManager() {
-    rules.put(DEFAULT_DATASOURCE, Collections.singletonList(new ForeverLoadRule(null)));
+    rules.put(DEFAULT_DATASOURCE, singletonList(new ForeverLoadRule(null)));
   }
 
   @Override
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestSegmentLoadingHttpClient.java b/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestSegmentLoadingHttpClient.java
index 09eaddef22..3e05a02e69 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestSegmentLoadingHttpClient.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestSegmentLoadingHttpClient.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.coordinator.simulate;
 
+import static java.util.stream.Collectors.toList;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.util.concurrent.ListenableFuture;
@@ -30,7 +32,6 @@ import java.io.IOException;
 import java.util.List;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 import org.apache.druid.java.util.http.client.HttpClient;
 import org.apache.druid.java.util.http.client.Request;
 import org.apache.druid.java.util.http.client.response.HttpResponseHandler;
@@ -116,7 +117,7 @@ public class TestSegmentLoadingHttpClient implements HttpClient {
 
     return changeRequests.stream()
         .map(changeRequest -> processRequest(changeRequest, changeHandler))
-        .collect(Collectors.toList());
+        .collect(toList());
   }
 
   /** Processes each DataSegmentChangeRequest using the handler. */
diff --git a/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestServerInventoryView.java b/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestServerInventoryView.java
index 480c667bae..903a96390b 100644
--- a/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestServerInventoryView.java
+++ b/server/src/test/java/org/apache/druid/server/coordinator/simulate/TestServerInventoryView.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.coordinator.simulate;
 
+import static java.util.Collections.unmodifiableCollection;
+
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.Executor;
@@ -102,7 +103,7 @@ public class TestServerInventoryView implements ServerInventoryView {
 
   @Override
   public Collection<DruidServer> getInventory() {
-    return Collections.unmodifiableCollection(servers.values());
+    return unmodifiableCollection(servers.values());
   }
 
   @Override
diff --git a/server/src/test/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResourceTest.java b/server/src/test/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResourceTest.java
index d28ef48002..12eea3e243 100644
--- a/server/src/test/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResourceTest.java
+++ b/server/src/test/java/org/apache/druid/server/http/CoordinatorCompactionConfigsResourceTest.java
@@ -19,6 +19,11 @@
 
 package org.apache.druid.server.http;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.ArgumentMatchers.anyString;
+import static org.mockito.ArgumentMatchers.eq;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.util.Collection;
@@ -42,9 +47,7 @@ import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.mockito.ArgumentCaptor;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -84,25 +87,19 @@ public class CoordinatorCompactionConfigsResourceTest {
 
   @Before
   public void setup() {
-    Mockito.when(
-            mockConnector.lookup(
-                ArgumentMatchers.anyString(),
-                ArgumentMatchers.eq("name"),
-                ArgumentMatchers.eq("payload"),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY)))
+    when(mockConnector.lookup(
+            anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY)))
         .thenReturn(OLD_CONFIG_IN_BYTES);
-    Mockito.when(
-            mockJacksonConfigManager.convertByteToConfig(
-                ArgumentMatchers.eq(OLD_CONFIG_IN_BYTES),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.empty())))
+    when(mockJacksonConfigManager.convertByteToConfig(
+            OLD_CONFIG_IN_BYTES,
+            CoordinatorCompactionConfig.class,
+            CoordinatorCompactionConfig.empty()))
         .thenReturn(ORIGINAL_CONFIG);
-    Mockito.when(mockConnectorConfig.getConfigTable()).thenReturn("druid_config");
-    Mockito.when(
-            mockAuditManager.fetchAuditHistory(
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                ArgumentMatchers.any()))
+    when(mockConnectorConfig.getConfigTable()).thenReturn("druid_config");
+    when(mockAuditManager.fetchAuditHistory(
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            any()))
         .thenReturn(ImmutableList.of());
     coordinatorCompactionConfigsResource =
         new CoordinatorCompactionConfigsResource(
@@ -111,7 +108,7 @@ public class CoordinatorCompactionConfigsResourceTest {
             mockConnectorConfig,
             mockAuditManager,
             new DefaultObjectMapper());
-    Mockito.when(mockHttpServletRequest.getRemoteAddr()).thenReturn("123");
+    when(mockHttpServletRequest.getRemoteAddr()).thenReturn("123");
   }
 
   @Test
@@ -119,12 +116,11 @@ public class CoordinatorCompactionConfigsResourceTest {
     final ArgumentCaptor<byte[]> oldConfigCaptor = ArgumentCaptor.forClass(byte[].class);
     final ArgumentCaptor<CoordinatorCompactionConfig> newConfigCaptor =
         ArgumentCaptor.forClass(CoordinatorCompactionConfig.class);
-    Mockito.when(
-            mockJacksonConfigManager.set(
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                oldConfigCaptor.capture(),
-                newConfigCaptor.capture(),
-                ArgumentMatchers.any()))
+    when(mockJacksonConfigManager.set(
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            oldConfigCaptor.capture(),
+            newConfigCaptor.capture(),
+            any()))
         .thenReturn(ConfigManager.SetResult.ok());
 
     double compactionTaskSlotRatio = 0.5;
@@ -155,12 +151,11 @@ public class CoordinatorCompactionConfigsResourceTest {
     final ArgumentCaptor<byte[]> oldConfigCaptor = ArgumentCaptor.forClass(byte[].class);
     final ArgumentCaptor<CoordinatorCompactionConfig> newConfigCaptor =
         ArgumentCaptor.forClass(CoordinatorCompactionConfig.class);
-    Mockito.when(
-            mockJacksonConfigManager.set(
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                oldConfigCaptor.capture(),
-                newConfigCaptor.capture(),
-                ArgumentMatchers.any()))
+    when(mockJacksonConfigManager.set(
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            oldConfigCaptor.capture(),
+            newConfigCaptor.capture(),
+            any()))
         .thenReturn(ConfigManager.SetResult.ok());
 
     final DataSourceCompactionConfig newConfig =
@@ -196,12 +191,11 @@ public class CoordinatorCompactionConfigsResourceTest {
     final ArgumentCaptor<byte[]> oldConfigCaptor = ArgumentCaptor.forClass(byte[].class);
     final ArgumentCaptor<CoordinatorCompactionConfig> newConfigCaptor =
         ArgumentCaptor.forClass(CoordinatorCompactionConfig.class);
-    Mockito.when(
-            mockJacksonConfigManager.set(
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                oldConfigCaptor.capture(),
-                newConfigCaptor.capture(),
-                ArgumentMatchers.any()))
+    when(mockJacksonConfigManager.set(
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            oldConfigCaptor.capture(),
+            newConfigCaptor.capture(),
+            any()))
         .thenReturn(ConfigManager.SetResult.ok());
     final String datasourceName = "dataSource";
     final DataSourceCompactionConfig toDelete =
@@ -220,11 +214,10 @@ public class CoordinatorCompactionConfigsResourceTest {
             ImmutableMap.of("key", "val"));
     final CoordinatorCompactionConfig originalConfig =
         CoordinatorCompactionConfig.from(ImmutableList.of(toDelete));
-    Mockito.when(
-            mockJacksonConfigManager.convertByteToConfig(
-                ArgumentMatchers.eq(OLD_CONFIG_IN_BYTES),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.empty())))
+    when(mockJacksonConfigManager.convertByteToConfig(
+            OLD_CONFIG_IN_BYTES,
+            CoordinatorCompactionConfig.class,
+            CoordinatorCompactionConfig.empty()))
         .thenReturn(originalConfig);
 
     String author = "maytas";
@@ -266,28 +259,20 @@ public class CoordinatorCompactionConfigsResourceTest {
 
   @Test
   public void testSetCompactionTaskLimitWithoutExistingConfig() {
-    Mockito.when(
-            mockConnector.lookup(
-                ArgumentMatchers.anyString(),
-                ArgumentMatchers.eq("name"),
-                ArgumentMatchers.eq("payload"),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY)))
+    when(mockConnector.lookup(
+            anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY)))
         .thenReturn(null);
-    Mockito.when(
-            mockJacksonConfigManager.convertByteToConfig(
-                ArgumentMatchers.eq(null),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.empty())))
+    when(mockJacksonConfigManager.convertByteToConfig(
+            null, CoordinatorCompactionConfig.class, CoordinatorCompactionConfig.empty()))
         .thenReturn(CoordinatorCompactionConfig.empty());
     final ArgumentCaptor<byte[]> oldConfigCaptor = ArgumentCaptor.forClass(byte[].class);
     final ArgumentCaptor<CoordinatorCompactionConfig> newConfigCaptor =
         ArgumentCaptor.forClass(CoordinatorCompactionConfig.class);
-    Mockito.when(
-            mockJacksonConfigManager.set(
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                oldConfigCaptor.capture(),
-                newConfigCaptor.capture(),
-                ArgumentMatchers.any()))
+    when(mockJacksonConfigManager.set(
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            oldConfigCaptor.capture(),
+            newConfigCaptor.capture(),
+            any()))
         .thenReturn(ConfigManager.SetResult.ok());
 
     double compactionTaskSlotRatio = 0.5;
@@ -314,28 +299,20 @@ public class CoordinatorCompactionConfigsResourceTest {
 
   @Test
   public void testAddOrUpdateCompactionConfigWithoutExistingConfig() {
-    Mockito.when(
-            mockConnector.lookup(
-                ArgumentMatchers.anyString(),
-                ArgumentMatchers.eq("name"),
-                ArgumentMatchers.eq("payload"),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY)))
+    when(mockConnector.lookup(
+            anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY)))
         .thenReturn(null);
-    Mockito.when(
-            mockJacksonConfigManager.convertByteToConfig(
-                ArgumentMatchers.eq(null),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.empty())))
+    when(mockJacksonConfigManager.convertByteToConfig(
+            null, CoordinatorCompactionConfig.class, CoordinatorCompactionConfig.empty()))
         .thenReturn(CoordinatorCompactionConfig.empty());
     final ArgumentCaptor<byte[]> oldConfigCaptor = ArgumentCaptor.forClass(byte[].class);
     final ArgumentCaptor<CoordinatorCompactionConfig> newConfigCaptor =
         ArgumentCaptor.forClass(CoordinatorCompactionConfig.class);
-    Mockito.when(
-            mockJacksonConfigManager.set(
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY),
-                oldConfigCaptor.capture(),
-                newConfigCaptor.capture(),
-                ArgumentMatchers.any()))
+    when(mockJacksonConfigManager.set(
+            eq(CoordinatorCompactionConfig.CONFIG_KEY),
+            oldConfigCaptor.capture(),
+            newConfigCaptor.capture(),
+            any()))
         .thenReturn(ConfigManager.SetResult.ok());
 
     final DataSourceCompactionConfig newConfig =
@@ -366,18 +343,11 @@ public class CoordinatorCompactionConfigsResourceTest {
 
   @Test
   public void testDeleteCompactionConfigWithoutExistingConfigShouldFailAsDatasourceNotExist() {
-    Mockito.when(
-            mockConnector.lookup(
-                ArgumentMatchers.anyString(),
-                ArgumentMatchers.eq("name"),
-                ArgumentMatchers.eq("payload"),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.CONFIG_KEY)))
+    when(mockConnector.lookup(
+            anyString(), eq("name"), eq("payload"), eq(CoordinatorCompactionConfig.CONFIG_KEY)))
         .thenReturn(null);
-    Mockito.when(
-            mockJacksonConfigManager.convertByteToConfig(
-                ArgumentMatchers.eq(null),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.class),
-                ArgumentMatchers.eq(CoordinatorCompactionConfig.empty())))
+    when(mockJacksonConfigManager.convertByteToConfig(
+            null, CoordinatorCompactionConfig.class, CoordinatorCompactionConfig.empty()))
         .thenReturn(CoordinatorCompactionConfig.empty());
     String author = "maytas";
     String comment = "hello";
diff --git a/server/src/test/java/org/apache/druid/server/http/DataSourcesResourceTest.java b/server/src/test/java/org/apache/druid/server/http/DataSourcesResourceTest.java
index e6399e39a4..533faa7664 100644
--- a/server/src/test/java/org/apache/druid/server/http/DataSourcesResourceTest.java
+++ b/server/src/test/java/org/apache/druid/server/http/DataSourcesResourceTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server.http;
 
+import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toSet;
+
 import com.google.common.base.Optional;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
@@ -28,14 +32,12 @@ import com.google.common.collect.Sets;
 import it.unimi.dsi.fastutil.objects.Object2LongMap;
 import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 import java.util.TreeMap;
 import java.util.TreeSet;
-import java.util.stream.Collectors;
 import javax.servlet.http.HttpServletRequest;
 import javax.ws.rs.core.Response;
 import org.apache.druid.client.CoordinatorServerView;
@@ -155,9 +157,7 @@ public class DataSourcesResourceTest {
     Assert.assertEquals(200, response.getStatus());
     Assert.assertEquals(2, result.size());
     ImmutableDruidDataSourceTestUtils.assertEquals(
-        listDataSources.stream()
-            .map(DruidDataSource::toImmutableDruidDataSource)
-            .collect(Collectors.toList()),
+        listDataSources.stream().map(DruidDataSource::toImmutableDruidDataSource).collect(toList()),
         new ArrayList<>(result));
 
     response = dataSourcesResource.getQueryableDataSources(null, null, request);
@@ -882,7 +882,7 @@ public class DataSourcesResourceTest {
     Interval interval = Intervals.of("2011-04-01/2011-04-02");
     Assert.assertFalse(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 2),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -890,7 +890,7 @@ public class DataSourcesResourceTest {
 
     Assert.assertTrue(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v2", 2),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -898,7 +898,7 @@ public class DataSourcesResourceTest {
 
     Assert.assertTrue(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 2),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -910,7 +910,7 @@ public class DataSourcesResourceTest {
     Interval interval = Intervals.of("2011-04-01/2011-04-02");
     Assert.assertTrue(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 2),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -918,7 +918,7 @@ public class DataSourcesResourceTest {
 
     Assert.assertFalse(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 2),
                     Sets.newHashSet(createRealtimeServerMetadata("a")))),
@@ -930,7 +930,7 @@ public class DataSourcesResourceTest {
     Interval interval = Intervals.of("2011-04-01/2011-04-02");
     Assert.assertTrue(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 1),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -938,7 +938,7 @@ public class DataSourcesResourceTest {
 
     Assert.assertFalse(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(interval, "v1", 1),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -950,7 +950,7 @@ public class DataSourcesResourceTest {
 
     Assert.assertFalse(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(Intervals.of("2011-04-01/2011-04-02"), "v1", 1),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -958,7 +958,7 @@ public class DataSourcesResourceTest {
 
     Assert.assertTrue(
         DataSourcesResource.isSegmentLoaded(
-            Collections.singletonList(
+            singletonList(
                 new ImmutableSegmentLoadInfo(
                     createSegment(Intervals.of("2011-04-01/2011-04-04"), "v1", 1),
                     Sets.newHashSet(createHistoricalServerMetadata("a")))),
@@ -972,7 +972,7 @@ public class DataSourcesResourceTest {
         dataSegmentList.stream()
             .filter(segment -> segment.getDataSource().equals(dataSource1.getName()))
             .map(DataSegment::getId)
-            .collect(Collectors.toSet());
+            .collect(toSet());
 
     EasyMock.expect(inventoryView.getInventory()).andReturn(ImmutableList.of(server)).once();
     EasyMock.expect(server.getDataSource("datasource1")).andReturn(dataSource1).once();
@@ -981,7 +981,7 @@ public class DataSourcesResourceTest {
 
     final DataSourcesResource.MarkDataSourceSegmentsPayload payload =
         new DataSourcesResource.MarkDataSourceSegmentsPayload(
-            null, segmentIds.stream().map(SegmentId::toString).collect(Collectors.toSet()));
+            null, segmentIds.stream().map(SegmentId::toString).collect(toSet()));
 
     DataSourcesResource dataSourcesResource =
         new DataSourcesResource(inventoryView, segmentsMetadataManager, null, null, null, null);
@@ -998,7 +998,7 @@ public class DataSourcesResourceTest {
         dataSegmentList.stream()
             .filter(segment -> segment.getDataSource().equals(dataSource1.getName()))
             .map(DataSegment::getId)
-            .collect(Collectors.toSet());
+            .collect(toSet());
 
     EasyMock.expect(inventoryView.getInventory()).andReturn(ImmutableList.of(server)).once();
     EasyMock.expect(server.getDataSource("datasource1")).andReturn(dataSource1).once();
@@ -1007,7 +1007,7 @@ public class DataSourcesResourceTest {
 
     final DataSourcesResource.MarkDataSourceSegmentsPayload payload =
         new DataSourcesResource.MarkDataSourceSegmentsPayload(
-            null, segmentIds.stream().map(SegmentId::toString).collect(Collectors.toSet()));
+            null, segmentIds.stream().map(SegmentId::toString).collect(toSet()));
 
     DataSourcesResource dataSourcesResource =
         new DataSourcesResource(inventoryView, segmentsMetadataManager, null, null, null, null);
@@ -1024,7 +1024,7 @@ public class DataSourcesResourceTest {
         dataSegmentList.stream()
             .filter(segment -> segment.getDataSource().equals(dataSource1.getName()))
             .map(DataSegment::getId)
-            .collect(Collectors.toSet());
+            .collect(toSet());
 
     EasyMock.expect(inventoryView.getInventory()).andReturn(ImmutableList.of(server)).once();
     EasyMock.expect(server.getDataSource("datasource1")).andReturn(dataSource1).once();
@@ -1035,7 +1035,7 @@ public class DataSourcesResourceTest {
 
     final DataSourcesResource.MarkDataSourceSegmentsPayload payload =
         new DataSourcesResource.MarkDataSourceSegmentsPayload(
-            null, segmentIds.stream().map(SegmentId::toString).collect(Collectors.toSet()));
+            null, segmentIds.stream().map(SegmentId::toString).collect(toSet()));
 
     DataSourcesResource dataSourcesResource =
         new DataSourcesResource(inventoryView, segmentsMetadataManager, null, null, null, null);
diff --git a/server/src/test/java/org/apache/druid/server/http/security/PreResponseAuthorizationCheckFilterTest.java b/server/src/test/java/org/apache/druid/server/http/security/PreResponseAuthorizationCheckFilterTest.java
index f873c29f15..77e1b335e2 100644
--- a/server/src/test/java/org/apache/druid/server/http/security/PreResponseAuthorizationCheckFilterTest.java
+++ b/server/src/test/java/org/apache/druid/server/http/security/PreResponseAuthorizationCheckFilterTest.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.server.http.security;
 
+import static java.util.Collections.singletonList;
+
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import javax.servlet.ServletException;
 import org.apache.druid.jackson.DefaultObjectMapper;
@@ -38,7 +39,7 @@ import org.junit.Test;
 
 public class PreResponseAuthorizationCheckFilterTest {
   private static final List<Authenticator> authenticators =
-      Collections.singletonList(new AllowAllAuthenticator());
+      singletonList(new AllowAllAuthenticator());
 
   @Test
   public void testValidRequest() throws Exception {
diff --git a/server/src/test/java/org/apache/druid/server/http/security/ResourceFilterTestHelper.java b/server/src/test/java/org/apache/druid/server/http/security/ResourceFilterTestHelper.java
index 4bf1ba7425..540914ad6d 100644
--- a/server/src/test/java/org/apache/druid/server/http/security/ResourceFilterTestHelper.java
+++ b/server/src/test/java/org/apache/druid/server/http/security/ResourceFilterTestHelper.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server.http.security;
 
+import static java.util.Collections.emptyList;
+import static java.util.Collections.singletonList;
+
 import com.google.common.base.Function;
 import com.google.common.base.Predicate;
 import com.google.common.collect.Collections2;
@@ -36,7 +39,6 @@ import java.lang.reflect.AnnotatedElement;
 import java.lang.reflect.Method;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import javax.servlet.http.HttpServletRequest;
 import javax.ws.rs.DELETE;
@@ -170,14 +172,14 @@ public class ResourceFilterTestHelper {
         classOrMethod.getAnnotation(Path.class).value().substring(1); // Ignore the first "/"
     final List<Class<? extends ResourceFilter>> baseResourceFilters =
         classOrMethod.getAnnotation(ResourceFilters.class) == null
-            ? Collections.emptyList()
+            ? emptyList()
             : ImmutableList.copyOf(classOrMethod.getAnnotation(ResourceFilters.class).value());
 
     List<Method> methods;
     if (classOrMethod instanceof Class<?>) {
       methods = ImmutableList.copyOf(((Class<?>) classOrMethod).getDeclaredMethods());
     } else {
-      methods = Collections.singletonList((Method) classOrMethod);
+      methods = singletonList((Method) classOrMethod);
     }
     return ImmutableList.copyOf(
         Iterables.concat(
diff --git a/server/src/test/java/org/apache/druid/server/initialization/BaseJettyTest.java b/server/src/test/java/org/apache/druid/server/initialization/BaseJettyTest.java
index 8f07296392..43f6ebcd11 100644
--- a/server/src/test/java/org/apache/druid/server/initialization/BaseJettyTest.java
+++ b/server/src/test/java/org/apache/druid/server/initialization/BaseJettyTest.java
@@ -247,8 +247,8 @@ public abstract class BaseJettyTest {
 
   @Path("/return")
   public static class DirectlyReturnResource {
-    @POST
     @Consumes(MediaType.TEXT_PLAIN)
+    @POST
     @Produces(MediaType.TEXT_PLAIN)
     public Response postText(String text) {
       return Response.ok(text).build();
diff --git a/server/src/test/java/org/apache/druid/server/initialization/JettyBindOnHostTest.java b/server/src/test/java/org/apache/druid/server/initialization/JettyBindOnHostTest.java
index e0037ba104..c5c2219d03 100644
--- a/server/src/test/java/org/apache/druid/server/initialization/JettyBindOnHostTest.java
+++ b/server/src/test/java/org/apache/druid/server/initialization/JettyBindOnHostTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.initialization;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Binder;
 import com.google.inject.Injector;
@@ -26,7 +28,6 @@ import com.google.inject.Key;
 import com.google.inject.Module;
 import java.net.HttpURLConnection;
 import java.net.URL;
-import java.nio.charset.StandardCharsets;
 import org.apache.commons.io.IOUtils;
 import org.apache.druid.guice.GuiceInjectors;
 import org.apache.druid.guice.Jerseys;
@@ -75,12 +76,10 @@ public class JettyBindOnHostTest extends BaseJettyTest {
 
     final URL url = new URL("http://localhost:" + port + "/default");
     final HttpURLConnection get = (HttpURLConnection) url.openConnection();
-    Assert.assertEquals(
-        DEFAULT_RESPONSE_CONTENT, IOUtils.toString(get.getInputStream(), StandardCharsets.UTF_8));
+    Assert.assertEquals(DEFAULT_RESPONSE_CONTENT, IOUtils.toString(get.getInputStream(), UTF_8));
 
     final HttpURLConnection post = (HttpURLConnection) url.openConnection();
     post.setRequestMethod("POST");
-    Assert.assertEquals(
-        DEFAULT_RESPONSE_CONTENT, IOUtils.toString(post.getInputStream(), StandardCharsets.UTF_8));
+    Assert.assertEquals(DEFAULT_RESPONSE_CONTENT, IOUtils.toString(post.getInputStream(), UTF_8));
   }
 }
diff --git a/server/src/test/java/org/apache/druid/server/initialization/JettyCertRenewTest.java b/server/src/test/java/org/apache/druid/server/initialization/JettyCertRenewTest.java
index 7f8d49b8ec..207afb188a 100644
--- a/server/src/test/java/org/apache/druid/server/initialization/JettyCertRenewTest.java
+++ b/server/src/test/java/org/apache/druid/server/initialization/JettyCertRenewTest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.initialization;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.inject.Binder;
@@ -32,7 +34,6 @@ import java.io.IOException;
 import java.io.InputStream;
 import java.net.URL;
 import java.nio.charset.Charset;
-import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardCopyOption;
@@ -371,6 +372,6 @@ public class JettyCertRenewTest extends BaseJettyTest {
     }
 
     ListenableFuture<InputStream> go = client.go(request, new InputStreamResponseHandler());
-    return IOUtils.toString(go.get(), StandardCharsets.UTF_8);
+    return IOUtils.toString(go.get(), UTF_8);
   }
 }
diff --git a/server/src/test/java/org/apache/druid/server/initialization/JettyTest.java b/server/src/test/java/org/apache/druid/server/initialization/JettyTest.java
index 8333107769..34684fafd5 100644
--- a/server/src/test/java/org/apache/druid/server/initialization/JettyTest.java
+++ b/server/src/test/java/org/apache/druid/server/initialization/JettyTest.java
@@ -19,6 +19,11 @@
 
 package org.apache.druid.server.initialization;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static org.mockito.ArgumentMatchers.eq;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.inject.Binder;
@@ -34,7 +39,6 @@ import java.io.StringWriter;
 import java.net.HttpURLConnection;
 import java.net.URL;
 import java.nio.charset.Charset;
-import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.security.cert.X509Certificate;
@@ -90,8 +94,6 @@ import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
 import org.mockito.ArgumentCaptor;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
 
 public class JettyTest extends BaseJettyTest {
   @Rule public TemporaryFolder folder = new TemporaryFolder();
@@ -268,8 +270,8 @@ public class JettyTest extends BaseJettyTest {
     return injector;
   }
 
-  @Test
-  @Ignore // this test will deadlock if it hits an issue, so ignored by default
+  @Ignore
+  @Test // this test will deadlock if it hits an issue, so ignored by default
   public void testTimeouts() throws Exception {
     // test for request timeouts properly not locking up all threads
     final ExecutorService executor = Execs.multiThreaded(100, "JettyTest-%d");
@@ -323,7 +325,7 @@ public class JettyTest extends BaseJettyTest {
     Assert.assertEquals("gzip", get.getContentEncoding());
     Assert.assertEquals(
         DEFAULT_RESPONSE_CONTENT,
-        IOUtils.toString(new GZIPInputStream(get.getInputStream()), StandardCharsets.UTF_8));
+        IOUtils.toString(new GZIPInputStream(get.getInputStream()), UTF_8));
 
     final HttpURLConnection post = (HttpURLConnection) url.openConnection();
     post.setRequestProperty("Accept-Encoding", "gzip");
@@ -331,26 +333,24 @@ public class JettyTest extends BaseJettyTest {
     Assert.assertEquals("gzip", post.getContentEncoding());
     Assert.assertEquals(
         DEFAULT_RESPONSE_CONTENT,
-        IOUtils.toString(new GZIPInputStream(post.getInputStream()), StandardCharsets.UTF_8));
+        IOUtils.toString(new GZIPInputStream(post.getInputStream()), UTF_8));
 
     final HttpURLConnection getNoGzip = (HttpURLConnection) url.openConnection();
     Assert.assertNotEquals("gzip", getNoGzip.getContentEncoding());
     Assert.assertEquals(
-        DEFAULT_RESPONSE_CONTENT,
-        IOUtils.toString(getNoGzip.getInputStream(), StandardCharsets.UTF_8));
+        DEFAULT_RESPONSE_CONTENT, IOUtils.toString(getNoGzip.getInputStream(), UTF_8));
 
     final HttpURLConnection postNoGzip = (HttpURLConnection) url.openConnection();
     postNoGzip.setRequestMethod("POST");
     Assert.assertNotEquals("gzip", postNoGzip.getContentEncoding());
     Assert.assertEquals(
-        DEFAULT_RESPONSE_CONTENT,
-        IOUtils.toString(postNoGzip.getInputStream(), StandardCharsets.UTF_8));
+        DEFAULT_RESPONSE_CONTENT, IOUtils.toString(postNoGzip.getInputStream(), UTF_8));
   }
 
   // Tests that threads are not stuck when partial chunk is not finalized
   // https://bugs.eclipse.org/bugs/show_bug.cgi?id=424107
-  @Test
   @Ignore
+  @Test
   // above bug is not fixed in jetty for gzip encoding, and the chunk is still finalized instead of
   // throwing exception.
   public void testChunkNotFinalized() throws Exception {
@@ -492,10 +492,9 @@ public class JettyTest extends BaseJettyTest {
     server.setEndpointIdentificationAlgorithm("HTTPS");
     server.start();
     SSLEngine sslEngine = server.newSSLEngine();
-    X509ExtendedTrustManager mockX509ExtendedTrustManager =
-        Mockito.mock(X509ExtendedTrustManager.class);
-    TLSCertificateChecker mockTLSCertificateChecker = Mockito.mock(TLSCertificateChecker.class);
-    X509Certificate mockX509Certificate = Mockito.mock(X509Certificate.class);
+    X509ExtendedTrustManager mockX509ExtendedTrustManager = mock(X509ExtendedTrustManager.class);
+    TLSCertificateChecker mockTLSCertificateChecker = mock(TLSCertificateChecker.class);
+    X509Certificate mockX509Certificate = mock(X509Certificate.class);
     String authType = "testAuthType";
     X509Certificate[] chain = new X509Certificate[] {mockX509Certificate};
 
@@ -509,12 +508,8 @@ public class JettyTest extends BaseJettyTest {
     customCheckX509TrustManager.checkServerTrusted(chain, authType, sslEngine);
 
     ArgumentCaptor<SSLEngine> captor = ArgumentCaptor.forClass(SSLEngine.class);
-    Mockito.verify(mockTLSCertificateChecker)
-        .checkServer(
-            ArgumentMatchers.eq(chain),
-            ArgumentMatchers.eq(authType),
-            captor.capture(),
-            ArgumentMatchers.eq(mockX509ExtendedTrustManager));
+    verify(mockTLSCertificateChecker)
+        .checkServer(eq(chain), eq(authType), captor.capture(), eq(mockX509ExtendedTrustManager));
     SSLEngine transformedSSLEngine = captor.getValue();
     // The EndpointIdentificationAlgorithm should be null or empty Stringas the
     // CustomCheckX509TrustManager
diff --git a/server/src/test/java/org/apache/druid/server/initialization/JettyWithResponseFilterEnabledTest.java b/server/src/test/java/org/apache/druid/server/initialization/JettyWithResponseFilterEnabledTest.java
index 40b2b43c84..c693192744 100644
--- a/server/src/test/java/org/apache/druid/server/initialization/JettyWithResponseFilterEnabledTest.java
+++ b/server/src/test/java/org/apache/druid/server/initialization/JettyWithResponseFilterEnabledTest.java
@@ -30,8 +30,8 @@ public class JettyWithResponseFilterEnabledTest extends JettyTest {
     System.setProperty("druid.server.http.showDetailedJettyErrors", "false");
   }
 
-  @Test
   @Override
+  @Test
   public void testJettyErrorHandlerWithFilter() {
     // Response filter is enabled by config hence we do not show servlet information
     Assert.assertFalse(server.getErrorHandler().isShowServlet());
diff --git a/server/src/test/java/org/apache/druid/server/initialization/jetty/JettyServerModuleTest.java b/server/src/test/java/org/apache/druid/server/initialization/jetty/JettyServerModuleTest.java
index ddcf5a2e30..3f9557a61e 100644
--- a/server/src/test/java/org/apache/druid/server/initialization/jetty/JettyServerModuleTest.java
+++ b/server/src/test/java/org/apache/druid/server/initialization/jetty/JettyServerModuleTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server.initialization.jetty;
 
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
@@ -30,28 +33,27 @@ import org.apache.druid.java.util.emitter.service.ServiceMetricEvent;
 import org.eclipse.jetty.util.thread.QueuedThreadPool;
 import org.junit.Assert;
 import org.junit.Test;
-import org.mockito.Mockito;
 
 public class JettyServerModuleTest {
   @Test
   public void testJettyServerModule() {
     List<Event> events = new ArrayList<>();
     ServiceEmitter serviceEmitter =
-        new ServiceEmitter("service", "host", Mockito.mock(Emitter.class)) {
+        new ServiceEmitter("service", "host", mock(Emitter.class)) {
           @Override
           public void emit(Event event) {
             events.add(event);
           }
         };
-    QueuedThreadPool jettyServerThreadPool = Mockito.mock(QueuedThreadPool.class);
+    QueuedThreadPool jettyServerThreadPool = mock(QueuedThreadPool.class);
     JettyServerModule.setJettyServerThreadPool(jettyServerThreadPool);
-    Mockito.when(jettyServerThreadPool.getThreads()).thenReturn(100);
-    Mockito.when(jettyServerThreadPool.getIdleThreads()).thenReturn(40);
-    Mockito.when(jettyServerThreadPool.isLowOnThreads()).thenReturn(true);
-    Mockito.when(jettyServerThreadPool.getMinThreads()).thenReturn(30);
-    Mockito.when(jettyServerThreadPool.getMaxThreads()).thenReturn(100);
-    Mockito.when(jettyServerThreadPool.getQueueSize()).thenReturn(50);
-    Mockito.when(jettyServerThreadPool.getBusyThreads()).thenReturn(60);
+    when(jettyServerThreadPool.getThreads()).thenReturn(100);
+    when(jettyServerThreadPool.getIdleThreads()).thenReturn(40);
+    when(jettyServerThreadPool.isLowOnThreads()).thenReturn(true);
+    when(jettyServerThreadPool.getMinThreads()).thenReturn(30);
+    when(jettyServerThreadPool.getMaxThreads()).thenReturn(100);
+    when(jettyServerThreadPool.getQueueSize()).thenReturn(50);
+    when(jettyServerThreadPool.getBusyThreads()).thenReturn(60);
 
     JettyServerModule.JettyMonitor jettyMonitor = new JettyServerModule.JettyMonitor("ds", "t0");
     jettyMonitor.doMonitor(serviceEmitter);
diff --git a/server/src/test/java/org/apache/druid/server/initialization/jetty/StandardResponseHeaderFilterHolderTest.java b/server/src/test/java/org/apache/druid/server/initialization/jetty/StandardResponseHeaderFilterHolderTest.java
index ce351bc0c3..12d19e177b 100644
--- a/server/src/test/java/org/apache/druid/server/initialization/jetty/StandardResponseHeaderFilterHolderTest.java
+++ b/server/src/test/java/org/apache/druid/server/initialization/jetty/StandardResponseHeaderFilterHolderTest.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.server.initialization.jetty;
 
+import static java.util.Collections.emptyMap;
+
 import com.google.common.collect.ImmutableMap;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 import javax.servlet.FilterChain;
@@ -74,7 +75,7 @@ public class StandardResponseHeaderFilterHolderTest {
     EasyMock.expect(serverConfig.getContentSecurityPolicy()).andReturn("").once();
     EasyMock.expect(httpRequest.getMethod()).andReturn(HttpMethod.POST).anyTimes();
 
-    runFilterAndVerifyHeaders(Collections.emptyMap());
+    runFilterAndVerifyHeaders(emptyMap());
   }
 
   @Test
diff --git a/server/src/test/java/org/apache/druid/server/log/AlertEventSerdeTest.java b/server/src/test/java/org/apache/druid/server/log/AlertEventSerdeTest.java
index e28e6a4225..1cb8ac7515 100644
--- a/server/src/test/java/org/apache/druid/server/log/AlertEventSerdeTest.java
+++ b/server/src/test/java/org/apache/druid/server/log/AlertEventSerdeTest.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.log;
 
+import static java.util.Collections.emptyMap;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import java.util.Collections;
 import org.apache.druid.jackson.DefaultObjectMapper;
 import org.apache.druid.java.util.common.DateTimes;
 import org.apache.druid.java.util.emitter.core.Event;
@@ -41,7 +42,7 @@ public class AlertEventSerdeTest {
             "my-host",
             AlertEvent.Severity.DEFAULT,
             "my-description",
-            Collections.emptyMap());
+            emptyMap());
 
     String actual = mapper.writeValueAsString(event.toMap());
     String expected =
diff --git a/server/src/test/java/org/apache/druid/server/log/DefaultRequestLogEventTest.java b/server/src/test/java/org/apache/druid/server/log/DefaultRequestLogEventTest.java
index 1d1a9c027c..1c1613f337 100644
--- a/server/src/test/java/org/apache/druid/server/log/DefaultRequestLogEventTest.java
+++ b/server/src/test/java/org/apache/druid/server/log/DefaultRequestLogEventTest.java
@@ -19,11 +19,12 @@
 
 package org.apache.druid.server.log;
 
+import static java.util.Collections.emptyMap;
+
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 import org.apache.druid.jackson.DefaultObjectMapper;
@@ -176,7 +177,7 @@ public class DefaultRequestLogEventTest {
                 "requests",
                 RequestLogLine.forSql(
                     "SELECT * FROM dummy",
-                    Collections.emptyMap(),
+                    emptyMap(),
                     DateTimes.of(timestamp),
                     "127.0.0.1",
                     new QueryStats(ImmutableMap.of())))
diff --git a/server/src/test/java/org/apache/druid/server/log/FileRequestLoggerTest.java b/server/src/test/java/org/apache/druid/server/log/FileRequestLoggerTest.java
index a4ceb95277..ada362d696 100644
--- a/server/src/test/java/org/apache/druid/server/log/FileRequestLoggerTest.java
+++ b/server/src/test/java/org/apache/druid/server/log/FileRequestLoggerTest.java
@@ -19,10 +19,11 @@
 
 package org.apache.druid.server.log;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.io.CharStreams;
 import java.io.File;
-import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.util.Date;
 import java.util.concurrent.Executors;
@@ -72,8 +73,7 @@ public class FileRequestLoggerTest {
     fileRequestLogger.logSqlQuery(sqlRequestLogLine);
 
     File logFile = new File(logDir, dateTime.toString("yyyy-MM-dd'.log'"));
-    String logString =
-        CharStreams.toString(Files.newBufferedReader(logFile.toPath(), StandardCharsets.UTF_8));
+    String logString = CharStreams.toString(Files.newBufferedReader(logFile.toPath(), UTF_8));
     Assert.assertTrue(logString.contains(nativeQueryLogString + "\n" + sqlQueryLogString + "\n"));
     fileRequestLogger.stop();
   }
@@ -86,7 +86,7 @@ public class FileRequestLoggerTest {
     String logString = dateTime + "\t" + HOST + "\t" + "logString";
 
     File oldLogFile = new File(logDir, "2000-01-01.log");
-    com.google.common.io.Files.write("testOldLogContent", oldLogFile, StandardCharsets.UTF_8);
+    com.google.common.io.Files.write("testOldLogContent", oldLogFile, UTF_8);
     oldLogFile.setLastModified(new Date(0).getTime());
     FileRequestLogger fileRequestLogger =
         new FileRequestLogger(
diff --git a/server/src/test/java/org/apache/druid/server/log/LoggingRequestLoggerProviderTest.java b/server/src/test/java/org/apache/druid/server/log/LoggingRequestLoggerProviderTest.java
index a0c59520f8..a49a8c6697 100644
--- a/server/src/test/java/org/apache/druid/server/log/LoggingRequestLoggerProviderTest.java
+++ b/server/src/test/java/org/apache/druid/server/log/LoggingRequestLoggerProviderTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server.log;
 
+import static java.util.UUID.randomUUID;
+import static org.hamcrest.Matchers.instanceOf;
+
 import com.google.common.collect.ImmutableList;
 import com.google.inject.Binder;
 import com.google.inject.Injector;
@@ -26,19 +29,17 @@ import com.google.inject.Key;
 import com.google.inject.Module;
 import com.google.inject.name.Names;
 import java.util.Properties;
-import java.util.UUID;
 import org.apache.druid.guice.GuiceInjectors;
 import org.apache.druid.guice.JsonConfigProvider;
 import org.apache.druid.guice.JsonConfigurator;
 import org.apache.druid.guice.ManageLifecycle;
 import org.apache.druid.guice.QueryableModule;
 import org.apache.druid.initialization.Initialization;
-import org.hamcrest.Matchers;
 import org.junit.Assert;
 import org.junit.Test;
 
 public class LoggingRequestLoggerProviderTest {
-  private final String propertyPrefix = UUID.randomUUID().toString().replace('-', '_');
+  private final String propertyPrefix = randomUUID().toString().replace('-', '_');
   private final JsonConfigProvider<RequestLoggerProvider> provider =
       JsonConfigProvider.of(propertyPrefix, RequestLoggerProvider.class);
   private final Injector injector = makeInjector();
@@ -70,7 +71,7 @@ public class LoggingRequestLoggerProviderTest {
     final Properties properties = new Properties();
     properties.put(propertyPrefix + ".type", "noop");
     provider.inject(properties, injector.getInstance(JsonConfigurator.class));
-    Assert.assertThat(provider.get().get().get(), Matchers.instanceOf(NoopRequestLogger.class));
+    Assert.assertThat(provider.get().get().get(), instanceOf(NoopRequestLogger.class));
   }
 
   private Injector makeInjector() {
diff --git a/server/src/test/java/org/apache/druid/server/log/LoggingRequestLoggerTest.java b/server/src/test/java/org/apache/druid/server/log/LoggingRequestLoggerTest.java
index d0e4684f88..b707b31e9b 100644
--- a/server/src/test/java/org/apache/druid/server/log/LoggingRequestLoggerTest.java
+++ b/server/src/test/java/org/apache/druid/server/log/LoggingRequestLoggerTest.java
@@ -19,14 +19,15 @@
 
 package org.apache.druid.server.log;
 
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static java.util.Collections.singletonList;
+
 import com.fasterxml.jackson.annotation.JsonTypeName;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.io.ByteArrayOutputStream;
-import java.nio.charset.StandardCharsets;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -76,7 +77,7 @@ public class LoggingRequestLoggerTest {
           new QuerySegmentSpec() {
             @Override
             public List<Interval> getIntervals() {
-              return Collections.singletonList(Intervals.of("2016-01-01T00Z/2016-01-02T00Z"));
+              return singletonList(Intervals.of("2016-01-01T00Z/2016-01-02T00Z"));
             }
 
             @Override
@@ -93,7 +94,7 @@ public class LoggingRequestLoggerTest {
           new QuerySegmentSpec() {
             @Override
             public List<Interval> getIntervals() {
-              return Collections.singletonList(Intervals.of("2016-01-01T00Z/2016-01-02T00Z"));
+              return singletonList(Intervals.of("2016-01-01T00Z/2016-01-02T00Z"));
             }
 
             @Override
@@ -110,7 +111,7 @@ public class LoggingRequestLoggerTest {
           new QuerySegmentSpec() {
             @Override
             public List<Interval> getIntervals() {
-              return Collections.singletonList(Intervals.of("2016-01-01T00Z/2016-01-02T00Z"));
+              return singletonList(Intervals.of("2016-01-01T00Z/2016-01-02T00Z"));
             }
 
             @Override
@@ -127,7 +128,7 @@ public class LoggingRequestLoggerTest {
           new QuerySegmentSpec() {
             @Override
             public List<Interval> getIntervals() {
-              return Collections.singletonList(Intervals.of("2016-01-01T00Z/2016-01-02T00Z"));
+              return singletonList(Intervals.of("2016-01-01T00Z/2016-01-02T00Z"));
             }
 
             @Override
@@ -151,17 +152,7 @@ public class LoggingRequestLoggerTest {
             .setTarget(BAOS)
             .setLayout(
                 JsonLayout.createLayout(
-                    configuration,
-                    false,
-                    true,
-                    true,
-                    false,
-                    true,
-                    true,
-                    "[",
-                    "]",
-                    StandardCharsets.UTF_8,
-                    true))
+                    configuration, false, true, true, false, true, true, "[", "]", UTF_8, true))
             .build();
     final Logger logger = (Logger) LogManager.getLogger(LoggingRequestLogger.class);
     appender.start();
@@ -287,7 +278,7 @@ public class LoggingRequestLoggerTest {
         context.put(key, value);
       }
     }
-    return ImmutableMap.copyOf(context);
+    return context;
   }
 }
 
diff --git a/server/src/test/java/org/apache/druid/server/log/TestRequestLogger.java b/server/src/test/java/org/apache/druid/server/log/TestRequestLogger.java
index 419dc76f45..5243e32aa6 100644
--- a/server/src/test/java/org/apache/druid/server/log/TestRequestLogger.java
+++ b/server/src/test/java/org/apache/druid/server/log/TestRequestLogger.java
@@ -19,7 +19,6 @@
 
 package org.apache.druid.server.log;
 
-import com.google.common.collect.ImmutableList;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -65,13 +64,13 @@ public class TestRequestLogger implements RequestLogger {
 
   public List<RequestLogLine> getNativeQuerylogs() {
     synchronized (nativeQuerylogs) {
-      return ImmutableList.copyOf(nativeQuerylogs);
+      return nativeQuerylogs;
     }
   }
 
   public List<RequestLogLine> getSqlQueryLogs() {
     synchronized (sqlQueryLogs) {
-      return ImmutableList.copyOf(sqlQueryLogs);
+      return sqlQueryLogs;
     }
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/lookup/cache/LookupCoordinatorManagerTest.java b/server/src/test/java/org/apache/druid/server/lookup/cache/LookupCoordinatorManagerTest.java
index df71a559f3..7f3ce1388b 100644
--- a/server/src/test/java/org/apache/druid/server/lookup/cache/LookupCoordinatorManagerTest.java
+++ b/server/src/test/java/org/apache/druid/server/lookup/cache/LookupCoordinatorManagerTest.java
@@ -19,6 +19,9 @@
 
 package org.apache.druid.server.lookup.cache;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.emptySet;
+
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.collect.ImmutableList;
@@ -29,7 +32,6 @@ import com.google.common.util.concurrent.SettableFuture;
 import java.io.ByteArrayInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -93,7 +95,7 @@ public class LookupCoordinatorManagerTest {
   private static final Map<String, Map<String, LookupExtractorFactoryMapContainer>>
       EMPTY_TIERED_LOOKUP = ImmutableMap.of();
   private static final LookupsState<LookupExtractorFactoryMapContainer> LOOKUPS_STATE =
-      new LookupsState<>(SINGLE_LOOKUP_MAP_V0, SINGLE_LOOKUP_MAP_V1, Collections.emptySet());
+      new LookupsState<>(SINGLE_LOOKUP_MAP_V0, SINGLE_LOOKUP_MAP_V1, emptySet());
 
   private static final AtomicLong EVENT_EMITS = new AtomicLong(0L);
   private static ServiceEmitter SERVICE_EMITTER;
@@ -1163,7 +1165,7 @@ public class LookupCoordinatorManagerTest {
                 EasyMock.eq(LookupCoordinatorManager.LOOKUP_CONFIG_KEY),
                 EasyMock.<TypeReference>anyObject(),
                 EasyMock.<AtomicReference>isNull()))
-        .andReturn(new AtomicReference<>(Collections.emptyMap()))
+        .andReturn(new AtomicReference<>(emptyMap()))
         .anyTimes();
 
     EasyMock.replay(configManager);
diff --git a/server/src/test/java/org/apache/druid/server/metrics/MetricsModuleTest.java b/server/src/test/java/org/apache/druid/server/metrics/MetricsModuleTest.java
index 514da6c02b..341ddd9eb3 100644
--- a/server/src/test/java/org/apache/druid/server/metrics/MetricsModuleTest.java
+++ b/server/src/test/java/org/apache/druid/server/metrics/MetricsModuleTest.java
@@ -19,6 +19,12 @@
 
 package org.apache.druid.server.metrics;
 
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.Mockito.atLeastOnce;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.verify;
+
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
 import com.google.inject.Binder;
@@ -56,8 +62,6 @@ import org.junit.Assume;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
 
 public class MetricsModuleTest {
   private static final String CPU_ARCH = System.getProperty("os.arch");
@@ -167,11 +171,11 @@ public class MetricsModuleTest {
 
     final Injector injector = createInjector(new Properties(), ImmutableSet.of(NodeRole.PEON));
     final SysMonitor sysMonitor = injector.getInstance(SysMonitor.class);
-    final ServiceEmitter emitter = Mockito.mock(ServiceEmitter.class);
+    final ServiceEmitter emitter = mock(ServiceEmitter.class);
     sysMonitor.doMonitor(emitter);
 
     Assert.assertTrue(sysMonitor instanceof NoopSysMonitor);
-    Mockito.verify(emitter, Mockito.never()).emit(ArgumentMatchers.any(ServiceEventBuilder.class));
+    verify(emitter, never()).emit(any(ServiceEventBuilder.class));
   }
 
   @Test
@@ -181,12 +185,11 @@ public class MetricsModuleTest {
 
     Injector injector = createInjector(new Properties(), ImmutableSet.of());
     final SysMonitor sysMonitor = injector.getInstance(SysMonitor.class);
-    final ServiceEmitter emitter = Mockito.mock(ServiceEmitter.class);
+    final ServiceEmitter emitter = mock(ServiceEmitter.class);
     sysMonitor.doMonitor(emitter);
 
     Assert.assertFalse(sysMonitor instanceof NoopSysMonitor);
-    Mockito.verify(emitter, Mockito.atLeastOnce())
-        .emit(ArgumentMatchers.any(ServiceEventBuilder.class));
+    verify(emitter, atLeastOnce()).emit(any(ServiceEventBuilder.class));
   }
 
   private static Injector createInjector(Properties properties, ImmutableSet<NodeRole> nodeRoles) {
diff --git a/server/src/test/java/org/apache/druid/server/metrics/QueryCountStatsMonitorTest.java b/server/src/test/java/org/apache/druid/server/metrics/QueryCountStatsMonitorTest.java
index 1e03eaff22..d2952b3521 100644
--- a/server/src/test/java/org/apache/druid/server/metrics/QueryCountStatsMonitorTest.java
+++ b/server/src/test/java/org/apache/druid/server/metrics/QueryCountStatsMonitorTest.java
@@ -19,8 +19,9 @@
 
 package org.apache.druid.server.metrics;
 
+import static java.util.stream.Collectors.toMap;
+
 import java.util.Map;
-import java.util.stream.Collectors;
 import org.apache.druid.java.util.metrics.StubServiceEmitter;
 import org.junit.Assert;
 import org.junit.Before;
@@ -74,7 +75,7 @@ public class QueryCountStatsMonitorTest {
     Map<String, Long> resultMap =
         emitter.getEvents().stream()
             .collect(
-                Collectors.toMap(
+                toMap(
                     event -> (String) event.toMap().get("metric"),
                     event -> (Long) event.toMap().get("value")));
     Assert.assertEquals(5, resultMap.size());
diff --git a/server/src/test/java/org/apache/druid/server/metrics/SegmentStatsMonitorTest.java b/server/src/test/java/org/apache/druid/server/metrics/SegmentStatsMonitorTest.java
index 0b56b0f085..86eda37ebf 100644
--- a/server/src/test/java/org/apache/druid/server/metrics/SegmentStatsMonitorTest.java
+++ b/server/src/test/java/org/apache/druid/server/metrics/SegmentStatsMonitorTest.java
@@ -19,13 +19,19 @@
 
 package org.apache.druid.server.metrics;
 
+import static java.util.function.Function.identity;
+import static java.util.stream.Collectors.toList;
+import static java.util.stream.Collectors.toMap;
+import static org.mockito.Mockito.atLeastOnce;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
 import com.google.common.collect.ImmutableMap;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.function.Function;
-import java.util.stream.Collectors;
 import javax.annotation.Nonnull;
 import org.apache.druid.client.DruidServerConfig;
 import org.apache.druid.java.util.emitter.service.ServiceEmitter;
@@ -38,7 +44,6 @@ import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
 import org.mockito.ArgumentCaptor;
-import org.mockito.Mockito;
 
 public class SegmentStatsMonitorTest {
   private static final String DATA_SOURCE = "dataSource";
@@ -53,18 +58,18 @@ public class SegmentStatsMonitorTest {
 
   @Before
   public void setUp() {
-    druidServerConfig = Mockito.mock(DruidServerConfig.class);
-    segmentLoadDropMgr = Mockito.mock(SegmentLoadDropHandler.class);
-    serviceEmitter = Mockito.mock(ServiceEmitter.class);
+    druidServerConfig = mock(DruidServerConfig.class);
+    segmentLoadDropMgr = mock(SegmentLoadDropHandler.class);
+    serviceEmitter = mock(ServiceEmitter.class);
     monitor = new SegmentStatsMonitor(druidServerConfig, segmentLoadDropMgr, segmentLoaderConfig);
-    Mockito.when(druidServerConfig.getTier()).thenReturn(TIER);
-    Mockito.when(druidServerConfig.getPriority()).thenReturn(PRIORITY);
+    when(druidServerConfig.getTier()).thenReturn(TIER);
+    when(druidServerConfig.getPriority()).thenReturn(PRIORITY);
   }
 
   @Test(expected = IllegalStateException.class)
   public void testLazyLoadOnStartThrowsException() {
-    SegmentLoaderConfig segmentLoaderConfig = Mockito.mock(SegmentLoaderConfig.class);
-    Mockito.when(segmentLoaderConfig.isLazyLoadOnStart()).thenReturn(true);
+    SegmentLoaderConfig segmentLoaderConfig = mock(SegmentLoaderConfig.class);
+    when(segmentLoaderConfig.isLazyLoadOnStart()).thenReturn(true);
 
     // should throw an exception here
     new SegmentStatsMonitor(druidServerConfig, segmentLoadDropMgr, segmentLoaderConfig);
@@ -76,15 +81,15 @@ public class SegmentStatsMonitorTest {
         new SegmentRowCountDistribution();
     segmentRowCountDistribution.addRowCountToDistribution(100_000L);
 
-    Mockito.when(segmentLoadDropMgr.getAverageNumOfRowsPerSegmentForDatasource())
+    when(segmentLoadDropMgr.getAverageNumOfRowsPerSegmentForDatasource())
         .thenReturn(ImmutableMap.of(DATA_SOURCE, 100_000L));
-    Mockito.when(segmentLoadDropMgr.getRowCountDistributionPerDatasource())
+    when(segmentLoadDropMgr.getRowCountDistributionPerDatasource())
         .thenReturn(ImmutableMap.of(DATA_SOURCE, segmentRowCountDistribution));
 
     ArgumentCaptor<ServiceEventBuilder<ServiceMetricEvent>> eventArgumentCaptor =
         ArgumentCaptor.forClass(ServiceEventBuilder.class);
     monitor.doMonitor(serviceEmitter);
-    Mockito.verify(serviceEmitter, Mockito.atLeastOnce()).emit(eventArgumentCaptor.capture());
+    verify(serviceEmitter, atLeastOnce()).emit(eventArgumentCaptor.capture());
 
     List<Map<String, Object>> eventsAsMaps = getEventMaps(eventArgumentCaptor.getAllValues());
     Map<String, Map<String, Object>> actual = metricKeyedMap(eventsAsMaps);
@@ -120,15 +125,15 @@ public class SegmentStatsMonitorTest {
     segmentRowCountDistribution.addTombstoneToDistribution();
     segmentRowCountDistribution.addTombstoneToDistribution();
 
-    Mockito.when(segmentLoadDropMgr.getAverageNumOfRowsPerSegmentForDatasource())
+    when(segmentLoadDropMgr.getAverageNumOfRowsPerSegmentForDatasource())
         .thenReturn(ImmutableMap.of(DATA_SOURCE, 50_000L));
-    Mockito.when(segmentLoadDropMgr.getRowCountDistributionPerDatasource())
+    when(segmentLoadDropMgr.getRowCountDistributionPerDatasource())
         .thenReturn(ImmutableMap.of(DATA_SOURCE, segmentRowCountDistribution));
 
     ArgumentCaptor<ServiceEventBuilder<ServiceMetricEvent>> eventArgumentCaptor =
         ArgumentCaptor.forClass(ServiceEventBuilder.class);
     monitor.doMonitor(serviceEmitter);
-    Mockito.verify(serviceEmitter, Mockito.atLeastOnce()).emit(eventArgumentCaptor.capture());
+    verify(serviceEmitter, atLeastOnce()).emit(eventArgumentCaptor.capture());
 
     List<Map<String, Object>> eventsAsMaps = getEventMaps(eventArgumentCaptor.getAllValues());
     Map<String, Map<String, Object>> actual = metricKeyedMap(eventsAsMaps);
@@ -175,19 +180,19 @@ public class SegmentStatsMonitorTest {
     return eventBuilders.stream()
         .map(eventBuilder -> new HashMap<>(eventBuilder.build(ImmutableMap.of()).toMap()))
         .peek(mappedValues -> mappedValues.remove("timestamp"))
-        .collect(Collectors.toList());
+        .collect(toList());
   }
 
   private Map<String, Map<String, Object>> metricKeyedMap(List<Map<String, Object>> eventsAsMaps) {
     return eventsAsMaps.stream()
         .collect(
-            Collectors.toMap(
+            toMap(
                 eventasdf -> {
                   String metricName = eventasdf.get("metric").toString();
                   String range = eventasdf.getOrDefault("range", "").toString();
                   return metricName + range;
                 },
-                Function.identity()));
+                identity()));
   }
 
   private ServiceEventBuilder<ServiceMetricEvent> averageRowCountEvent(Number value) {
diff --git a/server/src/test/java/org/apache/druid/server/mocks/MockHttpServletRequest.java b/server/src/test/java/org/apache/druid/server/mocks/MockHttpServletRequest.java
index 031abcf298..fe8d412d7e 100644
--- a/server/src/test/java/org/apache/druid/server/mocks/MockHttpServletRequest.java
+++ b/server/src/test/java/org/apache/druid/server/mocks/MockHttpServletRequest.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.mocks;
 
+import static com.google.common.base.Preconditions.checkState;
+
 import java.io.BufferedReader;
 import java.security.Principal;
 import java.util.Collection;
@@ -416,10 +418,9 @@ public class MockHttpServletRequest implements HttpServletRequest {
 
   @Override
   public AsyncContext getAsyncContext() {
-    if (currAsyncContext == null) {
-      throw new IllegalStateException(
-          "Must be put into Async mode before async context can be gottendid");
-    }
+    checkState(
+        currAsyncContext != null,
+        "Must be put into Async mode before async context can be gottendid");
     return currAsyncContext;
   }
 
diff --git a/server/src/test/java/org/apache/druid/server/mocks/MockHttpServletResponse.java b/server/src/test/java/org/apache/druid/server/mocks/MockHttpServletResponse.java
index 223371da70..4939a32578 100644
--- a/server/src/test/java/org/apache/druid/server/mocks/MockHttpServletResponse.java
+++ b/server/src/test/java/org/apache/druid/server/mocks/MockHttpServletResponse.java
@@ -19,6 +19,8 @@
 
 package org.apache.druid.server.mocks;
 
+import static com.google.common.base.Preconditions.checkState;
+
 import com.google.common.collect.Multimap;
 import com.google.common.collect.Multimaps;
 import java.io.ByteArrayOutputStream;
@@ -64,9 +66,7 @@ public class MockHttpServletResponse implements HttpServletResponse {
 
   @Override
   public void reset() {
-    if (isCommitted()) {
-      throw new IllegalStateException("Cannot reset a committed ServletResponse");
-    }
+    checkState(!isCommitted(), "Cannot reset a committed ServletResponse");
 
     headers.clear();
     statusCode = 0;
diff --git a/server/src/test/java/org/apache/druid/server/security/AllowHttpMethodsResourceFilterTest.java b/server/src/test/java/org/apache/druid/server/security/AllowHttpMethodsResourceFilterTest.java
index 0360da22cc..0e8b3b9168 100644
--- a/server/src/test/java/org/apache/druid/server/security/AllowHttpMethodsResourceFilterTest.java
+++ b/server/src/test/java/org/apache/druid/server/security/AllowHttpMethodsResourceFilterTest.java
@@ -19,6 +19,10 @@
 
 package org.apache.druid.server.security;
 
+import static org.mockito.Mockito.clearInvocations;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.verify;
+
 import com.google.common.collect.ImmutableList;
 import java.io.IOException;
 import javax.servlet.FilterChain;
@@ -29,7 +33,6 @@ import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -51,25 +54,25 @@ public class AllowHttpMethodsResourceFilterTest {
   @Test
   public void testDoFilterMethodNotAllowedShouldReturnAnError()
       throws IOException, ServletException {
-    Mockito.doReturn(METHOD_NOT_ALLOWED).when(request).getMethod();
+    doReturn(METHOD_NOT_ALLOWED).when(request).getMethod();
     target.doFilter(request, response, filterChain);
-    Mockito.verify(response).sendError(HttpServletResponse.SC_METHOD_NOT_ALLOWED);
+    verify(response).sendError(HttpServletResponse.SC_METHOD_NOT_ALLOWED);
   }
 
   @Test
   public void testDoFilterMethodAllowedShouldFilter() throws IOException, ServletException {
-    Mockito.doReturn(METHOD_ALLOWED).when(request).getMethod();
+    doReturn(METHOD_ALLOWED).when(request).getMethod();
     target.doFilter(request, response, filterChain);
-    Mockito.verify(filterChain).doFilter(request, response);
+    verify(filterChain).doFilter(request, response);
   }
 
   @Test
   public void testDoFilterSupportedHttpMethodsShouldFilter() throws IOException, ServletException {
     for (String method : AllowHttpMethodsResourceFilter.SUPPORTED_METHODS) {
-      Mockito.doReturn(method).when(request).getMethod();
+      doReturn(method).when(request).getMethod();
       target.doFilter(request, response, filterChain);
-      Mockito.verify(filterChain).doFilter(request, response);
-      Mockito.clearInvocations(filterChain);
+      verify(filterChain).doFilter(request, response);
+      clearInvocations(filterChain);
     }
   }
 }
diff --git a/server/src/test/java/org/apache/druid/server/security/AuthorizationUtilsTest.java b/server/src/test/java/org/apache/druid/server/security/AuthorizationUtilsTest.java
index 50f985bfd4..cd7b2384f9 100644
--- a/server/src/test/java/org/apache/druid/server/security/AuthorizationUtilsTest.java
+++ b/server/src/test/java/org/apache/druid/server/security/AuthorizationUtilsTest.java
@@ -19,9 +19,10 @@
 
 package org.apache.druid.server.security;
 
+import static java.util.Collections.singletonList;
+
 import com.google.common.base.Function;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -52,8 +53,7 @@ public class AuthorizationUtilsTest {
             if ("filteredResource".equals(input)) {
               return null;
             }
-            return Collections.singletonList(
-                new ResourceAction(Resource.STATE_RESOURCE, Action.READ));
+            return singletonList(new ResourceAction(Resource.STATE_RESOURCE, Action.READ));
           }
         };
 
diff --git a/server/src/test/java/org/apache/druid/server/security/ForbiddenExceptionTest.java b/server/src/test/java/org/apache/druid/server/security/ForbiddenExceptionTest.java
index c9decec45f..d049b3b3d5 100644
--- a/server/src/test/java/org/apache/druid/server/security/ForbiddenExceptionTest.java
+++ b/server/src/test/java/org/apache/druid/server/security/ForbiddenExceptionTest.java
@@ -19,13 +19,15 @@
 
 package org.apache.druid.server.security;
 
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyNoMoreInteractions;
+import static org.mockito.Mockito.when;
+
 import java.util.function.Function;
 import org.junit.Assert;
 import org.junit.Test;
 import org.junit.runner.RunWith;
-import org.mockito.ArgumentMatchers;
 import org.mockito.Mock;
-import org.mockito.Mockito;
 import org.mockito.junit.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
@@ -37,26 +39,24 @@ public class ForbiddenExceptionTest {
 
   @Test
   public void testSanitizeWithTransformFunctionReturningNull() {
-    Mockito.when(trasformFunction.apply(ArgumentMatchers.eq(ERROR_MESSAGE_ORIGINAL)))
-        .thenReturn(null);
+    when(trasformFunction.apply(ERROR_MESSAGE_ORIGINAL)).thenReturn(null);
     ForbiddenException forbiddenException = new ForbiddenException(ERROR_MESSAGE_ORIGINAL);
     ForbiddenException actual = forbiddenException.sanitize(trasformFunction);
     Assert.assertNotNull(actual);
     Assert.assertEquals(actual.getMessage(), Access.DEFAULT_ERROR_MESSAGE);
-    Mockito.verify(trasformFunction).apply(ArgumentMatchers.eq(ERROR_MESSAGE_ORIGINAL));
-    Mockito.verifyNoMoreInteractions(trasformFunction);
+    verify(trasformFunction).apply(ERROR_MESSAGE_ORIGINAL);
+    verifyNoMoreInteractions(trasformFunction);
   }
 
   @Test
   public void testSanitizeWithTransformFunctionReturningNewString() {
-    Mockito.when(trasformFunction.apply(ArgumentMatchers.eq(ERROR_MESSAGE_ORIGINAL)))
-        .thenReturn(ERROR_MESSAGE_TRANSFORMED);
+    when(trasformFunction.apply(ERROR_MESSAGE_ORIGINAL)).thenReturn(ERROR_MESSAGE_TRANSFORMED);
     ForbiddenException forbiddenException = new ForbiddenException(ERROR_MESSAGE_ORIGINAL);
     ForbiddenException actual = forbiddenException.sanitize(trasformFunction);
     Assert.assertNotNull(actual);
     Assert.assertEquals(actual.getMessage(), ERROR_MESSAGE_TRANSFORMED);
-    Mockito.verify(trasformFunction).apply(ArgumentMatchers.eq(ERROR_MESSAGE_ORIGINAL));
-    Mockito.verifyNoMoreInteractions(trasformFunction);
+    verify(trasformFunction).apply(ERROR_MESSAGE_ORIGINAL);
+    verifyNoMoreInteractions(trasformFunction);
   }
 
   // Silly, but required to get the code coverage tests to pass.
